{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cdc-2021-03-31_11_54_13\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "from timeit import default_timer as timer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "## Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from src.logger_v1 import setup_logs\n",
    "\n",
    "run_name = \"cdc\" + time.strftime(\"-%Y-%m-%d_%H_%M_%S\")\n",
    "print(run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window(a, window):\n",
    "    shape =  (a.shape[0] - window + 1, window) + a.shape[1:]\n",
    "    strides = (a.strides[0],) + a.strides\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides, writeable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScheduledOptim(object):\n",
    "    \"\"\"A simple wrapper class for learning rate scheduling\"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, n_warmup_steps):\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = 128 \n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.n_current_steps = 0 \n",
    "        self.delta = 1\n",
    "\n",
    "    def state_dict(self):\n",
    "        self.optimizer.state_dict()\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Step by the inner optimizer\"\"\"\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"Zero out the gradients by the inner optimizer\"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def increase_delta(self):\n",
    "        self.delta *= 2\n",
    "\n",
    "    def update_learning_rate(self):\n",
    "        \"\"\"Learning rate scheduling per step\"\"\"\n",
    "\n",
    "        self.n_current_steps += self.delta\n",
    "        new_lr = np.power(self.d_model, -0.5) * np.min([\n",
    "            np.power(self.n_current_steps, -0.5),\n",
    "            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n",
    "\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = new_lr\n",
    "        return new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TETrainDataset(data.Dataset):\n",
    "    def __init__(self, fault=list(range(1,21)), window=20):\n",
    "        \"\"\" fault: [1,2,3,4,5,6]\n",
    "            window: 20\n",
    "        \"\"\"\n",
    "        self.window = window \n",
    "        temp = torch.from_numpy(rolling_window(np.loadtxt('data/d00.dat').T, window))\n",
    "        self.sample = [temp]\n",
    "        self.label = [0 for _ in temp]\n",
    "\n",
    "        for label in fault:\n",
    "            if label < 10:\n",
    "                num = '0' + str(label)\n",
    "            else:\n",
    "                num = str(label)\n",
    "            temp = rolling_window(np.loadtxt('data/d' + num + '.dat'), window)\n",
    "            self.sample.append(temp)\n",
    "            self.label.extend([label for _ in temp])\n",
    "        \n",
    "        self.sample = np.concatenate(self.sample,0)\n",
    "        \n",
    "        if os.path.exists('./scalar'):\n",
    "            std = joblib.load('./scalar')\n",
    "            sh = self.sample.shape\n",
    "            self.sample = std.transform(self.sample.reshape(-1,sh[-1])).reshape(sh)\n",
    "        else:\n",
    "            std = StandardScaler()\n",
    "            sh = self.sample.shape\n",
    "            std.fit(self.sample.reshape(-1,sh[-1]))\n",
    "            self.sample = std.transform(self.sample.reshape(-1,sh[-1])).reshape(sh)\n",
    "            joblib.dump(std, './scalar')\n",
    "        \n",
    "        self.sample = torch.from_numpy(self.sample).float()\n",
    "        self.label = torch.tensor(self.label).float()\n",
    "        assert self.sample.shape[0] == self.label.shape[0]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sample)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.sample[index], self.label[index]\n",
    "\n",
    "class TETestDataset(data.Dataset):\n",
    "    def __init__(self, fault=list(range(1,21)), window=20):\n",
    "        \"\"\" fault: [1,2,3,4,5,6]\n",
    "            window: 20\n",
    "        \"\"\"\n",
    "        self.window = window \n",
    "        temp = torch.from_numpy(rolling_window(np.loadtxt('data/d00_te.dat'), window))\n",
    "        self.sample = [temp]\n",
    "        self.label = [0 for _ in temp]\n",
    "\n",
    "        for label in fault:\n",
    "            if label < 10:\n",
    "                num = '0' + str(label)\n",
    "            else:\n",
    "                num = str(label)\n",
    "            temp = rolling_window(np.loadtxt('data/d' + num + '_te.dat'), window)\n",
    "            self.sample.append(temp)\n",
    "            if window <= 160:\n",
    "                self.label.extend([0 for _ in range(160-window+1)])\n",
    "                self.label.extend([label for _ in range(800)])\n",
    "            else:\n",
    "                self.label.extend([label for _ in range(960-window+1)])\n",
    "        \n",
    "        self.sample = np.concatenate(self.sample,0)\n",
    "        \n",
    "        if os.path.exists('./scalar'):\n",
    "            std = joblib.load('./scalar')\n",
    "            sh = self.sample.shape\n",
    "            self.sample = std.transform(self.sample.reshape(-1,sh[-1])).reshape(sh)\n",
    "        else:\n",
    "            std = StandardScaler()\n",
    "            sh = self.sample.shape\n",
    "            std.fit(self.sample.reshape(-1,sh[-1]))\n",
    "            self.sample = std.transform(self.sample.reshape(-1,sh[-1])).reshape(sh)\n",
    "            joblib.dump(std, './scalar')\n",
    "        \n",
    "        self.sample = torch.from_numpy(self.sample).float()\n",
    "        self.label = torch.tensor(self.label).float()\n",
    "        assert self.sample.shape[0] == self.label.shape[0]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sample)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.sample[index], self.label[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPC_GRU(nn.Module):\n",
    "    def __init__(self, input_size, low_size, high_size, num_layers, timestep, batch_size, seq_len):\n",
    "\n",
    "        super(CPC_GRU, self).__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.timestep = timestep\n",
    "        self.input_size = input_size\n",
    "        self.low_size = low_size\n",
    "        self.high_size = high_size\n",
    "        self.num_layers = num_layers\n",
    "        self.encoder = nn.Sequential( # downsampling factor = 160\n",
    "            nn.Linear(self.input_size,self.low_size, bias=False),\n",
    "            nn.BatchNorm1d(self.seq_len+self.timestep),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(self.low_size, self.low_size, bias=False),\n",
    "            nn.BatchNorm1d(self.seq_len+self.timestep),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.gru = nn.GRU(self.low_size, self.high_size, num_layers=self.num_layers, bidirectional=False, batch_first=True)\n",
    "        self.Wk  = nn.ModuleList([nn.Linear(self.high_size, self.low_size) for i in range(timestep)])\n",
    "        self.softmax  = nn.Softmax()\n",
    "        self.lsoftmax = nn.LogSoftmax()\n",
    "\n",
    "        def _weights_init(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # initialize gru\n",
    "        for layer_p in self.gru._all_weights:\n",
    "            for p in layer_p:\n",
    "                if 'weight' in p:\n",
    "                    nn.init.kaiming_normal_(self.gru.__getattr__(p), mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def init_hidden(self, batch_size, use_gpu=True):\n",
    "        if use_gpu: return torch.zeros(self.num_layers, batch_size, self.high_size).cuda()\n",
    "        else: return torch.zeros(self.num_layers, batch_size, self.high_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch = x.size()[0]\n",
    "        z = self.encoder(x)\n",
    "\n",
    "        encode_samples = z[:,-1*self.timestep:,:].transpose(0,1)\n",
    "       \n",
    "        nce = 0 # average over timestep and batch\n",
    "        forward_seq = z[:,:self.seq_len,:] \n",
    "        output, hidden = self.gru(forward_seq, hidden) \n",
    "        c_t = output[:,-1,:].view(batch, -1) \n",
    "        pred = torch.empty((self.timestep,batch,32)).float().to(x.device) \n",
    "        for i in np.arange(0, self.timestep):\n",
    "            pred[i] = self.Wk[i](c_t) \n",
    "        for i in np.arange(0, self.timestep):\n",
    "            total = torch.mm(encode_samples[i], torch.transpose(pred[i],0,1)) # e.g. size 8*8\n",
    "            correct = torch.sum(torch.eq(torch.argmax(self.softmax(total), dim=0).cpu(), torch.arange(0, batch))) # correct is a tensor\n",
    "            nce += torch.sum(torch.diag(self.lsoftmax(total))) # nce is a tensor\n",
    "        nce /= -1.*batch*self.timestep\n",
    "        accuracy = 1.*correct.item()/batch\n",
    "\n",
    "        return accuracy, nce\n",
    "\n",
    "    def predict(self, x, hidden):\n",
    "        batch = x.size()[0]\n",
    "       \n",
    "        z = self.encoder(x)\n",
    "        \n",
    "        # z = z.transpose(1,2)\n",
    "        output, _ = self.gru(z, hidden) \n",
    "\n",
    "        return output[:,-1,:].view(batch, -1) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch, batch_size):\n",
    "    model.train()\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        seq, label = data\n",
    "        seq, label = seq.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        hidden = model.init_hidden(len(seq), use_gpu=True)\n",
    "        acc, loss = model(seq, hidden)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr = optimizer.update_learning_rate()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            logger.info('Train Epoch: {} [{}/{} ({:.0f}%)]\\tlr:{:.5f}\\tAccuracy: {:.4f}\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), lr, acc, loss.item()))\n",
    "\n",
    "def validation(args, model, device, data_loader, batch_size):\n",
    "    logger.info(\"Starting Validation\")\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_acc  = 0 \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            seq, label = data\n",
    "            seq, label = seq.to(device), label.to(device)\n",
    "            hidden = model.init_hidden(len(seq), use_gpu=True)\n",
    "            acc, loss = model(seq, hidden)\n",
    "            total_loss += len(seq) * loss \n",
    "            total_acc  += len(seq) * acc\n",
    "\n",
    "    total_loss /= len(data_loader.dataset) # average loss\n",
    "    total_acc  /= len(data_loader.dataset) # average acc\n",
    "\n",
    "    logger.info('===> Validation set: Average loss: {:.4f}\\tAccuracy: {:.4f}\\n'.format(\n",
    "                total_loss, total_acc))\n",
    "\n",
    "    return total_acc, total_loss\n",
    "\n",
    "def predict(args, model, device, data_loader, batch_size):\n",
    "    logger.info(\"Starting predict\")\n",
    "    model.eval()\n",
    "    features, labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            seq, label = data\n",
    "            seq, label = seq.to(device), label.to(device)\n",
    "            hidden = model.init_hidden(len(seq), use_gpu=True)\n",
    "            feature = model.predict(seq, hidden)\n",
    "            features.append(feature)\n",
    "            labels.append(label)\n",
    "\n",
    "    return torch.cat(features, axis=0), torch.cat(labels, axis=0)\n",
    "\n",
    "def snapshot(dir_path, run_name, state):\n",
    "    snapshot_file = os.path.join(dir_path,\n",
    "                    run_name + '-model_best.pth')\n",
    "    \n",
    "    torch.save(state, snapshot_file)\n",
    "    logger.info(\"Snapshot saved to {}\\n\".format(snapshot_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "use_cuda is True\n"
     ]
    }
   ],
   "source": [
    "# def main():\n",
    "parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "parser.add_argument('--logging-dir', default='snapshot/cdc/',\n",
    "                    help='model save directory')\n",
    "parser.add_argument('--epochs', type=int, default=60, metavar='N',\n",
    "                    help='number of epochs to train')\n",
    "parser.add_argument('--n-warmup-steps', type=int, default=50)\n",
    "parser.add_argument('--batch-size', type=int, default=10, \n",
    "                    help='batch size')\n",
    "parser.add_argument('--window', type=int, default=20, \n",
    "                    help='window length to sample from each utterance')\n",
    "parser.add_argument('--timestep', type=int, default=10) \n",
    "parser.add_argument('--masked-frames', type=int, default=20)\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "#     args = parser.parse_args()\n",
    "args, unknown = parser.parse_known_args()\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "print('use_cuda is', use_cuda)\n",
    "global_timer = timer() # global timer\n",
    "logger = setup_logs(args.logging_dir, run_name) # setup logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.timestep = 5\n",
    "args.window = 20\n",
    "args.batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "===> loading train, validation and eval dataset\n",
      "### Model summary below###\n",
      " CPC_GRU(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=52, out_features=32, bias=False)\n",
      "    (1): BatchNorm1d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=32, out_features=32, bias=False)\n",
      "    (4): BatchNorm1d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (gru): GRU(32, 16, num_layers=2, batch_first=True)\n",
      "  (Wk): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (3): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (4): Linear(in_features=16, out_features=32, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=None)\n",
      "  (lsoftmax): LogSoftmax(dim=None)\n",
      ")\n",
      "\n",
      "===> Model total parameter: 9540\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "model = CPC_GRU(input_size=52, low_size=32, high_size=16, num_layers=2, timestep=args.timestep, batch_size=args.batch_size, seq_len=args.window).to(device)\n",
    "\n",
    "params = {'num_workers': 0,\n",
    "            'pin_memory': False} if use_cuda else {}\n",
    "\n",
    "logger.info('===> loading train, validation and eval dataset')\n",
    "training_set   = TETrainDataset(window=args.window+args.timestep)\n",
    "#training_set   = ReverseRawDataset(args.train_raw, args.train_list, args.audio_window)\n",
    "#training_set   = RawXXreverseDataset(args.train_raw, args.train_list, args.audio_window)\n",
    "test_set = TETestDataset(window=args.window+args.timestep)\n",
    "#validation_set = ReverseRawDataset(args.validation_raw, args.validation_list, args.audio_window)\n",
    "#validation_set = RawXXreverseDataset(args.validation_raw, args.validation_list, args.audio_window)\n",
    "train_loader = data.DataLoader(training_set, batch_size=args.batch_size, shuffle=True, **params) # set shuffle to True\n",
    "test_loader = data.DataLoader(test_set, batch_size=args.batch_size, shuffle=False, **params) # set shuffle to False\n",
    "# nanxin optimizer  \n",
    "optimizer = ScheduledOptim(\n",
    "    optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()), \n",
    "        betas=(0.9, 0.98), eps=1e-09, weight_decay=1e-4, amsgrad=True),\n",
    "    args.n_warmup_steps)\n",
    "\n",
    "model_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "logger.info('### Model summary below###\\n {}\\n'.format(str(model)))\n",
    "logger.info('===> Model total parameter: {}\\n'.format(model_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-5-d7c65bca4815>:60: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  correct = torch.sum(torch.eq(torch.argmax(self.softmax(total), dim=0).cpu(), torch.arange(0, batch))) # correct is a tensor\n",
      "<ipython-input-5-d7c65bca4815>:61: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  nce += torch.sum(torch.diag(self.lsoftmax(total))) # nce is a tensor\n",
      "Train Epoch: 1 [0/9596 (0%)]\tlr:0.00202\tAccuracy: 1.0000\tLoss: 0.291288\n",
      "Train Epoch: 1 [20/9596 (1%)]\tlr:0.00201\tAccuracy: 1.0000\tLoss: 0.522958\n",
      "Train Epoch: 1 [40/9596 (2%)]\tlr:0.00201\tAccuracy: 0.8000\tLoss: 0.314301\n",
      "Train Epoch: 1 [60/9596 (3%)]\tlr:0.00200\tAccuracy: 0.8000\tLoss: 0.179928\n",
      "Train Epoch: 1 [80/9596 (4%)]\tlr:0.00200\tAccuracy: 1.0000\tLoss: 0.145778\n",
      "Train Epoch: 1 [100/9596 (5%)]\tlr:0.00199\tAccuracy: 1.0000\tLoss: 0.082156\n",
      "Train Epoch: 1 [120/9596 (6%)]\tlr:0.00199\tAccuracy: 0.8000\tLoss: 0.302828\n",
      "Train Epoch: 1 [140/9596 (7%)]\tlr:0.00198\tAccuracy: 1.0000\tLoss: 0.149307\n",
      "Train Epoch: 1 [160/9596 (8%)]\tlr:0.00198\tAccuracy: 1.0000\tLoss: 0.093662\n",
      "Train Epoch: 1 [180/9596 (9%)]\tlr:0.00197\tAccuracy: 1.0000\tLoss: 0.007771\n",
      "Train Epoch: 1 [200/9596 (10%)]\tlr:0.00197\tAccuracy: 0.7000\tLoss: 0.420934\n",
      "Train Epoch: 1 [220/9596 (11%)]\tlr:0.00196\tAccuracy: 1.0000\tLoss: 0.095344\n",
      "Train Epoch: 1 [240/9596 (12%)]\tlr:0.00196\tAccuracy: 0.6000\tLoss: 0.405250\n",
      "Train Epoch: 1 [260/9596 (14%)]\tlr:0.00195\tAccuracy: 1.0000\tLoss: 0.143460\n",
      "Train Epoch: 1 [280/9596 (15%)]\tlr:0.00195\tAccuracy: 1.0000\tLoss: 0.231152\n",
      "Train Epoch: 1 [300/9596 (16%)]\tlr:0.00194\tAccuracy: 1.0000\tLoss: 0.153546\n",
      "Train Epoch: 1 [320/9596 (17%)]\tlr:0.00194\tAccuracy: 1.0000\tLoss: 0.134381\n",
      "Train Epoch: 1 [340/9596 (18%)]\tlr:0.00193\tAccuracy: 1.0000\tLoss: 0.258653\n",
      "Train Epoch: 1 [360/9596 (19%)]\tlr:0.00193\tAccuracy: 1.0000\tLoss: 0.150603\n",
      "Train Epoch: 1 [380/9596 (20%)]\tlr:0.00192\tAccuracy: 1.0000\tLoss: 0.276036\n",
      "Train Epoch: 1 [400/9596 (21%)]\tlr:0.00192\tAccuracy: 1.0000\tLoss: 0.185589\n",
      "Train Epoch: 1 [420/9596 (22%)]\tlr:0.00191\tAccuracy: 1.0000\tLoss: 0.043289\n",
      "Train Epoch: 1 [440/9596 (23%)]\tlr:0.00191\tAccuracy: 0.8000\tLoss: 0.329736\n",
      "Train Epoch: 1 [460/9596 (24%)]\tlr:0.00191\tAccuracy: 1.0000\tLoss: 0.048148\n",
      "Train Epoch: 1 [480/9596 (25%)]\tlr:0.00190\tAccuracy: 1.0000\tLoss: 0.072891\n",
      "Train Epoch: 1 [500/9596 (26%)]\tlr:0.00190\tAccuracy: 1.0000\tLoss: 0.360746\n",
      "Train Epoch: 1 [520/9596 (27%)]\tlr:0.00189\tAccuracy: 1.0000\tLoss: 0.223153\n",
      "Train Epoch: 1 [540/9596 (28%)]\tlr:0.00189\tAccuracy: 0.9000\tLoss: 0.131838\n",
      "Train Epoch: 1 [560/9596 (29%)]\tlr:0.00188\tAccuracy: 1.0000\tLoss: 0.136046\n",
      "Train Epoch: 1 [580/9596 (30%)]\tlr:0.00188\tAccuracy: 0.7000\tLoss: 0.457459\n",
      "Train Epoch: 1 [600/9596 (31%)]\tlr:0.00188\tAccuracy: 1.0000\tLoss: 0.123620\n",
      "Train Epoch: 1 [620/9596 (32%)]\tlr:0.00187\tAccuracy: 1.0000\tLoss: 0.157035\n",
      "Train Epoch: 1 [640/9596 (33%)]\tlr:0.00187\tAccuracy: 0.8000\tLoss: 0.357489\n",
      "Train Epoch: 1 [660/9596 (34%)]\tlr:0.00186\tAccuracy: 0.8000\tLoss: 0.551757\n",
      "Train Epoch: 1 [680/9596 (35%)]\tlr:0.00186\tAccuracy: 0.9000\tLoss: 0.394680\n",
      "Train Epoch: 1 [700/9596 (36%)]\tlr:0.00185\tAccuracy: 1.0000\tLoss: 0.149818\n",
      "Train Epoch: 1 [720/9596 (38%)]\tlr:0.00185\tAccuracy: 0.9000\tLoss: 0.267099\n",
      "Train Epoch: 1 [740/9596 (39%)]\tlr:0.00185\tAccuracy: 1.0000\tLoss: 0.121023\n",
      "Train Epoch: 1 [760/9596 (40%)]\tlr:0.00184\tAccuracy: 1.0000\tLoss: 0.072109\n",
      "Train Epoch: 1 [780/9596 (41%)]\tlr:0.00184\tAccuracy: 1.0000\tLoss: 0.109878\n",
      "Train Epoch: 1 [800/9596 (42%)]\tlr:0.00183\tAccuracy: 1.0000\tLoss: 0.225141\n",
      "Train Epoch: 1 [820/9596 (43%)]\tlr:0.00183\tAccuracy: 1.0000\tLoss: 0.034890\n",
      "Train Epoch: 1 [840/9596 (44%)]\tlr:0.00183\tAccuracy: 1.0000\tLoss: 0.374984\n",
      "Train Epoch: 1 [860/9596 (45%)]\tlr:0.00182\tAccuracy: 1.0000\tLoss: 0.162867\n",
      "Train Epoch: 1 [880/9596 (46%)]\tlr:0.00182\tAccuracy: 1.0000\tLoss: 0.086356\n",
      "Train Epoch: 1 [900/9596 (47%)]\tlr:0.00182\tAccuracy: 1.0000\tLoss: 0.176560\n",
      "Train Epoch: 1 [920/9596 (48%)]\tlr:0.00181\tAccuracy: 1.0000\tLoss: 0.118203\n",
      "Train Epoch: 1 [940/9596 (49%)]\tlr:0.00181\tAccuracy: 0.8000\tLoss: 0.230724\n",
      "Train Epoch: 1 [960/9596 (50%)]\tlr:0.00180\tAccuracy: 1.0000\tLoss: 0.112554\n",
      "Train Epoch: 1 [980/9596 (51%)]\tlr:0.00180\tAccuracy: 1.0000\tLoss: 0.109066\n",
      "Train Epoch: 1 [1000/9596 (52%)]\tlr:0.00180\tAccuracy: 1.0000\tLoss: 0.315373\n",
      "Train Epoch: 1 [1020/9596 (53%)]\tlr:0.00179\tAccuracy: 1.0000\tLoss: 0.064355\n",
      "Train Epoch: 1 [1040/9596 (54%)]\tlr:0.00179\tAccuracy: 1.0000\tLoss: 0.163364\n",
      "Train Epoch: 1 [1060/9596 (55%)]\tlr:0.00179\tAccuracy: 0.8000\tLoss: 0.456813\n",
      "Train Epoch: 1 [1080/9596 (56%)]\tlr:0.00178\tAccuracy: 0.9000\tLoss: 0.953593\n",
      "Train Epoch: 1 [1100/9596 (57%)]\tlr:0.00178\tAccuracy: 0.9000\tLoss: 0.132174\n",
      "Train Epoch: 1 [1120/9596 (58%)]\tlr:0.00177\tAccuracy: 0.8000\tLoss: 0.255064\n",
      "Train Epoch: 1 [1140/9596 (59%)]\tlr:0.00177\tAccuracy: 1.0000\tLoss: 0.065539\n",
      "Train Epoch: 1 [1160/9596 (60%)]\tlr:0.00177\tAccuracy: 0.9000\tLoss: 0.177775\n",
      "Train Epoch: 1 [1180/9596 (61%)]\tlr:0.00176\tAccuracy: 1.0000\tLoss: 0.216988\n",
      "Train Epoch: 1 [1200/9596 (62%)]\tlr:0.00176\tAccuracy: 1.0000\tLoss: 0.439083\n",
      "Train Epoch: 1 [1220/9596 (64%)]\tlr:0.00176\tAccuracy: 1.0000\tLoss: 0.212535\n",
      "Train Epoch: 1 [1240/9596 (65%)]\tlr:0.00175\tAccuracy: 0.8000\tLoss: 0.314605\n",
      "Train Epoch: 1 [1260/9596 (66%)]\tlr:0.00175\tAccuracy: 0.9000\tLoss: 0.160964\n",
      "Train Epoch: 1 [1280/9596 (67%)]\tlr:0.00175\tAccuracy: 1.0000\tLoss: 0.128865\n",
      "Train Epoch: 1 [1300/9596 (68%)]\tlr:0.00174\tAccuracy: 1.0000\tLoss: 0.151419\n",
      "Train Epoch: 1 [1320/9596 (69%)]\tlr:0.00174\tAccuracy: 1.0000\tLoss: 0.022565\n",
      "Train Epoch: 1 [1340/9596 (70%)]\tlr:0.00174\tAccuracy: 0.9000\tLoss: 0.236129\n",
      "Train Epoch: 1 [1360/9596 (71%)]\tlr:0.00173\tAccuracy: 1.0000\tLoss: 0.150003\n",
      "Train Epoch: 1 [1380/9596 (72%)]\tlr:0.00173\tAccuracy: 1.0000\tLoss: 0.263813\n",
      "Train Epoch: 1 [1400/9596 (73%)]\tlr:0.00173\tAccuracy: 1.0000\tLoss: 0.165281\n",
      "Train Epoch: 1 [1420/9596 (74%)]\tlr:0.00172\tAccuracy: 1.0000\tLoss: 0.192073\n",
      "Train Epoch: 1 [1440/9596 (75%)]\tlr:0.00172\tAccuracy: 1.0000\tLoss: 0.153027\n",
      "Train Epoch: 1 [1460/9596 (76%)]\tlr:0.00172\tAccuracy: 0.9000\tLoss: 0.273509\n",
      "Train Epoch: 1 [1480/9596 (77%)]\tlr:0.00171\tAccuracy: 1.0000\tLoss: 0.238638\n",
      "Train Epoch: 1 [1500/9596 (78%)]\tlr:0.00171\tAccuracy: 1.0000\tLoss: 0.103662\n",
      "Train Epoch: 1 [1520/9596 (79%)]\tlr:0.00171\tAccuracy: 0.8000\tLoss: 0.290858\n",
      "Train Epoch: 1 [1540/9596 (80%)]\tlr:0.00170\tAccuracy: 1.0000\tLoss: 0.085188\n",
      "Train Epoch: 1 [1560/9596 (81%)]\tlr:0.00170\tAccuracy: 0.7000\tLoss: 0.424060\n",
      "Train Epoch: 1 [1580/9596 (82%)]\tlr:0.00170\tAccuracy: 0.9000\tLoss: 0.308244\n",
      "Train Epoch: 1 [1600/9596 (83%)]\tlr:0.00169\tAccuracy: 0.8000\tLoss: 0.261278\n",
      "Train Epoch: 1 [1620/9596 (84%)]\tlr:0.00169\tAccuracy: 1.0000\tLoss: 0.098117\n",
      "Train Epoch: 1 [1640/9596 (85%)]\tlr:0.00169\tAccuracy: 1.0000\tLoss: 0.264209\n",
      "Train Epoch: 1 [1660/9596 (86%)]\tlr:0.00169\tAccuracy: 1.0000\tLoss: 0.239268\n",
      "Train Epoch: 1 [1680/9596 (88%)]\tlr:0.00168\tAccuracy: 0.8000\tLoss: 0.219543\n",
      "Train Epoch: 1 [1700/9596 (89%)]\tlr:0.00168\tAccuracy: 0.7000\tLoss: 0.376812\n",
      "Train Epoch: 1 [1720/9596 (90%)]\tlr:0.00168\tAccuracy: 0.7000\tLoss: 0.645028\n",
      "Train Epoch: 1 [1740/9596 (91%)]\tlr:0.00167\tAccuracy: 0.8000\tLoss: 0.194969\n",
      "Train Epoch: 1 [1760/9596 (92%)]\tlr:0.00167\tAccuracy: 1.0000\tLoss: 0.179401\n",
      "Train Epoch: 1 [1780/9596 (93%)]\tlr:0.00167\tAccuracy: 0.8000\tLoss: 0.297283\n",
      "Train Epoch: 1 [1800/9596 (94%)]\tlr:0.00166\tAccuracy: 1.0000\tLoss: 0.278635\n",
      "Train Epoch: 1 [1820/9596 (95%)]\tlr:0.00166\tAccuracy: 1.0000\tLoss: 0.092565\n",
      "Train Epoch: 1 [1840/9596 (96%)]\tlr:0.00166\tAccuracy: 1.0000\tLoss: 0.068578\n",
      "Train Epoch: 1 [1860/9596 (97%)]\tlr:0.00166\tAccuracy: 1.0000\tLoss: 0.059199\n",
      "Train Epoch: 1 [1880/9596 (98%)]\tlr:0.00165\tAccuracy: 1.0000\tLoss: 0.071545\n",
      "Train Epoch: 1 [1900/9596 (99%)]\tlr:0.00165\tAccuracy: 1.0000\tLoss: 0.080597\n",
      "Starting Validation\n",
      "===> Validation set: Average loss: 2.1212\tAccuracy: 0.1949\n",
      "\n",
      "Snapshot saved to snapshot/cdc/cdc-2021-03-31_11_54_13-model_best.pth\n",
      "\n",
      "Starting predict\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got tuple",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-cd2d8c913ea0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m         })\n\u001b[0;32m     25\u001b[0m         \u001b[0mbest_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbest_epoch\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mincrease_delta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-2ceb50d67e94>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(args, model, device, data_loader, batch_size)\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msnapshot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got tuple"
     ]
    }
   ],
   "source": [
    "## Start training\n",
    "best_acc = 0\n",
    "best_loss = np.inf\n",
    "best_epoch = -1 \n",
    "features, labels = None, None\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    epoch_timer = timer()\n",
    "\n",
    "    # Train and validate\n",
    "    #trainXXreverse(args, model, device, train_loader, optimizer, epoch, args.batch_size)\n",
    "    #val_acc, val_loss = validationXXreverse(args, model, device, validation_loader, args.batch_size)\n",
    "    train(args, model, device, train_loader, optimizer, epoch, args.batch_size)\n",
    "    val_acc, val_loss = validation(args, model, device, test_loader, args.batch_size)\n",
    "    \n",
    "    # Save\n",
    "    if val_acc > best_acc: \n",
    "        best_acc = max(val_acc, best_acc)\n",
    "        snapshot(args.logging_dir, run_name, {\n",
    "            'epoch': epoch + 1,\n",
    "            'validation_acc': val_acc, \n",
    "            'state_dict': model.state_dict(),\n",
    "            'validation_loss': val_loss,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        })\n",
    "        best_epoch = epoch + 1\n",
    "        features, labels = predict(args, model, device, test_loader, args.batch_size)\n",
    "    elif epoch - best_epoch > 2:\n",
    "        optimizer.increase_delta()\n",
    "        best_epoch = epoch + 1\n",
    "    \n",
    "    end_epoch_timer = timer()\n",
    "    logger.info(\"#### End epoch {}/{}, elapsed time: {}\".format(epoch, args.epochs, end_epoch_timer - epoch_timer))\n",
    "\n",
    "## end \n",
    "end_global_timer = timer()\n",
    "logger.info(\"################## Success #########################\")\n",
    "logger.info(\"Total elapsed time: %s\" % (end_global_timer - global_timer))\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "    # main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'cdc-2021-03-27_21_17_22'"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "run_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}