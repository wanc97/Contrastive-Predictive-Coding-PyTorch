{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cdc-2021-04-01_16_21_18\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "from timeit import default_timer as timer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "## Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from src.logger_v1 import setup_logs\n",
    "\n",
    "run_name = \"cdc\" + time.strftime(\"-%Y-%m-%d_%H_%M_%S\")\n",
    "print(run_name)\n",
    "logging_dir = 'snapshot/cdc/'\n",
    "logger = setup_logs(logging_dir, run_name) # setup logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window(a, window):\n",
    "    shape =  (a.shape[0] - window + 1, window) + a.shape[1:]\n",
    "    strides = (a.strides[0],) + a.strides\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides, writeable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScheduledOptim(object):\n",
    "    \"\"\"A simple wrapper class for learning rate scheduling\"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, n_warmup_steps):\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = 128 \n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.n_current_steps = 0 \n",
    "        self.delta = 1\n",
    "\n",
    "    def state_dict(self):\n",
    "        self.optimizer.state_dict()\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Step by the inner optimizer\"\"\"\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"Zero out the gradients by the inner optimizer\"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def increase_delta(self):\n",
    "        self.delta *= 2\n",
    "\n",
    "    def update_learning_rate(self):\n",
    "        \"\"\"Learning rate scheduling per step\"\"\"\n",
    "\n",
    "        self.n_current_steps += self.delta\n",
    "        new_lr = np.power(self.d_model, -0.5) * np.min([\n",
    "            np.power(self.n_current_steps, -0.5),\n",
    "            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n",
    "\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = new_lr\n",
    "        return new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TETrainDataset(data.Dataset):\n",
    "    def __init__(self, fault=list(range(1,21)), window=20):\n",
    "        \"\"\" fault: [1,2,3,4,5,6]\n",
    "            window: 20\n",
    "        \"\"\"\n",
    "        self.window = window \n",
    "        temp = torch.from_numpy(rolling_window(np.loadtxt('data/d00.dat').T, window))\n",
    "        self.sample = [temp]\n",
    "        self.label = [0 for _ in temp]\n",
    "\n",
    "        for label in fault:\n",
    "            if label < 10:\n",
    "                num = '0' + str(label)\n",
    "            else:\n",
    "                num = str(label)\n",
    "            temp = rolling_window(np.loadtxt('data/d' + num + '.dat'), window)\n",
    "            self.sample.append(temp)\n",
    "            self.label.extend([label for _ in temp])\n",
    "        \n",
    "        self.sample = np.concatenate(self.sample,0)\n",
    "        \n",
    "        if os.path.exists('./scalar'):\n",
    "            std = joblib.load('./scalar')\n",
    "            sh = self.sample.shape\n",
    "            self.sample = std.transform(self.sample.reshape(-1,sh[-1])).reshape(sh)\n",
    "        else:\n",
    "            std = StandardScaler()\n",
    "            sh = self.sample.shape\n",
    "            std.fit(self.sample.reshape(-1,sh[-1]))\n",
    "            self.sample = std.transform(self.sample.reshape(-1,sh[-1])).reshape(sh)\n",
    "            joblib.dump(std, './scalar')\n",
    "        \n",
    "        self.sample = torch.from_numpy(self.sample).float()\n",
    "        self.label = torch.tensor(self.label).float()\n",
    "        assert self.sample.shape[0] == self.label.shape[0]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sample)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.sample[index], self.label[index]\n",
    "\n",
    "class TETestDataset(data.Dataset):\n",
    "    def __init__(self, fault=list(range(1,21)), window=20):\n",
    "        \"\"\" fault: [1,2,3,4,5,6]\n",
    "            window: 20\n",
    "        \"\"\"\n",
    "        self.window = window \n",
    "        temp = torch.from_numpy(rolling_window(np.loadtxt('data/d00_te.dat'), window))\n",
    "        self.sample = [temp]\n",
    "        self.label = [0 for _ in temp]\n",
    "\n",
    "        for label in fault:\n",
    "            if label < 10:\n",
    "                num = '0' + str(label)\n",
    "            else:\n",
    "                num = str(label)\n",
    "            temp = rolling_window(np.loadtxt('data/d' + num + '_te.dat'), window)\n",
    "            self.sample.append(temp)\n",
    "            if window <= 160:\n",
    "                self.label.extend([0 for _ in range(160-window+1)])\n",
    "                self.label.extend([label for _ in range(800)])\n",
    "            else:\n",
    "                self.label.extend([label for _ in range(960-window+1)])\n",
    "        \n",
    "        self.sample = np.concatenate(self.sample,0)\n",
    "        \n",
    "        if os.path.exists('./scalar'):\n",
    "            std = joblib.load('./scalar')\n",
    "            sh = self.sample.shape\n",
    "            self.sample = std.transform(self.sample.reshape(-1,sh[-1])).reshape(sh)\n",
    "        else:\n",
    "            std = StandardScaler()\n",
    "            sh = self.sample.shape\n",
    "            std.fit(self.sample.reshape(-1,sh[-1]))\n",
    "            self.sample = std.transform(self.sample.reshape(-1,sh[-1])).reshape(sh)\n",
    "            joblib.dump(std, './scalar')\n",
    "        \n",
    "        self.sample = torch.from_numpy(self.sample).float()\n",
    "        self.label = torch.tensor(self.label).float()\n",
    "        assert self.sample.shape[0] == self.label.shape[0]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sample)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.sample[index], self.label[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPC_GRU(nn.Module):\n",
    "    def __init__(self, input_size, low_size, high_size, num_layers, timestep, batch_size, seq_len):\n",
    "\n",
    "        super(CPC_GRU, self).__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.timestep = timestep\n",
    "        self.input_size = input_size\n",
    "        self.low_size = low_size\n",
    "        self.high_size = high_size\n",
    "        self.num_layers = num_layers\n",
    "        self.encoder = nn.Sequential( # downsampling factor = 160\n",
    "            nn.Linear(self.input_size,self.low_size, bias=False),\n",
    "            nn.BatchNorm1d(self.seq_len+self.timestep),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(self.low_size, self.low_size, bias=False),\n",
    "            nn.BatchNorm1d(self.seq_len+self.timestep),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.gru = nn.GRU(self.low_size, self.high_size, num_layers=self.num_layers, bidirectional=False, batch_first=True)\n",
    "        self.Wk  = nn.ModuleList([nn.Linear(self.high_size, self.low_size) for i in range(timestep)])\n",
    "        self.softmax  = nn.Softmax()\n",
    "        self.lsoftmax = nn.LogSoftmax()\n",
    "\n",
    "        def _weights_init(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # initialize gru\n",
    "        for layer_p in self.gru._all_weights:\n",
    "            for p in layer_p:\n",
    "                if 'weight' in p:\n",
    "                    nn.init.kaiming_normal_(self.gru.__getattr__(p), mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def init_hidden(self, batch_size, use_gpu=True):\n",
    "        if use_gpu: return torch.zeros(self.num_layers, batch_size, self.high_size).cuda()\n",
    "        else: return torch.zeros(self.num_layers, batch_size, self.high_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch = x.size()[0]\n",
    "        z = self.encoder(x)\n",
    "\n",
    "        encode_samples = z[:,-1*self.timestep:,:].transpose(0,1)\n",
    "       \n",
    "        nce = 0 # average over timestep and batch\n",
    "        forward_seq = z[:,:self.seq_len,:] \n",
    "        output, hidden = self.gru(forward_seq, hidden) \n",
    "        c_t = output[:,-1,:].view(batch, -1) \n",
    "        pred = torch.empty((self.timestep,batch,32)).float().to(x.device) \n",
    "        for i in np.arange(0, self.timestep):\n",
    "            pred[i] = self.Wk[i](c_t) \n",
    "        for i in np.arange(0, self.timestep):\n",
    "            total = torch.mm(encode_samples[i], torch.transpose(pred[i],0,1)) # e.g. size 8*8\n",
    "            correct = torch.sum(torch.eq(torch.argmax(self.softmax(total), dim=0).cpu(), torch.arange(0, batch))) # correct is a tensor\n",
    "            nce += torch.sum(torch.diag(self.lsoftmax(total))) # nce is a tensor\n",
    "        nce /= -1.*batch*self.timestep\n",
    "        accuracy = 1.*correct.item()/batch\n",
    "\n",
    "        return accuracy, nce\n",
    "\n",
    "    def predict(self, x, hidden):\n",
    "        batch = x.size()[0]\n",
    "       \n",
    "        z = self.encoder(x)\n",
    "        \n",
    "        # z = z.transpose(1,2)\n",
    "        output, _ = self.gru(z, hidden) \n",
    "\n",
    "        return output[:,-1,:].view(batch, -1) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, batch_size, log_interval):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_acc  = 0 \n",
    "    for batch_idx, data_use in enumerate(train_loader):\n",
    "        seq, label = data_use\n",
    "        seq, label = seq.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        hidden = model.init_hidden(len(seq), use_gpu=True)\n",
    "        acc, loss = model(seq, hidden)\n",
    "        total_loss += len(seq) * loss \n",
    "        total_acc  += len(seq) * acc\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr = optimizer.update_learning_rate()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            logger.info('Train Epoch: {} [{}/{} ({:.0f}%)]\\tlr:{:.5f}\\tAccuracy: {:.4f}\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data_use), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), lr, acc, loss.item()))\n",
    "    total_loss /= len(train_loader.dataset) # average loss\n",
    "    total_acc  /= len(train_loader.dataset) # average acc\n",
    "    return total_acc, total_loss\n",
    "\n",
    "def validation(model, device, data_loader, batch_size):\n",
    "    logger.info(\"Starting Validation\")\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_acc  = 0 \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data_use in data_loader:\n",
    "            seq, label = data_use\n",
    "            seq, label = seq.to(device), label.to(device)\n",
    "            hidden = model.init_hidden(len(seq), use_gpu=True)\n",
    "            acc, loss = model(seq, hidden)\n",
    "            total_loss += len(seq) * loss \n",
    "            total_acc  += len(seq) * acc\n",
    "\n",
    "    total_loss /= len(data_loader.dataset) # average loss\n",
    "    total_acc  /= len(data_loader.dataset) # average acc\n",
    "\n",
    "    logger.info('===> Validation set: Average loss: {:.4f}\\tAccuracy: {:.4f}\\n'.format(\n",
    "                total_loss, total_acc))\n",
    "\n",
    "    # return total_acc, total_loss\n",
    "\n",
    "def predict(model, device, train_loader, test_loader, batch_size):\n",
    "    logger.info(\"Starting predict\")\n",
    "    model.eval()\n",
    "\n",
    "    trains, trains_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for data_use in train_loader:\n",
    "            seq, label = data_use\n",
    "            seq, label = seq.to(device), label.to(device)\n",
    "            hidden = model.init_hidden(len(seq), use_gpu=True)\n",
    "            feature = model.predict(seq, hidden)\n",
    "            trains.append(feature)\n",
    "            trains_labels.append(label)\n",
    "    train_data = data.TensorDataset(torch.cat(trains, axis=0), torch.cat(trains_labels, axis=0))\n",
    "\n",
    "    tests, tests_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for data_use in test_loader:\n",
    "            seq, label = data_use\n",
    "            seq, label = seq.to(device), label.to(device)\n",
    "            hidden = model.init_hidden(len(seq), use_gpu=True)\n",
    "            feature = model.predict(seq, hidden)\n",
    "            tests.append(feature)\n",
    "            tests_labels.append(label)\n",
    "    test_data = data.TensorDataset(torch.cat(tests, axis=0), torch.cat(tests_labels, axis=0))\n",
    "\n",
    "    return  data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory), data.DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "def snapshot(dir_path, run_name, state):\n",
    "    snapshot_file = os.path.join(dir_path,\n",
    "                    run_name + '-model_best.pth')\n",
    "    torch.save(state, snapshot_file)\n",
    "    logger.info(\"Snapshot saved to {}\\n\".format(snapshot_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_data():\n",
    "    model = CPC_GRU(input_size=52, low_size=32, high_size=16, num_layers=2, timestep=timestep, batch_size=batch_size, seq_len=window).to(device)\n",
    "\n",
    "    params = {'num_workers': 0,\n",
    "                'pin_memory': False} if use_cuda else {}\n",
    "\n",
    "    logger.info('===> loading train and eval dataset')\n",
    "    training_set   = TETrainDataset(window=window+timestep)\n",
    "    test_set = TETestDataset(window=window+timestep)\n",
    "\n",
    "    train_loader = data.DataLoader(training_set, batch_size=batch_size, shuffle=True,\n",
    "     num_workers=num_workers, pin_memory=pin_memory) # set shuffle to True\n",
    "    test_loader = data.DataLoader(test_set, batch_size=batch_size, shuffle=False, \n",
    "    num_workers=num_workers, pin_memory=pin_memory) # set shuffle to False\n",
    "    \n",
    "    optimizer = ScheduledOptim(\n",
    "        optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, model.parameters()), \n",
    "            betas=(0.9, 0.98), eps=1e-09, weight_decay=1e-4, amsgrad=True),\n",
    "        n_warmup_steps)\n",
    "\n",
    "    model_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    logger.info('### Model summary below###\\n {}\\n'.format(str(model)))\n",
    "    logger.info('===> Model total parameter: {}\\n'.format(model_params))\n",
    "\n",
    "    return model, train_loader, test_loader, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process():\n",
    "    ## Start training\n",
    "    best_acc = 0\n",
    "    best_loss = np.inf\n",
    "    best_epoch = -1 \n",
    "    feature_train_loader, feature_test_loader = None, None\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_timer = timer()\n",
    "\n",
    "        # Train and validate\n",
    "        val_acc, val_loss = train(model, device, train_loader, optimizer, epoch, batch_size, log_interval)\n",
    "        validation(model, device, test_loader, batch_size)\n",
    "        \n",
    "        # Save\n",
    "        if val_acc > best_acc: \n",
    "            best_acc = max(val_acc, best_acc)\n",
    "            snapshot(logging_dir, run_name, {\n",
    "                'epoch': epoch + 1,\n",
    "                'validation_acc': val_acc, \n",
    "                'state_dict': model.state_dict(),\n",
    "                'validation_loss': val_loss,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "\n",
    "            })\n",
    "            best_epoch = epoch + 1\n",
    "            feature_train_loader, feature_test_loader = predict(model, device, train_loader, test_loader, batch_size)\n",
    "        elif epoch - best_epoch > 2:\n",
    "            optimizer.increase_delta()\n",
    "            best_epoch = epoch + 1\n",
    "        \n",
    "        end_epoch_timer = timer()\n",
    "        logger.info(\"#### End epoch {}/{}, elapsed time: {}\".format(epoch, epochs, end_epoch_timer - epoch_timer))\n",
    "\n",
    "    ## end \n",
    "    end_global_timer = timer()\n",
    "    logger.info(\"################## Success #########################\")\n",
    "    logger.info(\"Total elapsed time: %s\" % (end_global_timer - global_timer))\n",
    "    return feature_train_loader, feature_test_loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "use_cuda is True\n",
      "===> loading train and eval dataset\n",
      "<ipython-input-4-af71fae3d848>:7: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:141.)\n",
      "  temp = torch.from_numpy(rolling_window(np.loadtxt('data/d00.dat').T, window))\n",
      "### Model summary below###\n",
      " CPC_GRU(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=52, out_features=32, bias=False)\n",
      "    (1): BatchNorm1d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=32, out_features=32, bias=False)\n",
      "    (4): BatchNorm1d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (gru): GRU(32, 16, num_layers=2, batch_first=True)\n",
      "  (Wk): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (3): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (4): Linear(in_features=16, out_features=32, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=None)\n",
      "  (lsoftmax): LogSoftmax(dim=None)\n",
      ")\n",
      "\n",
      "===> Model total parameter: 9540\n",
      "\n",
      "<ipython-input-5-3d98d16daedd>:60: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  correct = torch.sum(torch.eq(torch.argmax(self.softmax(total), dim=0).cpu(), torch.arange(0, batch))) # correct is a tensor\n",
      "<ipython-input-5-3d98d16daedd>:61: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  nce += torch.sum(torch.diag(self.lsoftmax(total))) # nce is a tensor\n",
      "Train Epoch: 1 [0/9596 (0%)]\tlr:0.00025\tAccuracy: 0.1000\tLoss: 2.295969\n",
      "Train Epoch: 1 [60/9596 (3%)]\tlr:0.00775\tAccuracy: 0.5000\tLoss: 1.949805\n",
      "Train Epoch: 1 [120/9596 (6%)]\tlr:0.01132\tAccuracy: 0.2000\tLoss: 1.829911\n",
      "Train Epoch: 1 [180/9596 (9%)]\tlr:0.00927\tAccuracy: 0.7000\tLoss: 0.971546\n",
      "Train Epoch: 1 [240/9596 (12%)]\tlr:0.00804\tAccuracy: 0.7000\tLoss: 0.792282\n",
      "Train Epoch: 1 [300/9596 (16%)]\tlr:0.00719\tAccuracy: 0.8000\tLoss: 0.567435\n",
      "Train Epoch: 1 [360/9596 (19%)]\tlr:0.00657\tAccuracy: 1.0000\tLoss: 0.924274\n",
      "Train Epoch: 1 [420/9596 (22%)]\tlr:0.00608\tAccuracy: 0.6000\tLoss: 0.595677\n",
      "Train Epoch: 1 [480/9596 (25%)]\tlr:0.00569\tAccuracy: 0.3000\tLoss: 1.037298\n",
      "Train Epoch: 1 [540/9596 (28%)]\tlr:0.00537\tAccuracy: 1.0000\tLoss: 0.275726\n",
      "Train Epoch: 1 [600/9596 (31%)]\tlr:0.00509\tAccuracy: 1.0000\tLoss: 0.546099\n",
      "Train Epoch: 1 [660/9596 (34%)]\tlr:0.00486\tAccuracy: 0.9000\tLoss: 0.504790\n",
      "Train Epoch: 1 [720/9596 (38%)]\tlr:0.00465\tAccuracy: 1.0000\tLoss: 0.301022\n",
      "Train Epoch: 1 [780/9596 (41%)]\tlr:0.00447\tAccuracy: 1.0000\tLoss: 0.155658\n",
      "Train Epoch: 1 [840/9596 (44%)]\tlr:0.00431\tAccuracy: 0.7000\tLoss: 0.545123\n",
      "Train Epoch: 1 [900/9596 (47%)]\tlr:0.00416\tAccuracy: 0.8000\tLoss: 0.689997\n",
      "Train Epoch: 1 [960/9596 (50%)]\tlr:0.00403\tAccuracy: 0.9000\tLoss: 0.476610\n",
      "Train Epoch: 1 [1020/9596 (53%)]\tlr:0.00391\tAccuracy: 0.7000\tLoss: 0.425951\n",
      "Train Epoch: 1 [1080/9596 (56%)]\tlr:0.00380\tAccuracy: 0.9000\tLoss: 0.408002\n",
      "Train Epoch: 1 [1140/9596 (59%)]\tlr:0.00370\tAccuracy: 1.0000\tLoss: 0.200224\n",
      "Train Epoch: 1 [1200/9596 (62%)]\tlr:0.00361\tAccuracy: 1.0000\tLoss: 0.214073\n",
      "Train Epoch: 1 [1260/9596 (66%)]\tlr:0.00352\tAccuracy: 0.7000\tLoss: 0.419507\n",
      "Train Epoch: 1 [1320/9596 (69%)]\tlr:0.00344\tAccuracy: 0.7000\tLoss: 0.486450\n",
      "Train Epoch: 1 [1380/9596 (72%)]\tlr:0.00336\tAccuracy: 0.8000\tLoss: 0.233464\n",
      "Train Epoch: 1 [1440/9596 (75%)]\tlr:0.00329\tAccuracy: 1.0000\tLoss: 0.172220\n",
      "Train Epoch: 1 [1500/9596 (78%)]\tlr:0.00323\tAccuracy: 0.6000\tLoss: 0.387097\n",
      "Train Epoch: 1 [1560/9596 (81%)]\tlr:0.00316\tAccuracy: 1.0000\tLoss: 0.219720\n",
      "Train Epoch: 1 [1620/9596 (84%)]\tlr:0.00310\tAccuracy: 0.8000\tLoss: 0.269055\n",
      "Train Epoch: 1 [1680/9596 (88%)]\tlr:0.00305\tAccuracy: 0.7000\tLoss: 0.393085\n",
      "Train Epoch: 1 [1740/9596 (91%)]\tlr:0.00299\tAccuracy: 1.0000\tLoss: 0.190175\n",
      "Train Epoch: 1 [1800/9596 (94%)]\tlr:0.00294\tAccuracy: 1.0000\tLoss: 0.323410\n",
      "Train Epoch: 1 [1860/9596 (97%)]\tlr:0.00290\tAccuracy: 1.0000\tLoss: 0.323768\n",
      "Starting Validation\n",
      "===> Validation set: Average loss: 2.1846\tAccuracy: 0.1685\n",
      "\n",
      "Snapshot saved to snapshot/cdc/cdc-2021-04-01_16_21_18-model_best.pth\n",
      "\n",
      "Starting predict\n",
      "#### End epoch 1/6, elapsed time: 19.505663899999995\n",
      "Train Epoch: 2 [0/9596 (0%)]\tlr:0.00285\tAccuracy: 0.7000\tLoss: 0.555362\n",
      "Train Epoch: 2 [60/9596 (3%)]\tlr:0.00281\tAccuracy: 0.8000\tLoss: 0.421585\n",
      "Train Epoch: 2 [120/9596 (6%)]\tlr:0.00277\tAccuracy: 0.9000\tLoss: 0.434042\n",
      "Train Epoch: 2 [180/9596 (9%)]\tlr:0.00273\tAccuracy: 1.0000\tLoss: 0.085743\n",
      "Train Epoch: 2 [240/9596 (12%)]\tlr:0.00269\tAccuracy: 1.0000\tLoss: 0.246761\n",
      "Train Epoch: 2 [300/9596 (16%)]\tlr:0.00265\tAccuracy: 0.8000\tLoss: 0.463971\n",
      "Train Epoch: 2 [360/9596 (19%)]\tlr:0.00262\tAccuracy: 1.0000\tLoss: 0.456701\n",
      "Train Epoch: 2 [420/9596 (22%)]\tlr:0.00258\tAccuracy: 1.0000\tLoss: 0.175107\n",
      "Train Epoch: 2 [480/9596 (25%)]\tlr:0.00255\tAccuracy: 1.0000\tLoss: 0.046708\n",
      "Train Epoch: 2 [540/9596 (28%)]\tlr:0.00252\tAccuracy: 0.8000\tLoss: 0.258925\n",
      "Train Epoch: 2 [600/9596 (31%)]\tlr:0.00249\tAccuracy: 0.9000\tLoss: 0.324711\n",
      "Train Epoch: 2 [660/9596 (34%)]\tlr:0.00246\tAccuracy: 1.0000\tLoss: 0.332281\n",
      "Train Epoch: 2 [720/9596 (38%)]\tlr:0.00243\tAccuracy: 1.0000\tLoss: 0.094361\n",
      "Train Epoch: 2 [780/9596 (41%)]\tlr:0.00240\tAccuracy: 0.8000\tLoss: 0.217649\n",
      "Train Epoch: 2 [840/9596 (44%)]\tlr:0.00238\tAccuracy: 1.0000\tLoss: 0.087648\n",
      "Train Epoch: 2 [900/9596 (47%)]\tlr:0.00235\tAccuracy: 1.0000\tLoss: 0.032570\n",
      "Train Epoch: 2 [960/9596 (50%)]\tlr:0.00233\tAccuracy: 1.0000\tLoss: 0.246766\n",
      "Train Epoch: 2 [1020/9596 (53%)]\tlr:0.00230\tAccuracy: 1.0000\tLoss: 0.380718\n",
      "Train Epoch: 2 [1080/9596 (56%)]\tlr:0.00228\tAccuracy: 1.0000\tLoss: 0.188087\n",
      "Train Epoch: 2 [1140/9596 (59%)]\tlr:0.00226\tAccuracy: 1.0000\tLoss: 0.183794\n",
      "Train Epoch: 2 [1200/9596 (62%)]\tlr:0.00224\tAccuracy: 0.6000\tLoss: 0.433339\n",
      "Train Epoch: 2 [1260/9596 (66%)]\tlr:0.00222\tAccuracy: 1.0000\tLoss: 0.130117\n",
      "Train Epoch: 2 [1320/9596 (69%)]\tlr:0.00220\tAccuracy: 1.0000\tLoss: 0.284305\n",
      "Train Epoch: 2 [1380/9596 (72%)]\tlr:0.00218\tAccuracy: 0.8000\tLoss: 0.310685\n",
      "Train Epoch: 2 [1440/9596 (75%)]\tlr:0.00216\tAccuracy: 1.0000\tLoss: 0.234908\n",
      "Train Epoch: 2 [1500/9596 (78%)]\tlr:0.00214\tAccuracy: 1.0000\tLoss: 0.376389\n",
      "Train Epoch: 2 [1560/9596 (81%)]\tlr:0.00212\tAccuracy: 1.0000\tLoss: 0.120886\n",
      "Train Epoch: 2 [1620/9596 (84%)]\tlr:0.00210\tAccuracy: 0.8000\tLoss: 0.268884\n",
      "Train Epoch: 2 [1680/9596 (88%)]\tlr:0.00208\tAccuracy: 1.0000\tLoss: 0.219357\n",
      "Train Epoch: 2 [1740/9596 (91%)]\tlr:0.00207\tAccuracy: 0.9000\tLoss: 0.399197\n",
      "Train Epoch: 2 [1800/9596 (94%)]\tlr:0.00205\tAccuracy: 0.8000\tLoss: 0.291010\n",
      "Train Epoch: 2 [1860/9596 (97%)]\tlr:0.00203\tAccuracy: 0.7000\tLoss: 0.760814\n",
      "Starting Validation\n",
      "===> Validation set: Average loss: 2.1221\tAccuracy: 0.1826\n",
      "\n",
      "Snapshot saved to snapshot/cdc/cdc-2021-04-01_16_21_18-model_best.pth\n",
      "\n",
      "Starting predict\n",
      "#### End epoch 2/6, elapsed time: 18.90429350000001\n",
      "Train Epoch: 3 [0/9596 (0%)]\tlr:0.00202\tAccuracy: 1.0000\tLoss: 0.249436\n",
      "Train Epoch: 3 [60/9596 (3%)]\tlr:0.00200\tAccuracy: 1.0000\tLoss: 0.176421\n",
      "Train Epoch: 3 [120/9596 (6%)]\tlr:0.00199\tAccuracy: 0.9000\tLoss: 0.339778\n",
      "Train Epoch: 3 [180/9596 (9%)]\tlr:0.00197\tAccuracy: 1.0000\tLoss: 0.198213\n",
      "Train Epoch: 3 [240/9596 (12%)]\tlr:0.00196\tAccuracy: 0.8000\tLoss: 0.373818\n",
      "Train Epoch: 3 [300/9596 (16%)]\tlr:0.00194\tAccuracy: 1.0000\tLoss: 0.050349\n",
      "Train Epoch: 3 [360/9596 (19%)]\tlr:0.00193\tAccuracy: 1.0000\tLoss: 0.144399\n",
      "Train Epoch: 3 [420/9596 (22%)]\tlr:0.00191\tAccuracy: 0.9000\tLoss: 0.260525\n",
      "Train Epoch: 3 [480/9596 (25%)]\tlr:0.00190\tAccuracy: 1.0000\tLoss: 0.055393\n",
      "Train Epoch: 3 [540/9596 (28%)]\tlr:0.00189\tAccuracy: 1.0000\tLoss: 0.178304\n",
      "Train Epoch: 3 [600/9596 (31%)]\tlr:0.00188\tAccuracy: 1.0000\tLoss: 0.166021\n",
      "Train Epoch: 3 [660/9596 (34%)]\tlr:0.00186\tAccuracy: 1.0000\tLoss: 0.108241\n",
      "Train Epoch: 3 [720/9596 (38%)]\tlr:0.00185\tAccuracy: 0.8000\tLoss: 0.157705\n",
      "Train Epoch: 3 [780/9596 (41%)]\tlr:0.00184\tAccuracy: 1.0000\tLoss: 0.002742\n",
      "Train Epoch: 3 [840/9596 (44%)]\tlr:0.00183\tAccuracy: 1.0000\tLoss: 0.055479\n",
      "Train Epoch: 3 [900/9596 (47%)]\tlr:0.00182\tAccuracy: 0.7000\tLoss: 0.586379\n",
      "Train Epoch: 3 [960/9596 (50%)]\tlr:0.00180\tAccuracy: 1.0000\tLoss: 0.204092\n",
      "Train Epoch: 3 [1020/9596 (53%)]\tlr:0.00179\tAccuracy: 0.9000\tLoss: 0.392027\n",
      "Train Epoch: 3 [1080/9596 (56%)]\tlr:0.00178\tAccuracy: 1.0000\tLoss: 0.458092\n",
      "Train Epoch: 3 [1140/9596 (59%)]\tlr:0.00177\tAccuracy: 1.0000\tLoss: 0.066052\n",
      "Train Epoch: 3 [1200/9596 (62%)]\tlr:0.00176\tAccuracy: 0.9000\tLoss: 0.409355\n",
      "Train Epoch: 3 [1260/9596 (66%)]\tlr:0.00175\tAccuracy: 1.0000\tLoss: 0.067674\n",
      "Train Epoch: 3 [1320/9596 (69%)]\tlr:0.00174\tAccuracy: 1.0000\tLoss: 0.314103\n",
      "Train Epoch: 3 [1380/9596 (72%)]\tlr:0.00173\tAccuracy: 1.0000\tLoss: 0.218287\n",
      "Train Epoch: 3 [1440/9596 (75%)]\tlr:0.00172\tAccuracy: 1.0000\tLoss: 0.175213\n",
      "Train Epoch: 3 [1500/9596 (78%)]\tlr:0.00171\tAccuracy: 1.0000\tLoss: 0.336488\n",
      "Train Epoch: 3 [1560/9596 (81%)]\tlr:0.00170\tAccuracy: 0.9000\tLoss: 1.332398\n",
      "Train Epoch: 3 [1620/9596 (84%)]\tlr:0.00169\tAccuracy: 1.0000\tLoss: 0.199949\n",
      "Train Epoch: 3 [1680/9596 (88%)]\tlr:0.00168\tAccuracy: 0.8000\tLoss: 0.292787\n",
      "Train Epoch: 3 [1740/9596 (91%)]\tlr:0.00167\tAccuracy: 1.0000\tLoss: 0.138914\n",
      "Train Epoch: 3 [1800/9596 (94%)]\tlr:0.00166\tAccuracy: 1.0000\tLoss: 0.249543\n",
      "Train Epoch: 3 [1860/9596 (97%)]\tlr:0.00166\tAccuracy: 0.9000\tLoss: 0.215357\n",
      "Starting Validation\n",
      "===> Validation set: Average loss: 2.0968\tAccuracy: 0.1941\n",
      "\n",
      "Snapshot saved to snapshot/cdc/cdc-2021-04-01_16_21_18-model_best.pth\n",
      "\n",
      "Starting predict\n",
      "#### End epoch 3/6, elapsed time: 19.009566899999996\n",
      "Train Epoch: 4 [0/9596 (0%)]\tlr:0.00165\tAccuracy: 0.8000\tLoss: 0.145998\n",
      "Train Epoch: 4 [60/9596 (3%)]\tlr:0.00164\tAccuracy: 1.0000\tLoss: 0.060243\n",
      "Train Epoch: 4 [120/9596 (6%)]\tlr:0.00163\tAccuracy: 1.0000\tLoss: 0.153304\n",
      "Train Epoch: 4 [180/9596 (9%)]\tlr:0.00162\tAccuracy: 1.0000\tLoss: 0.229902\n",
      "Train Epoch: 4 [240/9596 (12%)]\tlr:0.00161\tAccuracy: 1.0000\tLoss: 0.117398\n",
      "Train Epoch: 4 [300/9596 (16%)]\tlr:0.00161\tAccuracy: 0.8000\tLoss: 0.612071\n",
      "Train Epoch: 4 [360/9596 (19%)]\tlr:0.00160\tAccuracy: 0.8000\tLoss: 0.432840\n",
      "Train Epoch: 4 [420/9596 (22%)]\tlr:0.00159\tAccuracy: 1.0000\tLoss: 0.190255\n",
      "Train Epoch: 4 [480/9596 (25%)]\tlr:0.00158\tAccuracy: 0.8000\tLoss: 0.364935\n",
      "Train Epoch: 4 [540/9596 (28%)]\tlr:0.00157\tAccuracy: 1.0000\tLoss: 0.035799\n",
      "Train Epoch: 4 [600/9596 (31%)]\tlr:0.00157\tAccuracy: 0.9000\tLoss: 0.249062\n",
      "Train Epoch: 4 [660/9596 (34%)]\tlr:0.00156\tAccuracy: 1.0000\tLoss: 0.256806\n",
      "Train Epoch: 4 [720/9596 (38%)]\tlr:0.00155\tAccuracy: 0.9000\tLoss: 0.259126\n",
      "Train Epoch: 4 [780/9596 (41%)]\tlr:0.00155\tAccuracy: 1.0000\tLoss: 0.077948\n",
      "Train Epoch: 4 [840/9596 (44%)]\tlr:0.00154\tAccuracy: 1.0000\tLoss: 0.142249\n",
      "Train Epoch: 4 [900/9596 (47%)]\tlr:0.00153\tAccuracy: 1.0000\tLoss: 0.257846\n",
      "Train Epoch: 4 [960/9596 (50%)]\tlr:0.00152\tAccuracy: 1.0000\tLoss: 0.119903\n",
      "Train Epoch: 4 [1020/9596 (53%)]\tlr:0.00152\tAccuracy: 1.0000\tLoss: 0.017910\n",
      "Train Epoch: 4 [1080/9596 (56%)]\tlr:0.00151\tAccuracy: 1.0000\tLoss: 0.245486\n",
      "Train Epoch: 4 [1140/9596 (59%)]\tlr:0.00150\tAccuracy: 1.0000\tLoss: 0.256763\n",
      "Train Epoch: 4 [1200/9596 (62%)]\tlr:0.00150\tAccuracy: 1.0000\tLoss: 0.294395\n",
      "Train Epoch: 4 [1260/9596 (66%)]\tlr:0.00149\tAccuracy: 1.0000\tLoss: 0.022812\n",
      "Train Epoch: 4 [1320/9596 (69%)]\tlr:0.00149\tAccuracy: 1.0000\tLoss: 0.154499\n",
      "Train Epoch: 4 [1380/9596 (72%)]\tlr:0.00148\tAccuracy: 1.0000\tLoss: 0.139493\n",
      "Train Epoch: 4 [1440/9596 (75%)]\tlr:0.00147\tAccuracy: 1.0000\tLoss: 0.308355\n",
      "Train Epoch: 4 [1500/9596 (78%)]\tlr:0.00147\tAccuracy: 1.0000\tLoss: 0.115410\n",
      "Train Epoch: 4 [1560/9596 (81%)]\tlr:0.00146\tAccuracy: 1.0000\tLoss: 0.149800\n",
      "Train Epoch: 4 [1620/9596 (84%)]\tlr:0.00145\tAccuracy: 1.0000\tLoss: 0.158544\n",
      "Train Epoch: 4 [1680/9596 (88%)]\tlr:0.00145\tAccuracy: 1.0000\tLoss: 0.161472\n",
      "Train Epoch: 4 [1740/9596 (91%)]\tlr:0.00144\tAccuracy: 1.0000\tLoss: 0.231694\n",
      "Train Epoch: 4 [1800/9596 (94%)]\tlr:0.00144\tAccuracy: 0.9000\tLoss: 0.177515\n",
      "Train Epoch: 4 [1860/9596 (97%)]\tlr:0.00143\tAccuracy: 1.0000\tLoss: 0.112210\n",
      "Starting Validation\n",
      "===> Validation set: Average loss: 2.0886\tAccuracy: 0.1993\n",
      "\n",
      "Snapshot saved to snapshot/cdc/cdc-2021-04-01_16_21_18-model_best.pth\n",
      "\n",
      "Starting predict\n",
      "#### End epoch 4/6, elapsed time: 19.2626664\n",
      "Train Epoch: 5 [0/9596 (0%)]\tlr:0.00143\tAccuracy: 1.0000\tLoss: 0.002328\n",
      "Train Epoch: 5 [60/9596 (3%)]\tlr:0.00142\tAccuracy: 1.0000\tLoss: 0.098700\n",
      "Train Epoch: 5 [120/9596 (6%)]\tlr:0.00142\tAccuracy: 0.8000\tLoss: 0.277516\n",
      "Train Epoch: 5 [180/9596 (9%)]\tlr:0.00141\tAccuracy: 1.0000\tLoss: 0.056892\n",
      "Train Epoch: 5 [240/9596 (12%)]\tlr:0.00140\tAccuracy: 0.9000\tLoss: 0.284766\n",
      "Train Epoch: 5 [300/9596 (16%)]\tlr:0.00140\tAccuracy: 0.8000\tLoss: 0.350073\n",
      "Train Epoch: 5 [360/9596 (19%)]\tlr:0.00139\tAccuracy: 1.0000\tLoss: 0.105881\n",
      "Train Epoch: 5 [420/9596 (22%)]\tlr:0.00139\tAccuracy: 1.0000\tLoss: 0.124436\n",
      "Train Epoch: 5 [480/9596 (25%)]\tlr:0.00138\tAccuracy: 1.0000\tLoss: 0.127876\n",
      "Train Epoch: 5 [540/9596 (28%)]\tlr:0.00138\tAccuracy: 0.8000\tLoss: 0.392922\n",
      "Train Epoch: 5 [600/9596 (31%)]\tlr:0.00137\tAccuracy: 0.9000\tLoss: 0.371164\n",
      "Train Epoch: 5 [660/9596 (34%)]\tlr:0.00137\tAccuracy: 1.0000\tLoss: 0.128904\n",
      "Train Epoch: 5 [720/9596 (38%)]\tlr:0.00136\tAccuracy: 1.0000\tLoss: 0.057120\n",
      "Train Epoch: 5 [780/9596 (41%)]\tlr:0.00136\tAccuracy: 1.0000\tLoss: 0.018572\n",
      "Train Epoch: 5 [840/9596 (44%)]\tlr:0.00135\tAccuracy: 1.0000\tLoss: 0.182494\n",
      "Train Epoch: 5 [900/9596 (47%)]\tlr:0.00135\tAccuracy: 0.8000\tLoss: 0.285644\n",
      "Train Epoch: 5 [960/9596 (50%)]\tlr:0.00134\tAccuracy: 1.0000\tLoss: 0.073252\n",
      "Train Epoch: 5 [1020/9596 (53%)]\tlr:0.00134\tAccuracy: 1.0000\tLoss: 0.193146\n",
      "Train Epoch: 5 [1080/9596 (56%)]\tlr:0.00134\tAccuracy: 1.0000\tLoss: 0.055253\n",
      "Train Epoch: 5 [1140/9596 (59%)]\tlr:0.00133\tAccuracy: 1.0000\tLoss: 0.150033\n",
      "Train Epoch: 5 [1200/9596 (62%)]\tlr:0.00133\tAccuracy: 1.0000\tLoss: 0.032155\n",
      "Train Epoch: 5 [1260/9596 (66%)]\tlr:0.00132\tAccuracy: 1.0000\tLoss: 0.103330\n",
      "Train Epoch: 5 [1320/9596 (69%)]\tlr:0.00132\tAccuracy: 0.8000\tLoss: 0.354887\n",
      "Train Epoch: 5 [1380/9596 (72%)]\tlr:0.00131\tAccuracy: 1.0000\tLoss: 0.096668\n",
      "Train Epoch: 5 [1440/9596 (75%)]\tlr:0.00131\tAccuracy: 1.0000\tLoss: 0.181332\n",
      "Train Epoch: 5 [1500/9596 (78%)]\tlr:0.00130\tAccuracy: 0.8000\tLoss: 0.209378\n",
      "Train Epoch: 5 [1560/9596 (81%)]\tlr:0.00130\tAccuracy: 0.9000\tLoss: 0.122635\n",
      "Train Epoch: 5 [1620/9596 (84%)]\tlr:0.00130\tAccuracy: 1.0000\tLoss: 0.137552\n",
      "Train Epoch: 5 [1680/9596 (88%)]\tlr:0.00129\tAccuracy: 1.0000\tLoss: 0.115981\n",
      "Train Epoch: 5 [1740/9596 (91%)]\tlr:0.00129\tAccuracy: 1.0000\tLoss: 0.137122\n",
      "Train Epoch: 5 [1800/9596 (94%)]\tlr:0.00128\tAccuracy: 1.0000\tLoss: 0.086920\n",
      "Train Epoch: 5 [1860/9596 (97%)]\tlr:0.00128\tAccuracy: 1.0000\tLoss: 0.083145\n",
      "Starting Validation\n",
      "===> Validation set: Average loss: 2.0710\tAccuracy: 0.2047\n",
      "\n",
      "#### End epoch 5/6, elapsed time: 15.330167000000017\n",
      "Train Epoch: 6 [0/9596 (0%)]\tlr:0.00128\tAccuracy: 1.0000\tLoss: 0.123529\n",
      "Train Epoch: 6 [60/9596 (3%)]\tlr:0.00127\tAccuracy: 1.0000\tLoss: 0.165335\n",
      "Train Epoch: 6 [120/9596 (6%)]\tlr:0.00127\tAccuracy: 1.0000\tLoss: 0.066488\n",
      "Train Epoch: 6 [180/9596 (9%)]\tlr:0.00126\tAccuracy: 1.0000\tLoss: 0.104164\n",
      "Train Epoch: 6 [240/9596 (12%)]\tlr:0.00126\tAccuracy: 1.0000\tLoss: 0.231862\n",
      "Train Epoch: 6 [300/9596 (16%)]\tlr:0.00126\tAccuracy: 1.0000\tLoss: 0.213172\n",
      "Train Epoch: 6 [360/9596 (19%)]\tlr:0.00125\tAccuracy: 1.0000\tLoss: 0.135127\n",
      "Train Epoch: 6 [420/9596 (22%)]\tlr:0.00125\tAccuracy: 1.0000\tLoss: 0.259228\n",
      "Train Epoch: 6 [480/9596 (25%)]\tlr:0.00124\tAccuracy: 1.0000\tLoss: 0.295866\n",
      "Train Epoch: 6 [540/9596 (28%)]\tlr:0.00124\tAccuracy: 1.0000\tLoss: 0.159039\n",
      "Train Epoch: 6 [600/9596 (31%)]\tlr:0.00124\tAccuracy: 1.0000\tLoss: 0.091029\n",
      "Train Epoch: 6 [660/9596 (34%)]\tlr:0.00123\tAccuracy: 0.8000\tLoss: 0.215809\n",
      "Train Epoch: 6 [720/9596 (38%)]\tlr:0.00123\tAccuracy: 1.0000\tLoss: 0.102962\n",
      "Train Epoch: 6 [780/9596 (41%)]\tlr:0.00123\tAccuracy: 1.0000\tLoss: 0.044571\n",
      "Train Epoch: 6 [840/9596 (44%)]\tlr:0.00122\tAccuracy: 1.0000\tLoss: 0.059920\n",
      "Train Epoch: 6 [900/9596 (47%)]\tlr:0.00122\tAccuracy: 1.0000\tLoss: 0.212427\n",
      "Train Epoch: 6 [960/9596 (50%)]\tlr:0.00122\tAccuracy: 1.0000\tLoss: 0.181035\n",
      "Train Epoch: 6 [1020/9596 (53%)]\tlr:0.00121\tAccuracy: 1.0000\tLoss: 0.153231\n",
      "Train Epoch: 6 [1080/9596 (56%)]\tlr:0.00121\tAccuracy: 0.9000\tLoss: 0.285919\n",
      "Train Epoch: 6 [1140/9596 (59%)]\tlr:0.00121\tAccuracy: 1.0000\tLoss: 0.130649\n",
      "Train Epoch: 6 [1200/9596 (62%)]\tlr:0.00120\tAccuracy: 1.0000\tLoss: 0.066114\n",
      "Train Epoch: 6 [1260/9596 (66%)]\tlr:0.00120\tAccuracy: 1.0000\tLoss: 0.126640\n",
      "Train Epoch: 6 [1320/9596 (69%)]\tlr:0.00120\tAccuracy: 1.0000\tLoss: 0.203684\n",
      "Train Epoch: 6 [1380/9596 (72%)]\tlr:0.00119\tAccuracy: 1.0000\tLoss: 0.246151\n",
      "Train Epoch: 6 [1440/9596 (75%)]\tlr:0.00119\tAccuracy: 0.7000\tLoss: 0.209426\n",
      "Train Epoch: 6 [1500/9596 (78%)]\tlr:0.00119\tAccuracy: 1.0000\tLoss: 0.137568\n",
      "Train Epoch: 6 [1560/9596 (81%)]\tlr:0.00118\tAccuracy: 1.0000\tLoss: 0.001676\n",
      "Train Epoch: 6 [1620/9596 (84%)]\tlr:0.00118\tAccuracy: 1.0000\tLoss: 0.118230\n",
      "Train Epoch: 6 [1680/9596 (88%)]\tlr:0.00118\tAccuracy: 1.0000\tLoss: 0.176656\n",
      "Train Epoch: 6 [1740/9596 (91%)]\tlr:0.00117\tAccuracy: 0.9000\tLoss: 0.296649\n",
      "Train Epoch: 6 [1800/9596 (94%)]\tlr:0.00117\tAccuracy: 1.0000\tLoss: 0.099597\n",
      "Train Epoch: 6 [1860/9596 (97%)]\tlr:0.00117\tAccuracy: 1.0000\tLoss: 0.125012\n",
      "Starting Validation\n",
      "===> Validation set: Average loss: 2.0883\tAccuracy: 0.2067\n",
      "\n",
      "Snapshot saved to snapshot/cdc/cdc-2021-04-01_16_21_18-model_best.pth\n",
      "\n",
      "Starting predict\n",
      "#### End epoch 6/6, elapsed time: 19.83603770000002\n",
      "################## Success #########################\n",
      "Total elapsed time: 114.2254227\n"
     ]
    }
   ],
   "source": [
    "epochs = 6\n",
    "n_warmup_steps = 50\n",
    "batch_size = 10              \n",
    "window = 20 \n",
    "timestep = 5    \n",
    "seed = 1     \n",
    "log_interval = 30\n",
    "num_workers = 4\n",
    "pin_memory = False\n",
    "use_cuda =  torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print('use_cuda is', use_cuda)\n",
    "global_timer = timer() # global timer\n",
    "model, train_loader, test_loader, optimizer = model_data()\n",
    "feature_train_loader, feature_test_loader = process()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9596"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}