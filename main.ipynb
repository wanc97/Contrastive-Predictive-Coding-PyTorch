{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cdc-2021-03-21_17_25_54\n"
     ]
    }
   ],
   "source": [
    "## Utilities\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "## Libraries\n",
    "import numpy as np\n",
    "\n",
    "## Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "## Custrom Imports\n",
    "# from src.logger_v1 import setup_logs\n",
    "# from src.data_reader.dataset import RawDataset, ReverseRawDataset, RawXXreverseDataset\n",
    "# from src.training_v1 import trainXXreverse, snapshot\n",
    "# from src.validation_v1 import validationXXreverse\n",
    "# from src.model.model import CDCK5, CDCK6\n",
    "############ Control Center and Hyperparameter ###############\n",
    "run_name = \"cdc\" + time.strftime(\"-%Y-%m-%d_%H_%M_%S\")\n",
    "print(run_name)\n",
    "logger = logging.getLogger(\"cdc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window(a, window):\n",
    "    shape =  (a.shape[0] - window + 1, window) + a.shape[1:]\n",
    "    strides = (a.strides[0],) + a.strides\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides, writeable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScheduledOptim(object):\n",
    "    \"\"\"A simple wrapper class for learning rate scheduling\"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, n_warmup_steps):\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = 128 \n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.n_current_steps = 0 \n",
    "        self.delta = 1\n",
    "\n",
    "    def state_dict(self):\n",
    "        self.optimizer.state_dict()\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Step by the inner optimizer\"\"\"\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"Zero out the gradients by the inner optimizer\"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def increase_delta(self):\n",
    "        self.delta *= 2\n",
    "\n",
    "    def update_learning_rate(self):\n",
    "        \"\"\"Learning rate scheduling per step\"\"\"\n",
    "\n",
    "        self.n_current_steps += self.delta\n",
    "        new_lr = np.power(self.d_model, -0.5) * np.min([\n",
    "            np.power(self.n_current_steps, -0.5),\n",
    "            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n",
    "\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = new_lr\n",
    "        return new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TETrainDataset(data.Dataset):\n",
    "    def __init__(self, fault=[1,2,3,4,5,6], window=20):\n",
    "        \"\"\" fault: [1,2,3,4,5,6]\n",
    "            window: 20\n",
    "        \"\"\"\n",
    "        self.window = window \n",
    "        temp = torch.from_numpy(rolling_window(np.loadtxt('data/d00.dat').T, window))\n",
    "        self.sample = [temp]\n",
    "        self.label = [0 for _ in temp]\n",
    "\n",
    "        for label in fault:\n",
    "            if label < 10:\n",
    "                num = '0' + str(label)\n",
    "            else:\n",
    "                num = str(label)\n",
    "            temp = torch.from_numpy(rolling_window(np.loadtxt('data/d' + num + '.dat'), window))\n",
    "            self.sample.append(temp)\n",
    "            self.label.extend([label for _ in temp])\n",
    "        \n",
    "        self.sample = torch.cat(self.sample,0).float()\n",
    "        self.label = torch.tensor(self.label).float()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sample)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.sample[index], self.label[index]\n",
    "\n",
    "class TETestDataset(data.Dataset):\n",
    "    def __init__(self, fault=[1,2,3,4,5,6], window=20):\n",
    "        \"\"\" fault: [1,2,3,4,5,6]\n",
    "            window: 20\n",
    "        \"\"\"\n",
    "        self.window = window \n",
    "        temp = torch.from_numpy(rolling_window(np.loadtxt('data/d00_te.dat'), window))\n",
    "        self.sample = [temp]\n",
    "        self.label = [0 for _ in temp]\n",
    "\n",
    "        for label in fault:\n",
    "            if label < 10:\n",
    "                num = '0' + str(label)\n",
    "            else:\n",
    "                num = str(label)\n",
    "            temp = torch.from_numpy(rolling_window(np.loadtxt('data/d' + num + '_te.dat'), window))\n",
    "            self.sample.append(temp)\n",
    "            if window <= 160:\n",
    "                self.label.extend([0 for _ in range(160-window+1)])\n",
    "                self.label.extend([label for _ in range(800)])\n",
    "            else:\n",
    "                self.label.extend([label for _ in range(960-window+1)])\n",
    "        \n",
    "        self.sample = torch.cat(self.sample,0).float()\n",
    "        self.label = torch.tensor(self.label).float()\n",
    "        assert self.sample.shape[0] == self.label.shape[0]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sample)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.sample[index], self.label[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPC(nn.Module):\n",
    "    def __init__(self, input_size, timestep, batch_size, seq_len):\n",
    "\n",
    "        super(CPC, self).__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.timestep = timestep\n",
    "        self.encoder = nn.Sequential( # downsampling factor = 160\n",
    "            nn.Linear(input_size,32, bias=False),\n",
    "            nn.BatchNorm1d(self.seq_len+self.timestep),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32,32, bias=False),\n",
    "            nn.BatchNorm1d(self.seq_len+self.timestep),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.gru = nn.GRU(32, 16, num_layers=1, bidirectional=False, batch_first=True)\n",
    "        self.Wk  = nn.ModuleList([nn.Linear(16, 32) for i in range(timestep)])\n",
    "        self.softmax  = nn.Softmax()\n",
    "        self.lsoftmax = nn.LogSoftmax()\n",
    "\n",
    "        def _weights_init(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # initialize gru\n",
    "        for layer_p in self.gru._all_weights:\n",
    "            for p in layer_p:\n",
    "                if 'weight' in p:\n",
    "                    nn.init.kaiming_normal_(self.gru.__getattr__(p), mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def init_hidden(self, batch_size, use_gpu=True):\n",
    "        if use_gpu: return torch.zeros(1, batch_size, 16).cuda()\n",
    "        else: return torch.zeros(1, batch_size, 16)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch = x.size()[0]\n",
    "        # t_samples = torch.randint(self.seq_len/160-self.timestep, size=(1,)).long() # randomly pick time stamps\n",
    "        # input sequence is N*C*L, e.g. 8*1*20480\n",
    "        z = self.encoder(x)\n",
    "        # encoded sequence is N*C*L, e.g. 8*512*128\n",
    "        # reshape to N*L*C for GRU, e.g. 8*128*512\n",
    "        encode_samples = z[:,-1*self.timestep:,:].transpose(0,1)\n",
    "        # z = z.transpose(1,2)\n",
    "        nce = 0 # average over timestep and batch\n",
    "        # encode_samples = torch.empty((self.timestep,batch,512)).float() # e.g. size 12*8*512\n",
    "        # for i in np.arange(1, self.timestep+1):\n",
    "        #     encode_samples[i-1] = z[:,t_samples+i,:].view(batch,512) # z_tk e.g. size 8*512\n",
    "        forward_seq = z[:,:self.seq_len,:] # e.g. size 8*100*512\n",
    "        output, hidden = self.gru(forward_seq, hidden) # output size e.g. 8*100*256\n",
    "        c_t = output[:,-1,:].view(batch, -1) # c_t e.g. size 8*256\n",
    "        pred = torch.empty((self.timestep,batch,32)).float().to(x.device) # e.g. size 12*8*512\n",
    "        for i in np.arange(0, self.timestep):\n",
    "            pred[i] = self.Wk[i](c_t) # Wk*c_t e.g. size 8*512\n",
    "        for i in np.arange(0, self.timestep):\n",
    "            total = torch.mm(encode_samples[i], torch.transpose(pred[i],0,1)) # e.g. size 8*8\n",
    "            correct = torch.sum(torch.eq(torch.argmax(self.softmax(total), dim=0).cpu(), torch.arange(0, batch))) # correct is a tensor\n",
    "            nce += torch.sum(torch.diag(self.lsoftmax(total))) # nce is a tensor\n",
    "        nce /= -1.*batch*self.timestep\n",
    "        accuracy = 1.*correct.item()/batch\n",
    "\n",
    "        return accuracy, nce\n",
    "\n",
    "    def predict(self, x, hidden):\n",
    "        batch = x.size()[0]\n",
    "        # input sequence is N*C*L, e.g. 8*1*20480\n",
    "        z = self.encoder(x)\n",
    "        # encoded sequence is N*C*L, e.g. 8*512*128\n",
    "        # reshape to N*L*C for GRU, e.g. 8*128*512\n",
    "        z = z.transpose(1,2)\n",
    "        output, hidden = self.gru(z, hidden) # output size e.g. 8*128*256\n",
    "\n",
    "        return output, hidden # return every frame\n",
    "        #return output[:,-1,:], hidden # only return the last frame per utt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch, batch_size):\n",
    "    model.train()\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        seq, label = data\n",
    "        seq, label = seq.to(device), label.to(device)\n",
    "        # seq = seq.to(device) # add channel dimension\n",
    "        optimizer.zero_grad()\n",
    "        hidden = model.init_hidden(len(seq), use_gpu=True)\n",
    "        acc, loss = model(seq, hidden)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr = optimizer.update_learning_rate()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            logger.info('Train Epoch: {} [{}/{} ({:.0f}%)]\\tlr:{:.5f}\\tAccuracy: {:.4f}\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), lr, acc, loss.item()))\n",
    "\n",
    "def validation(args, model, device, data_loader, batch_size):\n",
    "    logger.info(\"Starting Validation\")\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_acc  = 0 \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            seq, label = data\n",
    "            seq, label = seq.to(device), label.to(device)\n",
    "            # data = data.float().unsqueeze(1).to(device) # add channel dimension\n",
    "            hidden = model.init_hidden(len(seq), use_gpu=True)\n",
    "            acc, loss = model(seq, hidden)\n",
    "            total_loss += len(seq) * loss \n",
    "            total_acc  += len(seq) * acc\n",
    "\n",
    "    total_loss /= len(data_loader.dataset) # average loss\n",
    "    total_acc  /= len(data_loader.dataset) # average acc\n",
    "\n",
    "    logger.info('===> Validation set: Average loss: {:.4f}\\tAccuracy: {:.4f}\\n'.format(\n",
    "                total_loss, total_acc))\n",
    "\n",
    "    return total_acc, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "===> loading train, validation and eval dataset\n",
      "===> loading train, validation and eval dataset\n",
      "===> loading train, validation and eval dataset\n",
      "===> loading train, validation and eval dataset\n",
      "===> loading train, validation and eval dataset\n",
      "===> loading train, validation and eval dataset\n",
      "===> loading train, validation and eval dataset\n",
      "===> loading train, validation and eval dataset\n",
      "===> loading train, validation and eval dataset\n",
      "===> loading train, validation and eval dataset\n",
      "===> loading train, validation and eval dataset\n",
      "===> loading train, validation and eval dataset\n",
      "===> loading train, validation and eval dataset\n",
      "===> loading train, validation and eval dataset\n",
      "===> loading train, validation and eval dataset\n",
      "===> loading train, validation and eval dataset\n",
      "===> loading train, validation and eval dataset\n",
      "===> loading train, validation and eval dataset\n",
      "===> loading train, validation and eval dataset\n",
      "===> loading train, validation and eval dataset\n",
      "===> loading train, validation and eval dataset\n",
      "===> loading train, validation and eval dataset\n",
      "===> loading train, validation and eval dataset\n",
      "use_cuda is True\n",
      "### Model summary below###\n",
      " CPC(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=52, out_features=32, bias=False)\n",
      "    (1): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=32, out_features=32, bias=False)\n",
      "    (4): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (gru): GRU(32, 16, batch_first=True)\n",
      "  (Wk): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (3): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (4): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (5): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (6): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (7): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (8): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (9): Linear(in_features=16, out_features=32, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=None)\n",
      "  (lsoftmax): LogSoftmax(dim=None)\n",
      ")\n",
      "\n",
      "### Model summary below###\n",
      " CPC(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=52, out_features=32, bias=False)\n",
      "    (1): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=32, out_features=32, bias=False)\n",
      "    (4): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (gru): GRU(32, 16, batch_first=True)\n",
      "  (Wk): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (3): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (4): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (5): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (6): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (7): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (8): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (9): Linear(in_features=16, out_features=32, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=None)\n",
      "  (lsoftmax): LogSoftmax(dim=None)\n",
      ")\n",
      "\n",
      "### Model summary below###\n",
      " CPC(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=52, out_features=32, bias=False)\n",
      "    (1): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=32, out_features=32, bias=False)\n",
      "    (4): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (gru): GRU(32, 16, batch_first=True)\n",
      "  (Wk): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (3): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (4): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (5): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (6): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (7): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (8): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (9): Linear(in_features=16, out_features=32, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=None)\n",
      "  (lsoftmax): LogSoftmax(dim=None)\n",
      ")\n",
      "\n",
      "### Model summary below###\n",
      " CPC(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=52, out_features=32, bias=False)\n",
      "    (1): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=32, out_features=32, bias=False)\n",
      "    (4): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (gru): GRU(32, 16, batch_first=True)\n",
      "  (Wk): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (3): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (4): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (5): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (6): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (7): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (8): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (9): Linear(in_features=16, out_features=32, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=None)\n",
      "  (lsoftmax): LogSoftmax(dim=None)\n",
      ")\n",
      "\n",
      "### Model summary below###\n",
      " CPC(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=52, out_features=32, bias=False)\n",
      "    (1): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=32, out_features=32, bias=False)\n",
      "    (4): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (gru): GRU(32, 16, batch_first=True)\n",
      "  (Wk): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (3): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (4): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (5): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (6): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (7): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (8): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (9): Linear(in_features=16, out_features=32, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=None)\n",
      "  (lsoftmax): LogSoftmax(dim=None)\n",
      ")\n",
      "\n",
      "### Model summary below###\n",
      " CPC(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=52, out_features=32, bias=False)\n",
      "    (1): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=32, out_features=32, bias=False)\n",
      "    (4): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (gru): GRU(32, 16, batch_first=True)\n",
      "  (Wk): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (3): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (4): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (5): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (6): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (7): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (8): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (9): Linear(in_features=16, out_features=32, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=None)\n",
      "  (lsoftmax): LogSoftmax(dim=None)\n",
      ")\n",
      "\n",
      "### Model summary below###\n",
      " CPC(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=52, out_features=32, bias=False)\n",
      "    (1): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=32, out_features=32, bias=False)\n",
      "    (4): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (gru): GRU(32, 16, batch_first=True)\n",
      "  (Wk): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (3): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (4): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (5): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (6): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (7): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (8): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (9): Linear(in_features=16, out_features=32, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=None)\n",
      "  (lsoftmax): LogSoftmax(dim=None)\n",
      ")\n",
      "\n",
      "### Model summary below###\n",
      " CPC(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=52, out_features=32, bias=False)\n",
      "    (1): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=32, out_features=32, bias=False)\n",
      "    (4): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (gru): GRU(32, 16, batch_first=True)\n",
      "  (Wk): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (3): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (4): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (5): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (6): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (7): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (8): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (9): Linear(in_features=16, out_features=32, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=None)\n",
      "  (lsoftmax): LogSoftmax(dim=None)\n",
      ")\n",
      "\n",
      "### Model summary below###\n",
      " CPC(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=52, out_features=32, bias=False)\n",
      "    (1): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=32, out_features=32, bias=False)\n",
      "    (4): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (gru): GRU(32, 16, batch_first=True)\n",
      "  (Wk): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (3): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (4): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (5): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (6): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (7): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (8): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (9): Linear(in_features=16, out_features=32, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=None)\n",
      "  (lsoftmax): LogSoftmax(dim=None)\n",
      ")\n",
      "\n",
      "### Model summary below###\n",
      " CPC(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=52, out_features=32, bias=False)\n",
      "    (1): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=32, out_features=32, bias=False)\n",
      "    (4): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (gru): GRU(32, 16, batch_first=True)\n",
      "  (Wk): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (3): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (4): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (5): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (6): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (7): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (8): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (9): Linear(in_features=16, out_features=32, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=None)\n",
      "  (lsoftmax): LogSoftmax(dim=None)\n",
      ")\n",
      "\n",
      "### Model summary below###\n",
      " CPC(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=52, out_features=32, bias=False)\n",
      "    (1): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=32, out_features=32, bias=False)\n",
      "    (4): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (gru): GRU(32, 16, batch_first=True)\n",
      "  (Wk): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (3): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (4): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (5): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (6): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (7): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (8): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (9): Linear(in_features=16, out_features=32, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=None)\n",
      "  (lsoftmax): LogSoftmax(dim=None)\n",
      ")\n",
      "\n",
      "### Model summary below###\n",
      " CPC(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=52, out_features=32, bias=False)\n",
      "    (1): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=32, out_features=32, bias=False)\n",
      "    (4): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (gru): GRU(32, 16, batch_first=True)\n",
      "  (Wk): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (3): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (4): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (5): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (6): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (7): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (8): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (9): Linear(in_features=16, out_features=32, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=None)\n",
      "  (lsoftmax): LogSoftmax(dim=None)\n",
      ")\n",
      "\n",
      "### Model summary below###\n",
      " CPC(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=52, out_features=32, bias=False)\n",
      "    (1): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=32, out_features=32, bias=False)\n",
      "    (4): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (gru): GRU(32, 16, batch_first=True)\n",
      "  (Wk): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (3): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (4): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (5): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (6): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (7): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (8): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (9): Linear(in_features=16, out_features=32, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=None)\n",
      "  (lsoftmax): LogSoftmax(dim=None)\n",
      ")\n",
      "\n",
      "### Model summary below###\n",
      " CPC(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=52, out_features=32, bias=False)\n",
      "    (1): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=32, out_features=32, bias=False)\n",
      "    (4): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (gru): GRU(32, 16, batch_first=True)\n",
      "  (Wk): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (3): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (4): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (5): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (6): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (7): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (8): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (9): Linear(in_features=16, out_features=32, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=None)\n",
      "  (lsoftmax): LogSoftmax(dim=None)\n",
      ")\n",
      "\n",
      "### Model summary below###\n",
      " CPC(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=52, out_features=32, bias=False)\n",
      "    (1): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=32, out_features=32, bias=False)\n",
      "    (4): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (gru): GRU(32, 16, batch_first=True)\n",
      "  (Wk): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (3): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (4): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (5): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (6): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (7): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (8): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (9): Linear(in_features=16, out_features=32, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=None)\n",
      "  (lsoftmax): LogSoftmax(dim=None)\n",
      ")\n",
      "\n",
      "### Model summary below###\n",
      " CPC(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=52, out_features=32, bias=False)\n",
      "    (1): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=32, out_features=32, bias=False)\n",
      "    (4): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (gru): GRU(32, 16, batch_first=True)\n",
      "  (Wk): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (3): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (4): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (5): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (6): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (7): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (8): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (9): Linear(in_features=16, out_features=32, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=None)\n",
      "  (lsoftmax): LogSoftmax(dim=None)\n",
      ")\n",
      "\n",
      "### Model summary below###\n",
      " CPC(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=52, out_features=32, bias=False)\n",
      "    (1): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=32, out_features=32, bias=False)\n",
      "    (4): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (gru): GRU(32, 16, batch_first=True)\n",
      "  (Wk): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (3): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (4): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (5): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (6): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (7): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (8): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (9): Linear(in_features=16, out_features=32, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=None)\n",
      "  (lsoftmax): LogSoftmax(dim=None)\n",
      ")\n",
      "\n",
      "### Model summary below###\n",
      " CPC(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=52, out_features=32, bias=False)\n",
      "    (1): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=32, out_features=32, bias=False)\n",
      "    (4): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (gru): GRU(32, 16, batch_first=True)\n",
      "  (Wk): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (3): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (4): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (5): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (6): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (7): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (8): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (9): Linear(in_features=16, out_features=32, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=None)\n",
      "  (lsoftmax): LogSoftmax(dim=None)\n",
      ")\n",
      "\n",
      "### Model summary below###\n",
      " CPC(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=52, out_features=32, bias=False)\n",
      "    (1): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=32, out_features=32, bias=False)\n",
      "    (4): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (gru): GRU(32, 16, batch_first=True)\n",
      "  (Wk): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (3): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (4): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (5): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (6): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (7): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (8): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (9): Linear(in_features=16, out_features=32, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=None)\n",
      "  (lsoftmax): LogSoftmax(dim=None)\n",
      ")\n",
      "\n",
      "### Model summary below###\n",
      " CPC(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=52, out_features=32, bias=False)\n",
      "    (1): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=32, out_features=32, bias=False)\n",
      "    (4): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (gru): GRU(32, 16, batch_first=True)\n",
      "  (Wk): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (3): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (4): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (5): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (6): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (7): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (8): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (9): Linear(in_features=16, out_features=32, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=None)\n",
      "  (lsoftmax): LogSoftmax(dim=None)\n",
      ")\n",
      "\n",
      "### Model summary below###\n",
      " CPC(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=52, out_features=32, bias=False)\n",
      "    (1): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=32, out_features=32, bias=False)\n",
      "    (4): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (gru): GRU(32, 16, batch_first=True)\n",
      "  (Wk): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (3): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (4): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (5): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (6): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (7): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (8): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (9): Linear(in_features=16, out_features=32, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=None)\n",
      "  (lsoftmax): LogSoftmax(dim=None)\n",
      ")\n",
      "\n",
      "### Model summary below###\n",
      " CPC(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=52, out_features=32, bias=False)\n",
      "    (1): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=32, out_features=32, bias=False)\n",
      "    (4): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (gru): GRU(32, 16, batch_first=True)\n",
      "  (Wk): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (3): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (4): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (5): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (6): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (7): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (8): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (9): Linear(in_features=16, out_features=32, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=None)\n",
      "  (lsoftmax): LogSoftmax(dim=None)\n",
      ")\n",
      "\n",
      "### Model summary below###\n",
      " CPC(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=52, out_features=32, bias=False)\n",
      "    (1): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=32, out_features=32, bias=False)\n",
      "    (4): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (gru): GRU(32, 16, batch_first=True)\n",
      "  (Wk): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (3): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (4): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (5): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (6): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (7): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (8): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (9): Linear(in_features=16, out_features=32, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=None)\n",
      "  (lsoftmax): LogSoftmax(dim=None)\n",
      ")\n",
      "\n",
      "===> Model total parameter: 10648\n",
      "\n",
      "===> Model total parameter: 10648\n",
      "\n",
      "===> Model total parameter: 10648\n",
      "\n",
      "===> Model total parameter: 10648\n",
      "\n",
      "===> Model total parameter: 10648\n",
      "\n",
      "===> Model total parameter: 10648\n",
      "\n",
      "===> Model total parameter: 10648\n",
      "\n",
      "===> Model total parameter: 10648\n",
      "\n",
      "===> Model total parameter: 10648\n",
      "\n",
      "===> Model total parameter: 10648\n",
      "\n",
      "===> Model total parameter: 10648\n",
      "\n",
      "===> Model total parameter: 10648\n",
      "\n",
      "===> Model total parameter: 10648\n",
      "\n",
      "===> Model total parameter: 10648\n",
      "\n",
      "===> Model total parameter: 10648\n",
      "\n",
      "===> Model total parameter: 10648\n",
      "\n",
      "===> Model total parameter: 10648\n",
      "\n",
      "===> Model total parameter: 10648\n",
      "\n",
      "===> Model total parameter: 10648\n",
      "\n",
      "===> Model total parameter: 10648\n",
      "\n",
      "===> Model total parameter: 10648\n",
      "\n",
      "===> Model total parameter: 10648\n",
      "\n",
      "===> Model total parameter: 10648\n",
      "\n",
      "<ipython-input-158-cb37e5a1efc0>:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  correct = torch.sum(torch.eq(torch.argmax(self.softmax(total), dim=0).cpu(), torch.arange(0, batch))) # correct is a tensor\n",
      "<ipython-input-158-cb37e5a1efc0>:63: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  nce += torch.sum(torch.diag(self.lsoftmax(total))) # nce is a tensor\n",
      "Train Epoch: 1 [0/3177 (0%)]\tlr:0.00025\tAccuracy: 0.0000\tLoss: 2.303432\n",
      "Train Epoch: 1 [0/3177 (0%)]\tlr:0.00025\tAccuracy: 0.0000\tLoss: 2.303432\n",
      "Train Epoch: 1 [0/3177 (0%)]\tlr:0.00025\tAccuracy: 0.0000\tLoss: 2.303432\n",
      "Train Epoch: 1 [0/3177 (0%)]\tlr:0.00025\tAccuracy: 0.0000\tLoss: 2.303432\n",
      "Train Epoch: 1 [0/3177 (0%)]\tlr:0.00025\tAccuracy: 0.0000\tLoss: 2.303432\n",
      "Train Epoch: 1 [0/3177 (0%)]\tlr:0.00025\tAccuracy: 0.0000\tLoss: 2.303432\n",
      "Train Epoch: 1 [0/3177 (0%)]\tlr:0.00025\tAccuracy: 0.0000\tLoss: 2.303432\n",
      "Train Epoch: 1 [0/3177 (0%)]\tlr:0.00025\tAccuracy: 0.0000\tLoss: 2.303432\n",
      "Train Epoch: 1 [0/3177 (0%)]\tlr:0.00025\tAccuracy: 0.0000\tLoss: 2.303432\n",
      "Train Epoch: 1 [0/3177 (0%)]\tlr:0.00025\tAccuracy: 0.0000\tLoss: 2.303432\n",
      "Train Epoch: 1 [0/3177 (0%)]\tlr:0.00025\tAccuracy: 0.0000\tLoss: 2.303432\n",
      "Train Epoch: 1 [0/3177 (0%)]\tlr:0.00025\tAccuracy: 0.0000\tLoss: 2.303432\n",
      "Train Epoch: 1 [0/3177 (0%)]\tlr:0.00025\tAccuracy: 0.0000\tLoss: 2.303432\n",
      "Train Epoch: 1 [0/3177 (0%)]\tlr:0.00025\tAccuracy: 0.0000\tLoss: 2.303432\n",
      "Train Epoch: 1 [0/3177 (0%)]\tlr:0.00025\tAccuracy: 0.0000\tLoss: 2.303432\n",
      "Train Epoch: 1 [0/3177 (0%)]\tlr:0.00025\tAccuracy: 0.0000\tLoss: 2.303432\n",
      "Train Epoch: 1 [0/3177 (0%)]\tlr:0.00025\tAccuracy: 0.0000\tLoss: 2.303432\n",
      "Train Epoch: 1 [0/3177 (0%)]\tlr:0.00025\tAccuracy: 0.0000\tLoss: 2.303432\n",
      "Train Epoch: 1 [0/3177 (0%)]\tlr:0.00025\tAccuracy: 0.0000\tLoss: 2.303432\n",
      "Train Epoch: 1 [0/3177 (0%)]\tlr:0.00025\tAccuracy: 0.0000\tLoss: 2.303432\n",
      "Train Epoch: 1 [0/3177 (0%)]\tlr:0.00025\tAccuracy: 0.0000\tLoss: 2.303432\n",
      "Train Epoch: 1 [0/3177 (0%)]\tlr:0.00025\tAccuracy: 0.0000\tLoss: 2.303432\n",
      "Train Epoch: 1 [0/3177 (0%)]\tlr:0.00025\tAccuracy: 0.0000\tLoss: 2.303432\n",
      "Train Epoch: 1 [20/3177 (3%)]\tlr:0.00275\tAccuracy: 0.1000\tLoss: 2.302674\n",
      "Train Epoch: 1 [20/3177 (3%)]\tlr:0.00275\tAccuracy: 0.1000\tLoss: 2.302674\n",
      "Train Epoch: 1 [20/3177 (3%)]\tlr:0.00275\tAccuracy: 0.1000\tLoss: 2.302674\n",
      "Train Epoch: 1 [20/3177 (3%)]\tlr:0.00275\tAccuracy: 0.1000\tLoss: 2.302674\n",
      "Train Epoch: 1 [20/3177 (3%)]\tlr:0.00275\tAccuracy: 0.1000\tLoss: 2.302674\n",
      "Train Epoch: 1 [20/3177 (3%)]\tlr:0.00275\tAccuracy: 0.1000\tLoss: 2.302674\n",
      "Train Epoch: 1 [20/3177 (3%)]\tlr:0.00275\tAccuracy: 0.1000\tLoss: 2.302674\n",
      "Train Epoch: 1 [20/3177 (3%)]\tlr:0.00275\tAccuracy: 0.1000\tLoss: 2.302674\n",
      "Train Epoch: 1 [20/3177 (3%)]\tlr:0.00275\tAccuracy: 0.1000\tLoss: 2.302674\n",
      "Train Epoch: 1 [20/3177 (3%)]\tlr:0.00275\tAccuracy: 0.1000\tLoss: 2.302674\n",
      "Train Epoch: 1 [20/3177 (3%)]\tlr:0.00275\tAccuracy: 0.1000\tLoss: 2.302674\n",
      "Train Epoch: 1 [20/3177 (3%)]\tlr:0.00275\tAccuracy: 0.1000\tLoss: 2.302674\n",
      "Train Epoch: 1 [20/3177 (3%)]\tlr:0.00275\tAccuracy: 0.1000\tLoss: 2.302674\n",
      "Train Epoch: 1 [20/3177 (3%)]\tlr:0.00275\tAccuracy: 0.1000\tLoss: 2.302674\n",
      "Train Epoch: 1 [20/3177 (3%)]\tlr:0.00275\tAccuracy: 0.1000\tLoss: 2.302674\n",
      "Train Epoch: 1 [20/3177 (3%)]\tlr:0.00275\tAccuracy: 0.1000\tLoss: 2.302674\n",
      "Train Epoch: 1 [20/3177 (3%)]\tlr:0.00275\tAccuracy: 0.1000\tLoss: 2.302674\n",
      "Train Epoch: 1 [20/3177 (3%)]\tlr:0.00275\tAccuracy: 0.1000\tLoss: 2.302674\n",
      "Train Epoch: 1 [20/3177 (3%)]\tlr:0.00275\tAccuracy: 0.1000\tLoss: 2.302674\n",
      "Train Epoch: 1 [20/3177 (3%)]\tlr:0.00275\tAccuracy: 0.1000\tLoss: 2.302674\n",
      "Train Epoch: 1 [20/3177 (3%)]\tlr:0.00275\tAccuracy: 0.1000\tLoss: 2.302674\n",
      "Train Epoch: 1 [20/3177 (3%)]\tlr:0.00275\tAccuracy: 0.1000\tLoss: 2.302674\n",
      "Train Epoch: 1 [20/3177 (3%)]\tlr:0.00275\tAccuracy: 0.1000\tLoss: 2.302674\n",
      "Train Epoch: 1 [40/3177 (6%)]\tlr:0.00525\tAccuracy: 0.1000\tLoss: 2.302302\n",
      "Train Epoch: 1 [40/3177 (6%)]\tlr:0.00525\tAccuracy: 0.1000\tLoss: 2.302302\n",
      "Train Epoch: 1 [40/3177 (6%)]\tlr:0.00525\tAccuracy: 0.1000\tLoss: 2.302302\n",
      "Train Epoch: 1 [40/3177 (6%)]\tlr:0.00525\tAccuracy: 0.1000\tLoss: 2.302302\n",
      "Train Epoch: 1 [40/3177 (6%)]\tlr:0.00525\tAccuracy: 0.1000\tLoss: 2.302302\n",
      "Train Epoch: 1 [40/3177 (6%)]\tlr:0.00525\tAccuracy: 0.1000\tLoss: 2.302302\n",
      "Train Epoch: 1 [40/3177 (6%)]\tlr:0.00525\tAccuracy: 0.1000\tLoss: 2.302302\n",
      "Train Epoch: 1 [40/3177 (6%)]\tlr:0.00525\tAccuracy: 0.1000\tLoss: 2.302302\n",
      "Train Epoch: 1 [40/3177 (6%)]\tlr:0.00525\tAccuracy: 0.1000\tLoss: 2.302302\n",
      "Train Epoch: 1 [40/3177 (6%)]\tlr:0.00525\tAccuracy: 0.1000\tLoss: 2.302302\n",
      "Train Epoch: 1 [40/3177 (6%)]\tlr:0.00525\tAccuracy: 0.1000\tLoss: 2.302302\n",
      "Train Epoch: 1 [40/3177 (6%)]\tlr:0.00525\tAccuracy: 0.1000\tLoss: 2.302302\n",
      "Train Epoch: 1 [40/3177 (6%)]\tlr:0.00525\tAccuracy: 0.1000\tLoss: 2.302302\n",
      "Train Epoch: 1 [40/3177 (6%)]\tlr:0.00525\tAccuracy: 0.1000\tLoss: 2.302302\n",
      "Train Epoch: 1 [40/3177 (6%)]\tlr:0.00525\tAccuracy: 0.1000\tLoss: 2.302302\n",
      "Train Epoch: 1 [40/3177 (6%)]\tlr:0.00525\tAccuracy: 0.1000\tLoss: 2.302302\n",
      "Train Epoch: 1 [40/3177 (6%)]\tlr:0.00525\tAccuracy: 0.1000\tLoss: 2.302302\n",
      "Train Epoch: 1 [40/3177 (6%)]\tlr:0.00525\tAccuracy: 0.1000\tLoss: 2.302302\n",
      "Train Epoch: 1 [40/3177 (6%)]\tlr:0.00525\tAccuracy: 0.1000\tLoss: 2.302302\n",
      "Train Epoch: 1 [40/3177 (6%)]\tlr:0.00525\tAccuracy: 0.1000\tLoss: 2.302302\n",
      "Train Epoch: 1 [40/3177 (6%)]\tlr:0.00525\tAccuracy: 0.1000\tLoss: 2.302302\n",
      "Train Epoch: 1 [40/3177 (6%)]\tlr:0.00525\tAccuracy: 0.1000\tLoss: 2.302302\n",
      "Train Epoch: 1 [40/3177 (6%)]\tlr:0.00525\tAccuracy: 0.1000\tLoss: 2.302302\n",
      "Train Epoch: 1 [60/3177 (9%)]\tlr:0.00775\tAccuracy: 0.1000\tLoss: 2.299911\n",
      "Train Epoch: 1 [60/3177 (9%)]\tlr:0.00775\tAccuracy: 0.1000\tLoss: 2.299911\n",
      "Train Epoch: 1 [60/3177 (9%)]\tlr:0.00775\tAccuracy: 0.1000\tLoss: 2.299911\n",
      "Train Epoch: 1 [60/3177 (9%)]\tlr:0.00775\tAccuracy: 0.1000\tLoss: 2.299911\n",
      "Train Epoch: 1 [60/3177 (9%)]\tlr:0.00775\tAccuracy: 0.1000\tLoss: 2.299911\n",
      "Train Epoch: 1 [60/3177 (9%)]\tlr:0.00775\tAccuracy: 0.1000\tLoss: 2.299911\n",
      "Train Epoch: 1 [60/3177 (9%)]\tlr:0.00775\tAccuracy: 0.1000\tLoss: 2.299911\n",
      "Train Epoch: 1 [60/3177 (9%)]\tlr:0.00775\tAccuracy: 0.1000\tLoss: 2.299911\n",
      "Train Epoch: 1 [60/3177 (9%)]\tlr:0.00775\tAccuracy: 0.1000\tLoss: 2.299911\n",
      "Train Epoch: 1 [60/3177 (9%)]\tlr:0.00775\tAccuracy: 0.1000\tLoss: 2.299911\n",
      "Train Epoch: 1 [60/3177 (9%)]\tlr:0.00775\tAccuracy: 0.1000\tLoss: 2.299911\n",
      "Train Epoch: 1 [60/3177 (9%)]\tlr:0.00775\tAccuracy: 0.1000\tLoss: 2.299911\n",
      "Train Epoch: 1 [60/3177 (9%)]\tlr:0.00775\tAccuracy: 0.1000\tLoss: 2.299911\n",
      "Train Epoch: 1 [60/3177 (9%)]\tlr:0.00775\tAccuracy: 0.1000\tLoss: 2.299911\n",
      "Train Epoch: 1 [60/3177 (9%)]\tlr:0.00775\tAccuracy: 0.1000\tLoss: 2.299911\n",
      "Train Epoch: 1 [60/3177 (9%)]\tlr:0.00775\tAccuracy: 0.1000\tLoss: 2.299911\n",
      "Train Epoch: 1 [60/3177 (9%)]\tlr:0.00775\tAccuracy: 0.1000\tLoss: 2.299911\n",
      "Train Epoch: 1 [60/3177 (9%)]\tlr:0.00775\tAccuracy: 0.1000\tLoss: 2.299911\n",
      "Train Epoch: 1 [60/3177 (9%)]\tlr:0.00775\tAccuracy: 0.1000\tLoss: 2.299911\n",
      "Train Epoch: 1 [60/3177 (9%)]\tlr:0.00775\tAccuracy: 0.1000\tLoss: 2.299911\n",
      "Train Epoch: 1 [60/3177 (9%)]\tlr:0.00775\tAccuracy: 0.1000\tLoss: 2.299911\n",
      "Train Epoch: 1 [60/3177 (9%)]\tlr:0.00775\tAccuracy: 0.1000\tLoss: 2.299911\n",
      "Train Epoch: 1 [60/3177 (9%)]\tlr:0.00775\tAccuracy: 0.1000\tLoss: 2.299911\n",
      "Train Epoch: 1 [80/3177 (13%)]\tlr:0.01025\tAccuracy: 0.3000\tLoss: 1.758222\n",
      "Train Epoch: 1 [80/3177 (13%)]\tlr:0.01025\tAccuracy: 0.3000\tLoss: 1.758222\n",
      "Train Epoch: 1 [80/3177 (13%)]\tlr:0.01025\tAccuracy: 0.3000\tLoss: 1.758222\n",
      "Train Epoch: 1 [80/3177 (13%)]\tlr:0.01025\tAccuracy: 0.3000\tLoss: 1.758222\n",
      "Train Epoch: 1 [80/3177 (13%)]\tlr:0.01025\tAccuracy: 0.3000\tLoss: 1.758222\n",
      "Train Epoch: 1 [80/3177 (13%)]\tlr:0.01025\tAccuracy: 0.3000\tLoss: 1.758222\n",
      "Train Epoch: 1 [80/3177 (13%)]\tlr:0.01025\tAccuracy: 0.3000\tLoss: 1.758222\n",
      "Train Epoch: 1 [80/3177 (13%)]\tlr:0.01025\tAccuracy: 0.3000\tLoss: 1.758222\n",
      "Train Epoch: 1 [80/3177 (13%)]\tlr:0.01025\tAccuracy: 0.3000\tLoss: 1.758222\n",
      "Train Epoch: 1 [80/3177 (13%)]\tlr:0.01025\tAccuracy: 0.3000\tLoss: 1.758222\n",
      "Train Epoch: 1 [80/3177 (13%)]\tlr:0.01025\tAccuracy: 0.3000\tLoss: 1.758222\n",
      "Train Epoch: 1 [80/3177 (13%)]\tlr:0.01025\tAccuracy: 0.3000\tLoss: 1.758222\n",
      "Train Epoch: 1 [80/3177 (13%)]\tlr:0.01025\tAccuracy: 0.3000\tLoss: 1.758222\n",
      "Train Epoch: 1 [80/3177 (13%)]\tlr:0.01025\tAccuracy: 0.3000\tLoss: 1.758222\n",
      "Train Epoch: 1 [80/3177 (13%)]\tlr:0.01025\tAccuracy: 0.3000\tLoss: 1.758222\n",
      "Train Epoch: 1 [80/3177 (13%)]\tlr:0.01025\tAccuracy: 0.3000\tLoss: 1.758222\n",
      "Train Epoch: 1 [80/3177 (13%)]\tlr:0.01025\tAccuracy: 0.3000\tLoss: 1.758222\n",
      "Train Epoch: 1 [80/3177 (13%)]\tlr:0.01025\tAccuracy: 0.3000\tLoss: 1.758222\n",
      "Train Epoch: 1 [80/3177 (13%)]\tlr:0.01025\tAccuracy: 0.3000\tLoss: 1.758222\n",
      "Train Epoch: 1 [80/3177 (13%)]\tlr:0.01025\tAccuracy: 0.3000\tLoss: 1.758222\n",
      "Train Epoch: 1 [80/3177 (13%)]\tlr:0.01025\tAccuracy: 0.3000\tLoss: 1.758222\n",
      "Train Epoch: 1 [80/3177 (13%)]\tlr:0.01025\tAccuracy: 0.3000\tLoss: 1.758222\n",
      "Train Epoch: 1 [80/3177 (13%)]\tlr:0.01025\tAccuracy: 0.3000\tLoss: 1.758222\n",
      "Train Epoch: 1 [100/3177 (16%)]\tlr:0.01238\tAccuracy: 0.2000\tLoss: 2.265812\n",
      "Train Epoch: 1 [100/3177 (16%)]\tlr:0.01238\tAccuracy: 0.2000\tLoss: 2.265812\n",
      "Train Epoch: 1 [100/3177 (16%)]\tlr:0.01238\tAccuracy: 0.2000\tLoss: 2.265812\n",
      "Train Epoch: 1 [100/3177 (16%)]\tlr:0.01238\tAccuracy: 0.2000\tLoss: 2.265812\n",
      "Train Epoch: 1 [100/3177 (16%)]\tlr:0.01238\tAccuracy: 0.2000\tLoss: 2.265812\n",
      "Train Epoch: 1 [100/3177 (16%)]\tlr:0.01238\tAccuracy: 0.2000\tLoss: 2.265812\n",
      "Train Epoch: 1 [100/3177 (16%)]\tlr:0.01238\tAccuracy: 0.2000\tLoss: 2.265812\n",
      "Train Epoch: 1 [100/3177 (16%)]\tlr:0.01238\tAccuracy: 0.2000\tLoss: 2.265812\n",
      "Train Epoch: 1 [100/3177 (16%)]\tlr:0.01238\tAccuracy: 0.2000\tLoss: 2.265812\n",
      "Train Epoch: 1 [100/3177 (16%)]\tlr:0.01238\tAccuracy: 0.2000\tLoss: 2.265812\n",
      "Train Epoch: 1 [100/3177 (16%)]\tlr:0.01238\tAccuracy: 0.2000\tLoss: 2.265812\n",
      "Train Epoch: 1 [100/3177 (16%)]\tlr:0.01238\tAccuracy: 0.2000\tLoss: 2.265812\n",
      "Train Epoch: 1 [100/3177 (16%)]\tlr:0.01238\tAccuracy: 0.2000\tLoss: 2.265812\n",
      "Train Epoch: 1 [100/3177 (16%)]\tlr:0.01238\tAccuracy: 0.2000\tLoss: 2.265812\n",
      "Train Epoch: 1 [100/3177 (16%)]\tlr:0.01238\tAccuracy: 0.2000\tLoss: 2.265812\n",
      "Train Epoch: 1 [100/3177 (16%)]\tlr:0.01238\tAccuracy: 0.2000\tLoss: 2.265812\n",
      "Train Epoch: 1 [100/3177 (16%)]\tlr:0.01238\tAccuracy: 0.2000\tLoss: 2.265812\n",
      "Train Epoch: 1 [100/3177 (16%)]\tlr:0.01238\tAccuracy: 0.2000\tLoss: 2.265812\n",
      "Train Epoch: 1 [100/3177 (16%)]\tlr:0.01238\tAccuracy: 0.2000\tLoss: 2.265812\n",
      "Train Epoch: 1 [100/3177 (16%)]\tlr:0.01238\tAccuracy: 0.2000\tLoss: 2.265812\n",
      "Train Epoch: 1 [100/3177 (16%)]\tlr:0.01238\tAccuracy: 0.2000\tLoss: 2.265812\n",
      "Train Epoch: 1 [100/3177 (16%)]\tlr:0.01238\tAccuracy: 0.2000\tLoss: 2.265812\n",
      "Train Epoch: 1 [100/3177 (16%)]\tlr:0.01238\tAccuracy: 0.2000\tLoss: 2.265812\n",
      "Train Epoch: 1 [120/3177 (19%)]\tlr:0.01132\tAccuracy: 0.5000\tLoss: 1.697410\n",
      "Train Epoch: 1 [120/3177 (19%)]\tlr:0.01132\tAccuracy: 0.5000\tLoss: 1.697410\n",
      "Train Epoch: 1 [120/3177 (19%)]\tlr:0.01132\tAccuracy: 0.5000\tLoss: 1.697410\n",
      "Train Epoch: 1 [120/3177 (19%)]\tlr:0.01132\tAccuracy: 0.5000\tLoss: 1.697410\n",
      "Train Epoch: 1 [120/3177 (19%)]\tlr:0.01132\tAccuracy: 0.5000\tLoss: 1.697410\n",
      "Train Epoch: 1 [120/3177 (19%)]\tlr:0.01132\tAccuracy: 0.5000\tLoss: 1.697410\n",
      "Train Epoch: 1 [120/3177 (19%)]\tlr:0.01132\tAccuracy: 0.5000\tLoss: 1.697410\n",
      "Train Epoch: 1 [120/3177 (19%)]\tlr:0.01132\tAccuracy: 0.5000\tLoss: 1.697410\n",
      "Train Epoch: 1 [120/3177 (19%)]\tlr:0.01132\tAccuracy: 0.5000\tLoss: 1.697410\n",
      "Train Epoch: 1 [120/3177 (19%)]\tlr:0.01132\tAccuracy: 0.5000\tLoss: 1.697410\n",
      "Train Epoch: 1 [120/3177 (19%)]\tlr:0.01132\tAccuracy: 0.5000\tLoss: 1.697410\n",
      "Train Epoch: 1 [120/3177 (19%)]\tlr:0.01132\tAccuracy: 0.5000\tLoss: 1.697410\n",
      "Train Epoch: 1 [120/3177 (19%)]\tlr:0.01132\tAccuracy: 0.5000\tLoss: 1.697410\n",
      "Train Epoch: 1 [120/3177 (19%)]\tlr:0.01132\tAccuracy: 0.5000\tLoss: 1.697410\n",
      "Train Epoch: 1 [120/3177 (19%)]\tlr:0.01132\tAccuracy: 0.5000\tLoss: 1.697410\n",
      "Train Epoch: 1 [120/3177 (19%)]\tlr:0.01132\tAccuracy: 0.5000\tLoss: 1.697410\n",
      "Train Epoch: 1 [120/3177 (19%)]\tlr:0.01132\tAccuracy: 0.5000\tLoss: 1.697410\n",
      "Train Epoch: 1 [120/3177 (19%)]\tlr:0.01132\tAccuracy: 0.5000\tLoss: 1.697410\n",
      "Train Epoch: 1 [120/3177 (19%)]\tlr:0.01132\tAccuracy: 0.5000\tLoss: 1.697410\n",
      "Train Epoch: 1 [120/3177 (19%)]\tlr:0.01132\tAccuracy: 0.5000\tLoss: 1.697410\n",
      "Train Epoch: 1 [120/3177 (19%)]\tlr:0.01132\tAccuracy: 0.5000\tLoss: 1.697410\n",
      "Train Epoch: 1 [120/3177 (19%)]\tlr:0.01132\tAccuracy: 0.5000\tLoss: 1.697410\n",
      "Train Epoch: 1 [120/3177 (19%)]\tlr:0.01132\tAccuracy: 0.5000\tLoss: 1.697410\n",
      "Train Epoch: 1 [140/3177 (22%)]\tlr:0.01049\tAccuracy: 0.3000\tLoss: 2.299083\n",
      "Train Epoch: 1 [140/3177 (22%)]\tlr:0.01049\tAccuracy: 0.3000\tLoss: 2.299083\n",
      "Train Epoch: 1 [140/3177 (22%)]\tlr:0.01049\tAccuracy: 0.3000\tLoss: 2.299083\n",
      "Train Epoch: 1 [140/3177 (22%)]\tlr:0.01049\tAccuracy: 0.3000\tLoss: 2.299083\n",
      "Train Epoch: 1 [140/3177 (22%)]\tlr:0.01049\tAccuracy: 0.3000\tLoss: 2.299083\n",
      "Train Epoch: 1 [140/3177 (22%)]\tlr:0.01049\tAccuracy: 0.3000\tLoss: 2.299083\n",
      "Train Epoch: 1 [140/3177 (22%)]\tlr:0.01049\tAccuracy: 0.3000\tLoss: 2.299083\n",
      "Train Epoch: 1 [140/3177 (22%)]\tlr:0.01049\tAccuracy: 0.3000\tLoss: 2.299083\n",
      "Train Epoch: 1 [140/3177 (22%)]\tlr:0.01049\tAccuracy: 0.3000\tLoss: 2.299083\n",
      "Train Epoch: 1 [140/3177 (22%)]\tlr:0.01049\tAccuracy: 0.3000\tLoss: 2.299083\n",
      "Train Epoch: 1 [140/3177 (22%)]\tlr:0.01049\tAccuracy: 0.3000\tLoss: 2.299083\n",
      "Train Epoch: 1 [140/3177 (22%)]\tlr:0.01049\tAccuracy: 0.3000\tLoss: 2.299083\n",
      "Train Epoch: 1 [140/3177 (22%)]\tlr:0.01049\tAccuracy: 0.3000\tLoss: 2.299083\n",
      "Train Epoch: 1 [140/3177 (22%)]\tlr:0.01049\tAccuracy: 0.3000\tLoss: 2.299083\n",
      "Train Epoch: 1 [140/3177 (22%)]\tlr:0.01049\tAccuracy: 0.3000\tLoss: 2.299083\n",
      "Train Epoch: 1 [140/3177 (22%)]\tlr:0.01049\tAccuracy: 0.3000\tLoss: 2.299083\n",
      "Train Epoch: 1 [140/3177 (22%)]\tlr:0.01049\tAccuracy: 0.3000\tLoss: 2.299083\n",
      "Train Epoch: 1 [140/3177 (22%)]\tlr:0.01049\tAccuracy: 0.3000\tLoss: 2.299083\n",
      "Train Epoch: 1 [140/3177 (22%)]\tlr:0.01049\tAccuracy: 0.3000\tLoss: 2.299083\n",
      "Train Epoch: 1 [140/3177 (22%)]\tlr:0.01049\tAccuracy: 0.3000\tLoss: 2.299083\n",
      "Train Epoch: 1 [140/3177 (22%)]\tlr:0.01049\tAccuracy: 0.3000\tLoss: 2.299083\n",
      "Train Epoch: 1 [140/3177 (22%)]\tlr:0.01049\tAccuracy: 0.3000\tLoss: 2.299083\n",
      "Train Epoch: 1 [140/3177 (22%)]\tlr:0.01049\tAccuracy: 0.3000\tLoss: 2.299083\n",
      "Train Epoch: 1 [160/3177 (25%)]\tlr:0.00982\tAccuracy: 0.5000\tLoss: 1.534889\n",
      "Train Epoch: 1 [160/3177 (25%)]\tlr:0.00982\tAccuracy: 0.5000\tLoss: 1.534889\n",
      "Train Epoch: 1 [160/3177 (25%)]\tlr:0.00982\tAccuracy: 0.5000\tLoss: 1.534889\n",
      "Train Epoch: 1 [160/3177 (25%)]\tlr:0.00982\tAccuracy: 0.5000\tLoss: 1.534889\n",
      "Train Epoch: 1 [160/3177 (25%)]\tlr:0.00982\tAccuracy: 0.5000\tLoss: 1.534889\n",
      "Train Epoch: 1 [160/3177 (25%)]\tlr:0.00982\tAccuracy: 0.5000\tLoss: 1.534889\n",
      "Train Epoch: 1 [160/3177 (25%)]\tlr:0.00982\tAccuracy: 0.5000\tLoss: 1.534889\n",
      "Train Epoch: 1 [160/3177 (25%)]\tlr:0.00982\tAccuracy: 0.5000\tLoss: 1.534889\n",
      "Train Epoch: 1 [160/3177 (25%)]\tlr:0.00982\tAccuracy: 0.5000\tLoss: 1.534889\n",
      "Train Epoch: 1 [160/3177 (25%)]\tlr:0.00982\tAccuracy: 0.5000\tLoss: 1.534889\n",
      "Train Epoch: 1 [160/3177 (25%)]\tlr:0.00982\tAccuracy: 0.5000\tLoss: 1.534889\n",
      "Train Epoch: 1 [160/3177 (25%)]\tlr:0.00982\tAccuracy: 0.5000\tLoss: 1.534889\n",
      "Train Epoch: 1 [160/3177 (25%)]\tlr:0.00982\tAccuracy: 0.5000\tLoss: 1.534889\n",
      "Train Epoch: 1 [160/3177 (25%)]\tlr:0.00982\tAccuracy: 0.5000\tLoss: 1.534889\n",
      "Train Epoch: 1 [160/3177 (25%)]\tlr:0.00982\tAccuracy: 0.5000\tLoss: 1.534889\n",
      "Train Epoch: 1 [160/3177 (25%)]\tlr:0.00982\tAccuracy: 0.5000\tLoss: 1.534889\n",
      "Train Epoch: 1 [160/3177 (25%)]\tlr:0.00982\tAccuracy: 0.5000\tLoss: 1.534889\n",
      "Train Epoch: 1 [160/3177 (25%)]\tlr:0.00982\tAccuracy: 0.5000\tLoss: 1.534889\n",
      "Train Epoch: 1 [160/3177 (25%)]\tlr:0.00982\tAccuracy: 0.5000\tLoss: 1.534889\n",
      "Train Epoch: 1 [160/3177 (25%)]\tlr:0.00982\tAccuracy: 0.5000\tLoss: 1.534889\n",
      "Train Epoch: 1 [160/3177 (25%)]\tlr:0.00982\tAccuracy: 0.5000\tLoss: 1.534889\n",
      "Train Epoch: 1 [160/3177 (25%)]\tlr:0.00982\tAccuracy: 0.5000\tLoss: 1.534889\n",
      "Train Epoch: 1 [160/3177 (25%)]\tlr:0.00982\tAccuracy: 0.5000\tLoss: 1.534889\n",
      "Train Epoch: 1 [180/3177 (28%)]\tlr:0.00927\tAccuracy: 0.4000\tLoss: 1.826139\n",
      "Train Epoch: 1 [180/3177 (28%)]\tlr:0.00927\tAccuracy: 0.4000\tLoss: 1.826139\n",
      "Train Epoch: 1 [180/3177 (28%)]\tlr:0.00927\tAccuracy: 0.4000\tLoss: 1.826139\n",
      "Train Epoch: 1 [180/3177 (28%)]\tlr:0.00927\tAccuracy: 0.4000\tLoss: 1.826139\n",
      "Train Epoch: 1 [180/3177 (28%)]\tlr:0.00927\tAccuracy: 0.4000\tLoss: 1.826139\n",
      "Train Epoch: 1 [180/3177 (28%)]\tlr:0.00927\tAccuracy: 0.4000\tLoss: 1.826139\n",
      "Train Epoch: 1 [180/3177 (28%)]\tlr:0.00927\tAccuracy: 0.4000\tLoss: 1.826139\n",
      "Train Epoch: 1 [180/3177 (28%)]\tlr:0.00927\tAccuracy: 0.4000\tLoss: 1.826139\n",
      "Train Epoch: 1 [180/3177 (28%)]\tlr:0.00927\tAccuracy: 0.4000\tLoss: 1.826139\n",
      "Train Epoch: 1 [180/3177 (28%)]\tlr:0.00927\tAccuracy: 0.4000\tLoss: 1.826139\n",
      "Train Epoch: 1 [180/3177 (28%)]\tlr:0.00927\tAccuracy: 0.4000\tLoss: 1.826139\n",
      "Train Epoch: 1 [180/3177 (28%)]\tlr:0.00927\tAccuracy: 0.4000\tLoss: 1.826139\n",
      "Train Epoch: 1 [180/3177 (28%)]\tlr:0.00927\tAccuracy: 0.4000\tLoss: 1.826139\n",
      "Train Epoch: 1 [180/3177 (28%)]\tlr:0.00927\tAccuracy: 0.4000\tLoss: 1.826139\n",
      "Train Epoch: 1 [180/3177 (28%)]\tlr:0.00927\tAccuracy: 0.4000\tLoss: 1.826139\n",
      "Train Epoch: 1 [180/3177 (28%)]\tlr:0.00927\tAccuracy: 0.4000\tLoss: 1.826139\n",
      "Train Epoch: 1 [180/3177 (28%)]\tlr:0.00927\tAccuracy: 0.4000\tLoss: 1.826139\n",
      "Train Epoch: 1 [180/3177 (28%)]\tlr:0.00927\tAccuracy: 0.4000\tLoss: 1.826139\n",
      "Train Epoch: 1 [180/3177 (28%)]\tlr:0.00927\tAccuracy: 0.4000\tLoss: 1.826139\n",
      "Train Epoch: 1 [180/3177 (28%)]\tlr:0.00927\tAccuracy: 0.4000\tLoss: 1.826139\n",
      "Train Epoch: 1 [180/3177 (28%)]\tlr:0.00927\tAccuracy: 0.4000\tLoss: 1.826139\n",
      "Train Epoch: 1 [180/3177 (28%)]\tlr:0.00927\tAccuracy: 0.4000\tLoss: 1.826139\n",
      "Train Epoch: 1 [180/3177 (28%)]\tlr:0.00927\tAccuracy: 0.4000\tLoss: 1.826139\n",
      "Train Epoch: 1 [200/3177 (31%)]\tlr:0.00879\tAccuracy: 0.5000\tLoss: 1.480760\n",
      "Train Epoch: 1 [200/3177 (31%)]\tlr:0.00879\tAccuracy: 0.5000\tLoss: 1.480760\n",
      "Train Epoch: 1 [200/3177 (31%)]\tlr:0.00879\tAccuracy: 0.5000\tLoss: 1.480760\n",
      "Train Epoch: 1 [200/3177 (31%)]\tlr:0.00879\tAccuracy: 0.5000\tLoss: 1.480760\n",
      "Train Epoch: 1 [200/3177 (31%)]\tlr:0.00879\tAccuracy: 0.5000\tLoss: 1.480760\n",
      "Train Epoch: 1 [200/3177 (31%)]\tlr:0.00879\tAccuracy: 0.5000\tLoss: 1.480760\n",
      "Train Epoch: 1 [200/3177 (31%)]\tlr:0.00879\tAccuracy: 0.5000\tLoss: 1.480760\n",
      "Train Epoch: 1 [200/3177 (31%)]\tlr:0.00879\tAccuracy: 0.5000\tLoss: 1.480760\n",
      "Train Epoch: 1 [200/3177 (31%)]\tlr:0.00879\tAccuracy: 0.5000\tLoss: 1.480760\n",
      "Train Epoch: 1 [200/3177 (31%)]\tlr:0.00879\tAccuracy: 0.5000\tLoss: 1.480760\n",
      "Train Epoch: 1 [200/3177 (31%)]\tlr:0.00879\tAccuracy: 0.5000\tLoss: 1.480760\n",
      "Train Epoch: 1 [200/3177 (31%)]\tlr:0.00879\tAccuracy: 0.5000\tLoss: 1.480760\n",
      "Train Epoch: 1 [200/3177 (31%)]\tlr:0.00879\tAccuracy: 0.5000\tLoss: 1.480760\n",
      "Train Epoch: 1 [200/3177 (31%)]\tlr:0.00879\tAccuracy: 0.5000\tLoss: 1.480760\n",
      "Train Epoch: 1 [200/3177 (31%)]\tlr:0.00879\tAccuracy: 0.5000\tLoss: 1.480760\n",
      "Train Epoch: 1 [200/3177 (31%)]\tlr:0.00879\tAccuracy: 0.5000\tLoss: 1.480760\n",
      "Train Epoch: 1 [200/3177 (31%)]\tlr:0.00879\tAccuracy: 0.5000\tLoss: 1.480760\n",
      "Train Epoch: 1 [200/3177 (31%)]\tlr:0.00879\tAccuracy: 0.5000\tLoss: 1.480760\n",
      "Train Epoch: 1 [200/3177 (31%)]\tlr:0.00879\tAccuracy: 0.5000\tLoss: 1.480760\n",
      "Train Epoch: 1 [200/3177 (31%)]\tlr:0.00879\tAccuracy: 0.5000\tLoss: 1.480760\n",
      "Train Epoch: 1 [200/3177 (31%)]\tlr:0.00879\tAccuracy: 0.5000\tLoss: 1.480760\n",
      "Train Epoch: 1 [200/3177 (31%)]\tlr:0.00879\tAccuracy: 0.5000\tLoss: 1.480760\n",
      "Train Epoch: 1 [200/3177 (31%)]\tlr:0.00879\tAccuracy: 0.5000\tLoss: 1.480760\n",
      "Train Epoch: 1 [220/3177 (35%)]\tlr:0.00839\tAccuracy: 0.6000\tLoss: 1.331610\n",
      "Train Epoch: 1 [220/3177 (35%)]\tlr:0.00839\tAccuracy: 0.6000\tLoss: 1.331610\n",
      "Train Epoch: 1 [220/3177 (35%)]\tlr:0.00839\tAccuracy: 0.6000\tLoss: 1.331610\n",
      "Train Epoch: 1 [220/3177 (35%)]\tlr:0.00839\tAccuracy: 0.6000\tLoss: 1.331610\n",
      "Train Epoch: 1 [220/3177 (35%)]\tlr:0.00839\tAccuracy: 0.6000\tLoss: 1.331610\n",
      "Train Epoch: 1 [220/3177 (35%)]\tlr:0.00839\tAccuracy: 0.6000\tLoss: 1.331610\n",
      "Train Epoch: 1 [220/3177 (35%)]\tlr:0.00839\tAccuracy: 0.6000\tLoss: 1.331610\n",
      "Train Epoch: 1 [220/3177 (35%)]\tlr:0.00839\tAccuracy: 0.6000\tLoss: 1.331610\n",
      "Train Epoch: 1 [220/3177 (35%)]\tlr:0.00839\tAccuracy: 0.6000\tLoss: 1.331610\n",
      "Train Epoch: 1 [220/3177 (35%)]\tlr:0.00839\tAccuracy: 0.6000\tLoss: 1.331610\n",
      "Train Epoch: 1 [220/3177 (35%)]\tlr:0.00839\tAccuracy: 0.6000\tLoss: 1.331610\n",
      "Train Epoch: 1 [220/3177 (35%)]\tlr:0.00839\tAccuracy: 0.6000\tLoss: 1.331610\n",
      "Train Epoch: 1 [220/3177 (35%)]\tlr:0.00839\tAccuracy: 0.6000\tLoss: 1.331610\n",
      "Train Epoch: 1 [220/3177 (35%)]\tlr:0.00839\tAccuracy: 0.6000\tLoss: 1.331610\n",
      "Train Epoch: 1 [220/3177 (35%)]\tlr:0.00839\tAccuracy: 0.6000\tLoss: 1.331610\n",
      "Train Epoch: 1 [220/3177 (35%)]\tlr:0.00839\tAccuracy: 0.6000\tLoss: 1.331610\n",
      "Train Epoch: 1 [220/3177 (35%)]\tlr:0.00839\tAccuracy: 0.6000\tLoss: 1.331610\n",
      "Train Epoch: 1 [220/3177 (35%)]\tlr:0.00839\tAccuracy: 0.6000\tLoss: 1.331610\n",
      "Train Epoch: 1 [220/3177 (35%)]\tlr:0.00839\tAccuracy: 0.6000\tLoss: 1.331610\n",
      "Train Epoch: 1 [220/3177 (35%)]\tlr:0.00839\tAccuracy: 0.6000\tLoss: 1.331610\n",
      "Train Epoch: 1 [220/3177 (35%)]\tlr:0.00839\tAccuracy: 0.6000\tLoss: 1.331610\n",
      "Train Epoch: 1 [220/3177 (35%)]\tlr:0.00839\tAccuracy: 0.6000\tLoss: 1.331610\n",
      "Train Epoch: 1 [220/3177 (35%)]\tlr:0.00839\tAccuracy: 0.6000\tLoss: 1.331610\n",
      "Train Epoch: 1 [240/3177 (38%)]\tlr:0.00804\tAccuracy: 0.3000\tLoss: 1.371496\n",
      "Train Epoch: 1 [240/3177 (38%)]\tlr:0.00804\tAccuracy: 0.3000\tLoss: 1.371496\n",
      "Train Epoch: 1 [240/3177 (38%)]\tlr:0.00804\tAccuracy: 0.3000\tLoss: 1.371496\n",
      "Train Epoch: 1 [240/3177 (38%)]\tlr:0.00804\tAccuracy: 0.3000\tLoss: 1.371496\n",
      "Train Epoch: 1 [240/3177 (38%)]\tlr:0.00804\tAccuracy: 0.3000\tLoss: 1.371496\n",
      "Train Epoch: 1 [240/3177 (38%)]\tlr:0.00804\tAccuracy: 0.3000\tLoss: 1.371496\n",
      "Train Epoch: 1 [240/3177 (38%)]\tlr:0.00804\tAccuracy: 0.3000\tLoss: 1.371496\n",
      "Train Epoch: 1 [240/3177 (38%)]\tlr:0.00804\tAccuracy: 0.3000\tLoss: 1.371496\n",
      "Train Epoch: 1 [240/3177 (38%)]\tlr:0.00804\tAccuracy: 0.3000\tLoss: 1.371496\n",
      "Train Epoch: 1 [240/3177 (38%)]\tlr:0.00804\tAccuracy: 0.3000\tLoss: 1.371496\n",
      "Train Epoch: 1 [240/3177 (38%)]\tlr:0.00804\tAccuracy: 0.3000\tLoss: 1.371496\n",
      "Train Epoch: 1 [240/3177 (38%)]\tlr:0.00804\tAccuracy: 0.3000\tLoss: 1.371496\n",
      "Train Epoch: 1 [240/3177 (38%)]\tlr:0.00804\tAccuracy: 0.3000\tLoss: 1.371496\n",
      "Train Epoch: 1 [240/3177 (38%)]\tlr:0.00804\tAccuracy: 0.3000\tLoss: 1.371496\n",
      "Train Epoch: 1 [240/3177 (38%)]\tlr:0.00804\tAccuracy: 0.3000\tLoss: 1.371496\n",
      "Train Epoch: 1 [240/3177 (38%)]\tlr:0.00804\tAccuracy: 0.3000\tLoss: 1.371496\n",
      "Train Epoch: 1 [240/3177 (38%)]\tlr:0.00804\tAccuracy: 0.3000\tLoss: 1.371496\n",
      "Train Epoch: 1 [240/3177 (38%)]\tlr:0.00804\tAccuracy: 0.3000\tLoss: 1.371496\n",
      "Train Epoch: 1 [240/3177 (38%)]\tlr:0.00804\tAccuracy: 0.3000\tLoss: 1.371496\n",
      "Train Epoch: 1 [240/3177 (38%)]\tlr:0.00804\tAccuracy: 0.3000\tLoss: 1.371496\n",
      "Train Epoch: 1 [240/3177 (38%)]\tlr:0.00804\tAccuracy: 0.3000\tLoss: 1.371496\n",
      "Train Epoch: 1 [240/3177 (38%)]\tlr:0.00804\tAccuracy: 0.3000\tLoss: 1.371496\n",
      "Train Epoch: 1 [240/3177 (38%)]\tlr:0.00804\tAccuracy: 0.3000\tLoss: 1.371496\n",
      "Train Epoch: 1 [260/3177 (41%)]\tlr:0.00772\tAccuracy: 0.6000\tLoss: 1.289241\n",
      "Train Epoch: 1 [260/3177 (41%)]\tlr:0.00772\tAccuracy: 0.6000\tLoss: 1.289241\n",
      "Train Epoch: 1 [260/3177 (41%)]\tlr:0.00772\tAccuracy: 0.6000\tLoss: 1.289241\n",
      "Train Epoch: 1 [260/3177 (41%)]\tlr:0.00772\tAccuracy: 0.6000\tLoss: 1.289241\n",
      "Train Epoch: 1 [260/3177 (41%)]\tlr:0.00772\tAccuracy: 0.6000\tLoss: 1.289241\n",
      "Train Epoch: 1 [260/3177 (41%)]\tlr:0.00772\tAccuracy: 0.6000\tLoss: 1.289241\n",
      "Train Epoch: 1 [260/3177 (41%)]\tlr:0.00772\tAccuracy: 0.6000\tLoss: 1.289241\n",
      "Train Epoch: 1 [260/3177 (41%)]\tlr:0.00772\tAccuracy: 0.6000\tLoss: 1.289241\n",
      "Train Epoch: 1 [260/3177 (41%)]\tlr:0.00772\tAccuracy: 0.6000\tLoss: 1.289241\n",
      "Train Epoch: 1 [260/3177 (41%)]\tlr:0.00772\tAccuracy: 0.6000\tLoss: 1.289241\n",
      "Train Epoch: 1 [260/3177 (41%)]\tlr:0.00772\tAccuracy: 0.6000\tLoss: 1.289241\n",
      "Train Epoch: 1 [260/3177 (41%)]\tlr:0.00772\tAccuracy: 0.6000\tLoss: 1.289241\n",
      "Train Epoch: 1 [260/3177 (41%)]\tlr:0.00772\tAccuracy: 0.6000\tLoss: 1.289241\n",
      "Train Epoch: 1 [260/3177 (41%)]\tlr:0.00772\tAccuracy: 0.6000\tLoss: 1.289241\n",
      "Train Epoch: 1 [260/3177 (41%)]\tlr:0.00772\tAccuracy: 0.6000\tLoss: 1.289241\n",
      "Train Epoch: 1 [260/3177 (41%)]\tlr:0.00772\tAccuracy: 0.6000\tLoss: 1.289241\n",
      "Train Epoch: 1 [260/3177 (41%)]\tlr:0.00772\tAccuracy: 0.6000\tLoss: 1.289241\n",
      "Train Epoch: 1 [260/3177 (41%)]\tlr:0.00772\tAccuracy: 0.6000\tLoss: 1.289241\n",
      "Train Epoch: 1 [260/3177 (41%)]\tlr:0.00772\tAccuracy: 0.6000\tLoss: 1.289241\n",
      "Train Epoch: 1 [260/3177 (41%)]\tlr:0.00772\tAccuracy: 0.6000\tLoss: 1.289241\n",
      "Train Epoch: 1 [260/3177 (41%)]\tlr:0.00772\tAccuracy: 0.6000\tLoss: 1.289241\n",
      "Train Epoch: 1 [260/3177 (41%)]\tlr:0.00772\tAccuracy: 0.6000\tLoss: 1.289241\n",
      "Train Epoch: 1 [260/3177 (41%)]\tlr:0.00772\tAccuracy: 0.6000\tLoss: 1.289241\n",
      "Train Epoch: 1 [280/3177 (44%)]\tlr:0.00744\tAccuracy: 0.5000\tLoss: 1.266872\n",
      "Train Epoch: 1 [280/3177 (44%)]\tlr:0.00744\tAccuracy: 0.5000\tLoss: 1.266872\n",
      "Train Epoch: 1 [280/3177 (44%)]\tlr:0.00744\tAccuracy: 0.5000\tLoss: 1.266872\n",
      "Train Epoch: 1 [280/3177 (44%)]\tlr:0.00744\tAccuracy: 0.5000\tLoss: 1.266872\n",
      "Train Epoch: 1 [280/3177 (44%)]\tlr:0.00744\tAccuracy: 0.5000\tLoss: 1.266872\n",
      "Train Epoch: 1 [280/3177 (44%)]\tlr:0.00744\tAccuracy: 0.5000\tLoss: 1.266872\n",
      "Train Epoch: 1 [280/3177 (44%)]\tlr:0.00744\tAccuracy: 0.5000\tLoss: 1.266872\n",
      "Train Epoch: 1 [280/3177 (44%)]\tlr:0.00744\tAccuracy: 0.5000\tLoss: 1.266872\n",
      "Train Epoch: 1 [280/3177 (44%)]\tlr:0.00744\tAccuracy: 0.5000\tLoss: 1.266872\n",
      "Train Epoch: 1 [280/3177 (44%)]\tlr:0.00744\tAccuracy: 0.5000\tLoss: 1.266872\n",
      "Train Epoch: 1 [280/3177 (44%)]\tlr:0.00744\tAccuracy: 0.5000\tLoss: 1.266872\n",
      "Train Epoch: 1 [280/3177 (44%)]\tlr:0.00744\tAccuracy: 0.5000\tLoss: 1.266872\n",
      "Train Epoch: 1 [280/3177 (44%)]\tlr:0.00744\tAccuracy: 0.5000\tLoss: 1.266872\n",
      "Train Epoch: 1 [280/3177 (44%)]\tlr:0.00744\tAccuracy: 0.5000\tLoss: 1.266872\n",
      "Train Epoch: 1 [280/3177 (44%)]\tlr:0.00744\tAccuracy: 0.5000\tLoss: 1.266872\n",
      "Train Epoch: 1 [280/3177 (44%)]\tlr:0.00744\tAccuracy: 0.5000\tLoss: 1.266872\n",
      "Train Epoch: 1 [280/3177 (44%)]\tlr:0.00744\tAccuracy: 0.5000\tLoss: 1.266872\n",
      "Train Epoch: 1 [280/3177 (44%)]\tlr:0.00744\tAccuracy: 0.5000\tLoss: 1.266872\n",
      "Train Epoch: 1 [280/3177 (44%)]\tlr:0.00744\tAccuracy: 0.5000\tLoss: 1.266872\n",
      "Train Epoch: 1 [280/3177 (44%)]\tlr:0.00744\tAccuracy: 0.5000\tLoss: 1.266872\n",
      "Train Epoch: 1 [280/3177 (44%)]\tlr:0.00744\tAccuracy: 0.5000\tLoss: 1.266872\n",
      "Train Epoch: 1 [280/3177 (44%)]\tlr:0.00744\tAccuracy: 0.5000\tLoss: 1.266872\n",
      "Train Epoch: 1 [280/3177 (44%)]\tlr:0.00744\tAccuracy: 0.5000\tLoss: 1.266872\n",
      "Train Epoch: 1 [300/3177 (47%)]\tlr:0.00719\tAccuracy: 0.4000\tLoss: 1.196117\n",
      "Train Epoch: 1 [300/3177 (47%)]\tlr:0.00719\tAccuracy: 0.4000\tLoss: 1.196117\n",
      "Train Epoch: 1 [300/3177 (47%)]\tlr:0.00719\tAccuracy: 0.4000\tLoss: 1.196117\n",
      "Train Epoch: 1 [300/3177 (47%)]\tlr:0.00719\tAccuracy: 0.4000\tLoss: 1.196117\n",
      "Train Epoch: 1 [300/3177 (47%)]\tlr:0.00719\tAccuracy: 0.4000\tLoss: 1.196117\n",
      "Train Epoch: 1 [300/3177 (47%)]\tlr:0.00719\tAccuracy: 0.4000\tLoss: 1.196117\n",
      "Train Epoch: 1 [300/3177 (47%)]\tlr:0.00719\tAccuracy: 0.4000\tLoss: 1.196117\n",
      "Train Epoch: 1 [300/3177 (47%)]\tlr:0.00719\tAccuracy: 0.4000\tLoss: 1.196117\n",
      "Train Epoch: 1 [300/3177 (47%)]\tlr:0.00719\tAccuracy: 0.4000\tLoss: 1.196117\n",
      "Train Epoch: 1 [300/3177 (47%)]\tlr:0.00719\tAccuracy: 0.4000\tLoss: 1.196117\n",
      "Train Epoch: 1 [300/3177 (47%)]\tlr:0.00719\tAccuracy: 0.4000\tLoss: 1.196117\n",
      "Train Epoch: 1 [300/3177 (47%)]\tlr:0.00719\tAccuracy: 0.4000\tLoss: 1.196117\n",
      "Train Epoch: 1 [300/3177 (47%)]\tlr:0.00719\tAccuracy: 0.4000\tLoss: 1.196117\n",
      "Train Epoch: 1 [300/3177 (47%)]\tlr:0.00719\tAccuracy: 0.4000\tLoss: 1.196117\n",
      "Train Epoch: 1 [300/3177 (47%)]\tlr:0.00719\tAccuracy: 0.4000\tLoss: 1.196117\n",
      "Train Epoch: 1 [300/3177 (47%)]\tlr:0.00719\tAccuracy: 0.4000\tLoss: 1.196117\n",
      "Train Epoch: 1 [300/3177 (47%)]\tlr:0.00719\tAccuracy: 0.4000\tLoss: 1.196117\n",
      "Train Epoch: 1 [300/3177 (47%)]\tlr:0.00719\tAccuracy: 0.4000\tLoss: 1.196117\n",
      "Train Epoch: 1 [300/3177 (47%)]\tlr:0.00719\tAccuracy: 0.4000\tLoss: 1.196117\n",
      "Train Epoch: 1 [300/3177 (47%)]\tlr:0.00719\tAccuracy: 0.4000\tLoss: 1.196117\n",
      "Train Epoch: 1 [300/3177 (47%)]\tlr:0.00719\tAccuracy: 0.4000\tLoss: 1.196117\n",
      "Train Epoch: 1 [300/3177 (47%)]\tlr:0.00719\tAccuracy: 0.4000\tLoss: 1.196117\n",
      "Train Epoch: 1 [300/3177 (47%)]\tlr:0.00719\tAccuracy: 0.4000\tLoss: 1.196117\n",
      "Train Epoch: 1 [320/3177 (50%)]\tlr:0.00697\tAccuracy: 0.5000\tLoss: 1.441401\n",
      "Train Epoch: 1 [320/3177 (50%)]\tlr:0.00697\tAccuracy: 0.5000\tLoss: 1.441401\n",
      "Train Epoch: 1 [320/3177 (50%)]\tlr:0.00697\tAccuracy: 0.5000\tLoss: 1.441401\n",
      "Train Epoch: 1 [320/3177 (50%)]\tlr:0.00697\tAccuracy: 0.5000\tLoss: 1.441401\n",
      "Train Epoch: 1 [320/3177 (50%)]\tlr:0.00697\tAccuracy: 0.5000\tLoss: 1.441401\n",
      "Train Epoch: 1 [320/3177 (50%)]\tlr:0.00697\tAccuracy: 0.5000\tLoss: 1.441401\n",
      "Train Epoch: 1 [320/3177 (50%)]\tlr:0.00697\tAccuracy: 0.5000\tLoss: 1.441401\n",
      "Train Epoch: 1 [320/3177 (50%)]\tlr:0.00697\tAccuracy: 0.5000\tLoss: 1.441401\n",
      "Train Epoch: 1 [320/3177 (50%)]\tlr:0.00697\tAccuracy: 0.5000\tLoss: 1.441401\n",
      "Train Epoch: 1 [320/3177 (50%)]\tlr:0.00697\tAccuracy: 0.5000\tLoss: 1.441401\n",
      "Train Epoch: 1 [320/3177 (50%)]\tlr:0.00697\tAccuracy: 0.5000\tLoss: 1.441401\n",
      "Train Epoch: 1 [320/3177 (50%)]\tlr:0.00697\tAccuracy: 0.5000\tLoss: 1.441401\n",
      "Train Epoch: 1 [320/3177 (50%)]\tlr:0.00697\tAccuracy: 0.5000\tLoss: 1.441401\n",
      "Train Epoch: 1 [320/3177 (50%)]\tlr:0.00697\tAccuracy: 0.5000\tLoss: 1.441401\n",
      "Train Epoch: 1 [320/3177 (50%)]\tlr:0.00697\tAccuracy: 0.5000\tLoss: 1.441401\n",
      "Train Epoch: 1 [320/3177 (50%)]\tlr:0.00697\tAccuracy: 0.5000\tLoss: 1.441401\n",
      "Train Epoch: 1 [320/3177 (50%)]\tlr:0.00697\tAccuracy: 0.5000\tLoss: 1.441401\n",
      "Train Epoch: 1 [320/3177 (50%)]\tlr:0.00697\tAccuracy: 0.5000\tLoss: 1.441401\n",
      "Train Epoch: 1 [320/3177 (50%)]\tlr:0.00697\tAccuracy: 0.5000\tLoss: 1.441401\n",
      "Train Epoch: 1 [320/3177 (50%)]\tlr:0.00697\tAccuracy: 0.5000\tLoss: 1.441401\n",
      "Train Epoch: 1 [320/3177 (50%)]\tlr:0.00697\tAccuracy: 0.5000\tLoss: 1.441401\n",
      "Train Epoch: 1 [320/3177 (50%)]\tlr:0.00697\tAccuracy: 0.5000\tLoss: 1.441401\n",
      "Train Epoch: 1 [320/3177 (50%)]\tlr:0.00697\tAccuracy: 0.5000\tLoss: 1.441401\n",
      "Train Epoch: 1 [340/3177 (53%)]\tlr:0.00676\tAccuracy: 0.5000\tLoss: 1.041867\n",
      "Train Epoch: 1 [340/3177 (53%)]\tlr:0.00676\tAccuracy: 0.5000\tLoss: 1.041867\n",
      "Train Epoch: 1 [340/3177 (53%)]\tlr:0.00676\tAccuracy: 0.5000\tLoss: 1.041867\n",
      "Train Epoch: 1 [340/3177 (53%)]\tlr:0.00676\tAccuracy: 0.5000\tLoss: 1.041867\n",
      "Train Epoch: 1 [340/3177 (53%)]\tlr:0.00676\tAccuracy: 0.5000\tLoss: 1.041867\n",
      "Train Epoch: 1 [340/3177 (53%)]\tlr:0.00676\tAccuracy: 0.5000\tLoss: 1.041867\n",
      "Train Epoch: 1 [340/3177 (53%)]\tlr:0.00676\tAccuracy: 0.5000\tLoss: 1.041867\n",
      "Train Epoch: 1 [340/3177 (53%)]\tlr:0.00676\tAccuracy: 0.5000\tLoss: 1.041867\n",
      "Train Epoch: 1 [340/3177 (53%)]\tlr:0.00676\tAccuracy: 0.5000\tLoss: 1.041867\n",
      "Train Epoch: 1 [340/3177 (53%)]\tlr:0.00676\tAccuracy: 0.5000\tLoss: 1.041867\n",
      "Train Epoch: 1 [340/3177 (53%)]\tlr:0.00676\tAccuracy: 0.5000\tLoss: 1.041867\n",
      "Train Epoch: 1 [340/3177 (53%)]\tlr:0.00676\tAccuracy: 0.5000\tLoss: 1.041867\n",
      "Train Epoch: 1 [340/3177 (53%)]\tlr:0.00676\tAccuracy: 0.5000\tLoss: 1.041867\n",
      "Train Epoch: 1 [340/3177 (53%)]\tlr:0.00676\tAccuracy: 0.5000\tLoss: 1.041867\n",
      "Train Epoch: 1 [340/3177 (53%)]\tlr:0.00676\tAccuracy: 0.5000\tLoss: 1.041867\n",
      "Train Epoch: 1 [340/3177 (53%)]\tlr:0.00676\tAccuracy: 0.5000\tLoss: 1.041867\n",
      "Train Epoch: 1 [340/3177 (53%)]\tlr:0.00676\tAccuracy: 0.5000\tLoss: 1.041867\n",
      "Train Epoch: 1 [340/3177 (53%)]\tlr:0.00676\tAccuracy: 0.5000\tLoss: 1.041867\n",
      "Train Epoch: 1 [340/3177 (53%)]\tlr:0.00676\tAccuracy: 0.5000\tLoss: 1.041867\n",
      "Train Epoch: 1 [340/3177 (53%)]\tlr:0.00676\tAccuracy: 0.5000\tLoss: 1.041867\n",
      "Train Epoch: 1 [340/3177 (53%)]\tlr:0.00676\tAccuracy: 0.5000\tLoss: 1.041867\n",
      "Train Epoch: 1 [340/3177 (53%)]\tlr:0.00676\tAccuracy: 0.5000\tLoss: 1.041867\n",
      "Train Epoch: 1 [340/3177 (53%)]\tlr:0.00676\tAccuracy: 0.5000\tLoss: 1.041867\n",
      "Train Epoch: 1 [360/3177 (57%)]\tlr:0.00657\tAccuracy: 0.4000\tLoss: 1.807789\n",
      "Train Epoch: 1 [360/3177 (57%)]\tlr:0.00657\tAccuracy: 0.4000\tLoss: 1.807789\n",
      "Train Epoch: 1 [360/3177 (57%)]\tlr:0.00657\tAccuracy: 0.4000\tLoss: 1.807789\n",
      "Train Epoch: 1 [360/3177 (57%)]\tlr:0.00657\tAccuracy: 0.4000\tLoss: 1.807789\n",
      "Train Epoch: 1 [360/3177 (57%)]\tlr:0.00657\tAccuracy: 0.4000\tLoss: 1.807789\n",
      "Train Epoch: 1 [360/3177 (57%)]\tlr:0.00657\tAccuracy: 0.4000\tLoss: 1.807789\n",
      "Train Epoch: 1 [360/3177 (57%)]\tlr:0.00657\tAccuracy: 0.4000\tLoss: 1.807789\n",
      "Train Epoch: 1 [360/3177 (57%)]\tlr:0.00657\tAccuracy: 0.4000\tLoss: 1.807789\n",
      "Train Epoch: 1 [360/3177 (57%)]\tlr:0.00657\tAccuracy: 0.4000\tLoss: 1.807789\n",
      "Train Epoch: 1 [360/3177 (57%)]\tlr:0.00657\tAccuracy: 0.4000\tLoss: 1.807789\n",
      "Train Epoch: 1 [360/3177 (57%)]\tlr:0.00657\tAccuracy: 0.4000\tLoss: 1.807789\n",
      "Train Epoch: 1 [360/3177 (57%)]\tlr:0.00657\tAccuracy: 0.4000\tLoss: 1.807789\n",
      "Train Epoch: 1 [360/3177 (57%)]\tlr:0.00657\tAccuracy: 0.4000\tLoss: 1.807789\n",
      "Train Epoch: 1 [360/3177 (57%)]\tlr:0.00657\tAccuracy: 0.4000\tLoss: 1.807789\n",
      "Train Epoch: 1 [360/3177 (57%)]\tlr:0.00657\tAccuracy: 0.4000\tLoss: 1.807789\n",
      "Train Epoch: 1 [360/3177 (57%)]\tlr:0.00657\tAccuracy: 0.4000\tLoss: 1.807789\n",
      "Train Epoch: 1 [360/3177 (57%)]\tlr:0.00657\tAccuracy: 0.4000\tLoss: 1.807789\n",
      "Train Epoch: 1 [360/3177 (57%)]\tlr:0.00657\tAccuracy: 0.4000\tLoss: 1.807789\n",
      "Train Epoch: 1 [360/3177 (57%)]\tlr:0.00657\tAccuracy: 0.4000\tLoss: 1.807789\n",
      "Train Epoch: 1 [360/3177 (57%)]\tlr:0.00657\tAccuracy: 0.4000\tLoss: 1.807789\n",
      "Train Epoch: 1 [360/3177 (57%)]\tlr:0.00657\tAccuracy: 0.4000\tLoss: 1.807789\n",
      "Train Epoch: 1 [360/3177 (57%)]\tlr:0.00657\tAccuracy: 0.4000\tLoss: 1.807789\n",
      "Train Epoch: 1 [360/3177 (57%)]\tlr:0.00657\tAccuracy: 0.4000\tLoss: 1.807789\n",
      "Train Epoch: 1 [380/3177 (60%)]\tlr:0.00640\tAccuracy: 0.2000\tLoss: 1.465574\n",
      "Train Epoch: 1 [380/3177 (60%)]\tlr:0.00640\tAccuracy: 0.2000\tLoss: 1.465574\n",
      "Train Epoch: 1 [380/3177 (60%)]\tlr:0.00640\tAccuracy: 0.2000\tLoss: 1.465574\n",
      "Train Epoch: 1 [380/3177 (60%)]\tlr:0.00640\tAccuracy: 0.2000\tLoss: 1.465574\n",
      "Train Epoch: 1 [380/3177 (60%)]\tlr:0.00640\tAccuracy: 0.2000\tLoss: 1.465574\n",
      "Train Epoch: 1 [380/3177 (60%)]\tlr:0.00640\tAccuracy: 0.2000\tLoss: 1.465574\n",
      "Train Epoch: 1 [380/3177 (60%)]\tlr:0.00640\tAccuracy: 0.2000\tLoss: 1.465574\n",
      "Train Epoch: 1 [380/3177 (60%)]\tlr:0.00640\tAccuracy: 0.2000\tLoss: 1.465574\n",
      "Train Epoch: 1 [380/3177 (60%)]\tlr:0.00640\tAccuracy: 0.2000\tLoss: 1.465574\n",
      "Train Epoch: 1 [380/3177 (60%)]\tlr:0.00640\tAccuracy: 0.2000\tLoss: 1.465574\n",
      "Train Epoch: 1 [380/3177 (60%)]\tlr:0.00640\tAccuracy: 0.2000\tLoss: 1.465574\n",
      "Train Epoch: 1 [380/3177 (60%)]\tlr:0.00640\tAccuracy: 0.2000\tLoss: 1.465574\n",
      "Train Epoch: 1 [380/3177 (60%)]\tlr:0.00640\tAccuracy: 0.2000\tLoss: 1.465574\n",
      "Train Epoch: 1 [380/3177 (60%)]\tlr:0.00640\tAccuracy: 0.2000\tLoss: 1.465574\n",
      "Train Epoch: 1 [380/3177 (60%)]\tlr:0.00640\tAccuracy: 0.2000\tLoss: 1.465574\n",
      "Train Epoch: 1 [380/3177 (60%)]\tlr:0.00640\tAccuracy: 0.2000\tLoss: 1.465574\n",
      "Train Epoch: 1 [380/3177 (60%)]\tlr:0.00640\tAccuracy: 0.2000\tLoss: 1.465574\n",
      "Train Epoch: 1 [380/3177 (60%)]\tlr:0.00640\tAccuracy: 0.2000\tLoss: 1.465574\n",
      "Train Epoch: 1 [380/3177 (60%)]\tlr:0.00640\tAccuracy: 0.2000\tLoss: 1.465574\n",
      "Train Epoch: 1 [380/3177 (60%)]\tlr:0.00640\tAccuracy: 0.2000\tLoss: 1.465574\n",
      "Train Epoch: 1 [380/3177 (60%)]\tlr:0.00640\tAccuracy: 0.2000\tLoss: 1.465574\n",
      "Train Epoch: 1 [380/3177 (60%)]\tlr:0.00640\tAccuracy: 0.2000\tLoss: 1.465574\n",
      "Train Epoch: 1 [380/3177 (60%)]\tlr:0.00640\tAccuracy: 0.2000\tLoss: 1.465574\n",
      "Train Epoch: 1 [400/3177 (63%)]\tlr:0.00623\tAccuracy: 0.9000\tLoss: 0.834742\n",
      "Train Epoch: 1 [400/3177 (63%)]\tlr:0.00623\tAccuracy: 0.9000\tLoss: 0.834742\n",
      "Train Epoch: 1 [400/3177 (63%)]\tlr:0.00623\tAccuracy: 0.9000\tLoss: 0.834742\n",
      "Train Epoch: 1 [400/3177 (63%)]\tlr:0.00623\tAccuracy: 0.9000\tLoss: 0.834742\n",
      "Train Epoch: 1 [400/3177 (63%)]\tlr:0.00623\tAccuracy: 0.9000\tLoss: 0.834742\n",
      "Train Epoch: 1 [400/3177 (63%)]\tlr:0.00623\tAccuracy: 0.9000\tLoss: 0.834742\n",
      "Train Epoch: 1 [400/3177 (63%)]\tlr:0.00623\tAccuracy: 0.9000\tLoss: 0.834742\n",
      "Train Epoch: 1 [400/3177 (63%)]\tlr:0.00623\tAccuracy: 0.9000\tLoss: 0.834742\n",
      "Train Epoch: 1 [400/3177 (63%)]\tlr:0.00623\tAccuracy: 0.9000\tLoss: 0.834742\n",
      "Train Epoch: 1 [400/3177 (63%)]\tlr:0.00623\tAccuracy: 0.9000\tLoss: 0.834742\n",
      "Train Epoch: 1 [400/3177 (63%)]\tlr:0.00623\tAccuracy: 0.9000\tLoss: 0.834742\n",
      "Train Epoch: 1 [400/3177 (63%)]\tlr:0.00623\tAccuracy: 0.9000\tLoss: 0.834742\n",
      "Train Epoch: 1 [400/3177 (63%)]\tlr:0.00623\tAccuracy: 0.9000\tLoss: 0.834742\n",
      "Train Epoch: 1 [400/3177 (63%)]\tlr:0.00623\tAccuracy: 0.9000\tLoss: 0.834742\n",
      "Train Epoch: 1 [400/3177 (63%)]\tlr:0.00623\tAccuracy: 0.9000\tLoss: 0.834742\n",
      "Train Epoch: 1 [400/3177 (63%)]\tlr:0.00623\tAccuracy: 0.9000\tLoss: 0.834742\n",
      "Train Epoch: 1 [400/3177 (63%)]\tlr:0.00623\tAccuracy: 0.9000\tLoss: 0.834742\n",
      "Train Epoch: 1 [400/3177 (63%)]\tlr:0.00623\tAccuracy: 0.9000\tLoss: 0.834742\n",
      "Train Epoch: 1 [400/3177 (63%)]\tlr:0.00623\tAccuracy: 0.9000\tLoss: 0.834742\n",
      "Train Epoch: 1 [400/3177 (63%)]\tlr:0.00623\tAccuracy: 0.9000\tLoss: 0.834742\n",
      "Train Epoch: 1 [400/3177 (63%)]\tlr:0.00623\tAccuracy: 0.9000\tLoss: 0.834742\n",
      "Train Epoch: 1 [400/3177 (63%)]\tlr:0.00623\tAccuracy: 0.9000\tLoss: 0.834742\n",
      "Train Epoch: 1 [400/3177 (63%)]\tlr:0.00623\tAccuracy: 0.9000\tLoss: 0.834742\n",
      "Train Epoch: 1 [420/3177 (66%)]\tlr:0.00608\tAccuracy: 0.6000\tLoss: 0.965946\n",
      "Train Epoch: 1 [420/3177 (66%)]\tlr:0.00608\tAccuracy: 0.6000\tLoss: 0.965946\n",
      "Train Epoch: 1 [420/3177 (66%)]\tlr:0.00608\tAccuracy: 0.6000\tLoss: 0.965946\n",
      "Train Epoch: 1 [420/3177 (66%)]\tlr:0.00608\tAccuracy: 0.6000\tLoss: 0.965946\n",
      "Train Epoch: 1 [420/3177 (66%)]\tlr:0.00608\tAccuracy: 0.6000\tLoss: 0.965946\n",
      "Train Epoch: 1 [420/3177 (66%)]\tlr:0.00608\tAccuracy: 0.6000\tLoss: 0.965946\n",
      "Train Epoch: 1 [420/3177 (66%)]\tlr:0.00608\tAccuracy: 0.6000\tLoss: 0.965946\n",
      "Train Epoch: 1 [420/3177 (66%)]\tlr:0.00608\tAccuracy: 0.6000\tLoss: 0.965946\n",
      "Train Epoch: 1 [420/3177 (66%)]\tlr:0.00608\tAccuracy: 0.6000\tLoss: 0.965946\n",
      "Train Epoch: 1 [420/3177 (66%)]\tlr:0.00608\tAccuracy: 0.6000\tLoss: 0.965946\n",
      "Train Epoch: 1 [420/3177 (66%)]\tlr:0.00608\tAccuracy: 0.6000\tLoss: 0.965946\n",
      "Train Epoch: 1 [420/3177 (66%)]\tlr:0.00608\tAccuracy: 0.6000\tLoss: 0.965946\n",
      "Train Epoch: 1 [420/3177 (66%)]\tlr:0.00608\tAccuracy: 0.6000\tLoss: 0.965946\n",
      "Train Epoch: 1 [420/3177 (66%)]\tlr:0.00608\tAccuracy: 0.6000\tLoss: 0.965946\n",
      "Train Epoch: 1 [420/3177 (66%)]\tlr:0.00608\tAccuracy: 0.6000\tLoss: 0.965946\n",
      "Train Epoch: 1 [420/3177 (66%)]\tlr:0.00608\tAccuracy: 0.6000\tLoss: 0.965946\n",
      "Train Epoch: 1 [420/3177 (66%)]\tlr:0.00608\tAccuracy: 0.6000\tLoss: 0.965946\n",
      "Train Epoch: 1 [420/3177 (66%)]\tlr:0.00608\tAccuracy: 0.6000\tLoss: 0.965946\n",
      "Train Epoch: 1 [420/3177 (66%)]\tlr:0.00608\tAccuracy: 0.6000\tLoss: 0.965946\n",
      "Train Epoch: 1 [420/3177 (66%)]\tlr:0.00608\tAccuracy: 0.6000\tLoss: 0.965946\n",
      "Train Epoch: 1 [420/3177 (66%)]\tlr:0.00608\tAccuracy: 0.6000\tLoss: 0.965946\n",
      "Train Epoch: 1 [420/3177 (66%)]\tlr:0.00608\tAccuracy: 0.6000\tLoss: 0.965946\n",
      "Train Epoch: 1 [420/3177 (66%)]\tlr:0.00608\tAccuracy: 0.6000\tLoss: 0.965946\n",
      "Train Epoch: 1 [440/3177 (69%)]\tlr:0.00595\tAccuracy: 0.5000\tLoss: 1.480630\n",
      "Train Epoch: 1 [440/3177 (69%)]\tlr:0.00595\tAccuracy: 0.5000\tLoss: 1.480630\n",
      "Train Epoch: 1 [440/3177 (69%)]\tlr:0.00595\tAccuracy: 0.5000\tLoss: 1.480630\n",
      "Train Epoch: 1 [440/3177 (69%)]\tlr:0.00595\tAccuracy: 0.5000\tLoss: 1.480630\n",
      "Train Epoch: 1 [440/3177 (69%)]\tlr:0.00595\tAccuracy: 0.5000\tLoss: 1.480630\n",
      "Train Epoch: 1 [440/3177 (69%)]\tlr:0.00595\tAccuracy: 0.5000\tLoss: 1.480630\n",
      "Train Epoch: 1 [440/3177 (69%)]\tlr:0.00595\tAccuracy: 0.5000\tLoss: 1.480630\n",
      "Train Epoch: 1 [440/3177 (69%)]\tlr:0.00595\tAccuracy: 0.5000\tLoss: 1.480630\n",
      "Train Epoch: 1 [440/3177 (69%)]\tlr:0.00595\tAccuracy: 0.5000\tLoss: 1.480630\n",
      "Train Epoch: 1 [440/3177 (69%)]\tlr:0.00595\tAccuracy: 0.5000\tLoss: 1.480630\n",
      "Train Epoch: 1 [440/3177 (69%)]\tlr:0.00595\tAccuracy: 0.5000\tLoss: 1.480630\n",
      "Train Epoch: 1 [440/3177 (69%)]\tlr:0.00595\tAccuracy: 0.5000\tLoss: 1.480630\n",
      "Train Epoch: 1 [440/3177 (69%)]\tlr:0.00595\tAccuracy: 0.5000\tLoss: 1.480630\n",
      "Train Epoch: 1 [440/3177 (69%)]\tlr:0.00595\tAccuracy: 0.5000\tLoss: 1.480630\n",
      "Train Epoch: 1 [440/3177 (69%)]\tlr:0.00595\tAccuracy: 0.5000\tLoss: 1.480630\n",
      "Train Epoch: 1 [440/3177 (69%)]\tlr:0.00595\tAccuracy: 0.5000\tLoss: 1.480630\n",
      "Train Epoch: 1 [440/3177 (69%)]\tlr:0.00595\tAccuracy: 0.5000\tLoss: 1.480630\n",
      "Train Epoch: 1 [440/3177 (69%)]\tlr:0.00595\tAccuracy: 0.5000\tLoss: 1.480630\n",
      "Train Epoch: 1 [440/3177 (69%)]\tlr:0.00595\tAccuracy: 0.5000\tLoss: 1.480630\n",
      "Train Epoch: 1 [440/3177 (69%)]\tlr:0.00595\tAccuracy: 0.5000\tLoss: 1.480630\n",
      "Train Epoch: 1 [440/3177 (69%)]\tlr:0.00595\tAccuracy: 0.5000\tLoss: 1.480630\n",
      "Train Epoch: 1 [440/3177 (69%)]\tlr:0.00595\tAccuracy: 0.5000\tLoss: 1.480630\n",
      "Train Epoch: 1 [440/3177 (69%)]\tlr:0.00595\tAccuracy: 0.5000\tLoss: 1.480630\n",
      "Train Epoch: 1 [460/3177 (72%)]\tlr:0.00582\tAccuracy: 0.3000\tLoss: 1.491686\n",
      "Train Epoch: 1 [460/3177 (72%)]\tlr:0.00582\tAccuracy: 0.3000\tLoss: 1.491686\n",
      "Train Epoch: 1 [460/3177 (72%)]\tlr:0.00582\tAccuracy: 0.3000\tLoss: 1.491686\n",
      "Train Epoch: 1 [460/3177 (72%)]\tlr:0.00582\tAccuracy: 0.3000\tLoss: 1.491686\n",
      "Train Epoch: 1 [460/3177 (72%)]\tlr:0.00582\tAccuracy: 0.3000\tLoss: 1.491686\n",
      "Train Epoch: 1 [460/3177 (72%)]\tlr:0.00582\tAccuracy: 0.3000\tLoss: 1.491686\n",
      "Train Epoch: 1 [460/3177 (72%)]\tlr:0.00582\tAccuracy: 0.3000\tLoss: 1.491686\n",
      "Train Epoch: 1 [460/3177 (72%)]\tlr:0.00582\tAccuracy: 0.3000\tLoss: 1.491686\n",
      "Train Epoch: 1 [460/3177 (72%)]\tlr:0.00582\tAccuracy: 0.3000\tLoss: 1.491686\n",
      "Train Epoch: 1 [460/3177 (72%)]\tlr:0.00582\tAccuracy: 0.3000\tLoss: 1.491686\n",
      "Train Epoch: 1 [460/3177 (72%)]\tlr:0.00582\tAccuracy: 0.3000\tLoss: 1.491686\n",
      "Train Epoch: 1 [460/3177 (72%)]\tlr:0.00582\tAccuracy: 0.3000\tLoss: 1.491686\n",
      "Train Epoch: 1 [460/3177 (72%)]\tlr:0.00582\tAccuracy: 0.3000\tLoss: 1.491686\n",
      "Train Epoch: 1 [460/3177 (72%)]\tlr:0.00582\tAccuracy: 0.3000\tLoss: 1.491686\n",
      "Train Epoch: 1 [460/3177 (72%)]\tlr:0.00582\tAccuracy: 0.3000\tLoss: 1.491686\n",
      "Train Epoch: 1 [460/3177 (72%)]\tlr:0.00582\tAccuracy: 0.3000\tLoss: 1.491686\n",
      "Train Epoch: 1 [460/3177 (72%)]\tlr:0.00582\tAccuracy: 0.3000\tLoss: 1.491686\n",
      "Train Epoch: 1 [460/3177 (72%)]\tlr:0.00582\tAccuracy: 0.3000\tLoss: 1.491686\n",
      "Train Epoch: 1 [460/3177 (72%)]\tlr:0.00582\tAccuracy: 0.3000\tLoss: 1.491686\n",
      "Train Epoch: 1 [460/3177 (72%)]\tlr:0.00582\tAccuracy: 0.3000\tLoss: 1.491686\n",
      "Train Epoch: 1 [460/3177 (72%)]\tlr:0.00582\tAccuracy: 0.3000\tLoss: 1.491686\n",
      "Train Epoch: 1 [460/3177 (72%)]\tlr:0.00582\tAccuracy: 0.3000\tLoss: 1.491686\n",
      "Train Epoch: 1 [460/3177 (72%)]\tlr:0.00582\tAccuracy: 0.3000\tLoss: 1.491686\n",
      "Train Epoch: 1 [480/3177 (75%)]\tlr:0.00569\tAccuracy: 0.7000\tLoss: 1.045535\n",
      "Train Epoch: 1 [480/3177 (75%)]\tlr:0.00569\tAccuracy: 0.7000\tLoss: 1.045535\n",
      "Train Epoch: 1 [480/3177 (75%)]\tlr:0.00569\tAccuracy: 0.7000\tLoss: 1.045535\n",
      "Train Epoch: 1 [480/3177 (75%)]\tlr:0.00569\tAccuracy: 0.7000\tLoss: 1.045535\n",
      "Train Epoch: 1 [480/3177 (75%)]\tlr:0.00569\tAccuracy: 0.7000\tLoss: 1.045535\n",
      "Train Epoch: 1 [480/3177 (75%)]\tlr:0.00569\tAccuracy: 0.7000\tLoss: 1.045535\n",
      "Train Epoch: 1 [480/3177 (75%)]\tlr:0.00569\tAccuracy: 0.7000\tLoss: 1.045535\n",
      "Train Epoch: 1 [480/3177 (75%)]\tlr:0.00569\tAccuracy: 0.7000\tLoss: 1.045535\n",
      "Train Epoch: 1 [480/3177 (75%)]\tlr:0.00569\tAccuracy: 0.7000\tLoss: 1.045535\n",
      "Train Epoch: 1 [480/3177 (75%)]\tlr:0.00569\tAccuracy: 0.7000\tLoss: 1.045535\n",
      "Train Epoch: 1 [480/3177 (75%)]\tlr:0.00569\tAccuracy: 0.7000\tLoss: 1.045535\n",
      "Train Epoch: 1 [480/3177 (75%)]\tlr:0.00569\tAccuracy: 0.7000\tLoss: 1.045535\n",
      "Train Epoch: 1 [480/3177 (75%)]\tlr:0.00569\tAccuracy: 0.7000\tLoss: 1.045535\n",
      "Train Epoch: 1 [480/3177 (75%)]\tlr:0.00569\tAccuracy: 0.7000\tLoss: 1.045535\n",
      "Train Epoch: 1 [480/3177 (75%)]\tlr:0.00569\tAccuracy: 0.7000\tLoss: 1.045535\n",
      "Train Epoch: 1 [480/3177 (75%)]\tlr:0.00569\tAccuracy: 0.7000\tLoss: 1.045535\n",
      "Train Epoch: 1 [480/3177 (75%)]\tlr:0.00569\tAccuracy: 0.7000\tLoss: 1.045535\n",
      "Train Epoch: 1 [480/3177 (75%)]\tlr:0.00569\tAccuracy: 0.7000\tLoss: 1.045535\n",
      "Train Epoch: 1 [480/3177 (75%)]\tlr:0.00569\tAccuracy: 0.7000\tLoss: 1.045535\n",
      "Train Epoch: 1 [480/3177 (75%)]\tlr:0.00569\tAccuracy: 0.7000\tLoss: 1.045535\n",
      "Train Epoch: 1 [480/3177 (75%)]\tlr:0.00569\tAccuracy: 0.7000\tLoss: 1.045535\n",
      "Train Epoch: 1 [480/3177 (75%)]\tlr:0.00569\tAccuracy: 0.7000\tLoss: 1.045535\n",
      "Train Epoch: 1 [480/3177 (75%)]\tlr:0.00569\tAccuracy: 0.7000\tLoss: 1.045535\n",
      "Train Epoch: 1 [500/3177 (79%)]\tlr:0.00558\tAccuracy: 0.3000\tLoss: 1.456710\n",
      "Train Epoch: 1 [500/3177 (79%)]\tlr:0.00558\tAccuracy: 0.3000\tLoss: 1.456710\n",
      "Train Epoch: 1 [500/3177 (79%)]\tlr:0.00558\tAccuracy: 0.3000\tLoss: 1.456710\n",
      "Train Epoch: 1 [500/3177 (79%)]\tlr:0.00558\tAccuracy: 0.3000\tLoss: 1.456710\n",
      "Train Epoch: 1 [500/3177 (79%)]\tlr:0.00558\tAccuracy: 0.3000\tLoss: 1.456710\n",
      "Train Epoch: 1 [500/3177 (79%)]\tlr:0.00558\tAccuracy: 0.3000\tLoss: 1.456710\n",
      "Train Epoch: 1 [500/3177 (79%)]\tlr:0.00558\tAccuracy: 0.3000\tLoss: 1.456710\n",
      "Train Epoch: 1 [500/3177 (79%)]\tlr:0.00558\tAccuracy: 0.3000\tLoss: 1.456710\n",
      "Train Epoch: 1 [500/3177 (79%)]\tlr:0.00558\tAccuracy: 0.3000\tLoss: 1.456710\n",
      "Train Epoch: 1 [500/3177 (79%)]\tlr:0.00558\tAccuracy: 0.3000\tLoss: 1.456710\n",
      "Train Epoch: 1 [500/3177 (79%)]\tlr:0.00558\tAccuracy: 0.3000\tLoss: 1.456710\n",
      "Train Epoch: 1 [500/3177 (79%)]\tlr:0.00558\tAccuracy: 0.3000\tLoss: 1.456710\n",
      "Train Epoch: 1 [500/3177 (79%)]\tlr:0.00558\tAccuracy: 0.3000\tLoss: 1.456710\n",
      "Train Epoch: 1 [500/3177 (79%)]\tlr:0.00558\tAccuracy: 0.3000\tLoss: 1.456710\n",
      "Train Epoch: 1 [500/3177 (79%)]\tlr:0.00558\tAccuracy: 0.3000\tLoss: 1.456710\n",
      "Train Epoch: 1 [500/3177 (79%)]\tlr:0.00558\tAccuracy: 0.3000\tLoss: 1.456710\n",
      "Train Epoch: 1 [500/3177 (79%)]\tlr:0.00558\tAccuracy: 0.3000\tLoss: 1.456710\n",
      "Train Epoch: 1 [500/3177 (79%)]\tlr:0.00558\tAccuracy: 0.3000\tLoss: 1.456710\n",
      "Train Epoch: 1 [500/3177 (79%)]\tlr:0.00558\tAccuracy: 0.3000\tLoss: 1.456710\n",
      "Train Epoch: 1 [500/3177 (79%)]\tlr:0.00558\tAccuracy: 0.3000\tLoss: 1.456710\n",
      "Train Epoch: 1 [500/3177 (79%)]\tlr:0.00558\tAccuracy: 0.3000\tLoss: 1.456710\n",
      "Train Epoch: 1 [500/3177 (79%)]\tlr:0.00558\tAccuracy: 0.3000\tLoss: 1.456710\n",
      "Train Epoch: 1 [500/3177 (79%)]\tlr:0.00558\tAccuracy: 0.3000\tLoss: 1.456710\n",
      "Train Epoch: 1 [520/3177 (82%)]\tlr:0.00547\tAccuracy: 0.6000\tLoss: 1.103112\n",
      "Train Epoch: 1 [520/3177 (82%)]\tlr:0.00547\tAccuracy: 0.6000\tLoss: 1.103112\n",
      "Train Epoch: 1 [520/3177 (82%)]\tlr:0.00547\tAccuracy: 0.6000\tLoss: 1.103112\n",
      "Train Epoch: 1 [520/3177 (82%)]\tlr:0.00547\tAccuracy: 0.6000\tLoss: 1.103112\n",
      "Train Epoch: 1 [520/3177 (82%)]\tlr:0.00547\tAccuracy: 0.6000\tLoss: 1.103112\n",
      "Train Epoch: 1 [520/3177 (82%)]\tlr:0.00547\tAccuracy: 0.6000\tLoss: 1.103112\n",
      "Train Epoch: 1 [520/3177 (82%)]\tlr:0.00547\tAccuracy: 0.6000\tLoss: 1.103112\n",
      "Train Epoch: 1 [520/3177 (82%)]\tlr:0.00547\tAccuracy: 0.6000\tLoss: 1.103112\n",
      "Train Epoch: 1 [520/3177 (82%)]\tlr:0.00547\tAccuracy: 0.6000\tLoss: 1.103112\n",
      "Train Epoch: 1 [520/3177 (82%)]\tlr:0.00547\tAccuracy: 0.6000\tLoss: 1.103112\n",
      "Train Epoch: 1 [520/3177 (82%)]\tlr:0.00547\tAccuracy: 0.6000\tLoss: 1.103112\n",
      "Train Epoch: 1 [520/3177 (82%)]\tlr:0.00547\tAccuracy: 0.6000\tLoss: 1.103112\n",
      "Train Epoch: 1 [520/3177 (82%)]\tlr:0.00547\tAccuracy: 0.6000\tLoss: 1.103112\n",
      "Train Epoch: 1 [520/3177 (82%)]\tlr:0.00547\tAccuracy: 0.6000\tLoss: 1.103112\n",
      "Train Epoch: 1 [520/3177 (82%)]\tlr:0.00547\tAccuracy: 0.6000\tLoss: 1.103112\n",
      "Train Epoch: 1 [520/3177 (82%)]\tlr:0.00547\tAccuracy: 0.6000\tLoss: 1.103112\n",
      "Train Epoch: 1 [520/3177 (82%)]\tlr:0.00547\tAccuracy: 0.6000\tLoss: 1.103112\n",
      "Train Epoch: 1 [520/3177 (82%)]\tlr:0.00547\tAccuracy: 0.6000\tLoss: 1.103112\n",
      "Train Epoch: 1 [520/3177 (82%)]\tlr:0.00547\tAccuracy: 0.6000\tLoss: 1.103112\n",
      "Train Epoch: 1 [520/3177 (82%)]\tlr:0.00547\tAccuracy: 0.6000\tLoss: 1.103112\n",
      "Train Epoch: 1 [520/3177 (82%)]\tlr:0.00547\tAccuracy: 0.6000\tLoss: 1.103112\n",
      "Train Epoch: 1 [520/3177 (82%)]\tlr:0.00547\tAccuracy: 0.6000\tLoss: 1.103112\n",
      "Train Epoch: 1 [520/3177 (82%)]\tlr:0.00547\tAccuracy: 0.6000\tLoss: 1.103112\n",
      "Train Epoch: 1 [540/3177 (85%)]\tlr:0.00537\tAccuracy: 0.5000\tLoss: 1.222998\n",
      "Train Epoch: 1 [540/3177 (85%)]\tlr:0.00537\tAccuracy: 0.5000\tLoss: 1.222998\n",
      "Train Epoch: 1 [540/3177 (85%)]\tlr:0.00537\tAccuracy: 0.5000\tLoss: 1.222998\n",
      "Train Epoch: 1 [540/3177 (85%)]\tlr:0.00537\tAccuracy: 0.5000\tLoss: 1.222998\n",
      "Train Epoch: 1 [540/3177 (85%)]\tlr:0.00537\tAccuracy: 0.5000\tLoss: 1.222998\n",
      "Train Epoch: 1 [540/3177 (85%)]\tlr:0.00537\tAccuracy: 0.5000\tLoss: 1.222998\n",
      "Train Epoch: 1 [540/3177 (85%)]\tlr:0.00537\tAccuracy: 0.5000\tLoss: 1.222998\n",
      "Train Epoch: 1 [540/3177 (85%)]\tlr:0.00537\tAccuracy: 0.5000\tLoss: 1.222998\n",
      "Train Epoch: 1 [540/3177 (85%)]\tlr:0.00537\tAccuracy: 0.5000\tLoss: 1.222998\n",
      "Train Epoch: 1 [540/3177 (85%)]\tlr:0.00537\tAccuracy: 0.5000\tLoss: 1.222998\n",
      "Train Epoch: 1 [540/3177 (85%)]\tlr:0.00537\tAccuracy: 0.5000\tLoss: 1.222998\n",
      "Train Epoch: 1 [540/3177 (85%)]\tlr:0.00537\tAccuracy: 0.5000\tLoss: 1.222998\n",
      "Train Epoch: 1 [540/3177 (85%)]\tlr:0.00537\tAccuracy: 0.5000\tLoss: 1.222998\n",
      "Train Epoch: 1 [540/3177 (85%)]\tlr:0.00537\tAccuracy: 0.5000\tLoss: 1.222998\n",
      "Train Epoch: 1 [540/3177 (85%)]\tlr:0.00537\tAccuracy: 0.5000\tLoss: 1.222998\n",
      "Train Epoch: 1 [540/3177 (85%)]\tlr:0.00537\tAccuracy: 0.5000\tLoss: 1.222998\n",
      "Train Epoch: 1 [540/3177 (85%)]\tlr:0.00537\tAccuracy: 0.5000\tLoss: 1.222998\n",
      "Train Epoch: 1 [540/3177 (85%)]\tlr:0.00537\tAccuracy: 0.5000\tLoss: 1.222998\n",
      "Train Epoch: 1 [540/3177 (85%)]\tlr:0.00537\tAccuracy: 0.5000\tLoss: 1.222998\n",
      "Train Epoch: 1 [540/3177 (85%)]\tlr:0.00537\tAccuracy: 0.5000\tLoss: 1.222998\n",
      "Train Epoch: 1 [540/3177 (85%)]\tlr:0.00537\tAccuracy: 0.5000\tLoss: 1.222998\n",
      "Train Epoch: 1 [540/3177 (85%)]\tlr:0.00537\tAccuracy: 0.5000\tLoss: 1.222998\n",
      "Train Epoch: 1 [540/3177 (85%)]\tlr:0.00537\tAccuracy: 0.5000\tLoss: 1.222998\n",
      "Train Epoch: 1 [560/3177 (88%)]\tlr:0.00527\tAccuracy: 0.5000\tLoss: 0.867480\n",
      "Train Epoch: 1 [560/3177 (88%)]\tlr:0.00527\tAccuracy: 0.5000\tLoss: 0.867480\n",
      "Train Epoch: 1 [560/3177 (88%)]\tlr:0.00527\tAccuracy: 0.5000\tLoss: 0.867480\n",
      "Train Epoch: 1 [560/3177 (88%)]\tlr:0.00527\tAccuracy: 0.5000\tLoss: 0.867480\n",
      "Train Epoch: 1 [560/3177 (88%)]\tlr:0.00527\tAccuracy: 0.5000\tLoss: 0.867480\n",
      "Train Epoch: 1 [560/3177 (88%)]\tlr:0.00527\tAccuracy: 0.5000\tLoss: 0.867480\n",
      "Train Epoch: 1 [560/3177 (88%)]\tlr:0.00527\tAccuracy: 0.5000\tLoss: 0.867480\n",
      "Train Epoch: 1 [560/3177 (88%)]\tlr:0.00527\tAccuracy: 0.5000\tLoss: 0.867480\n",
      "Train Epoch: 1 [560/3177 (88%)]\tlr:0.00527\tAccuracy: 0.5000\tLoss: 0.867480\n",
      "Train Epoch: 1 [560/3177 (88%)]\tlr:0.00527\tAccuracy: 0.5000\tLoss: 0.867480\n",
      "Train Epoch: 1 [560/3177 (88%)]\tlr:0.00527\tAccuracy: 0.5000\tLoss: 0.867480\n",
      "Train Epoch: 1 [560/3177 (88%)]\tlr:0.00527\tAccuracy: 0.5000\tLoss: 0.867480\n",
      "Train Epoch: 1 [560/3177 (88%)]\tlr:0.00527\tAccuracy: 0.5000\tLoss: 0.867480\n",
      "Train Epoch: 1 [560/3177 (88%)]\tlr:0.00527\tAccuracy: 0.5000\tLoss: 0.867480\n",
      "Train Epoch: 1 [560/3177 (88%)]\tlr:0.00527\tAccuracy: 0.5000\tLoss: 0.867480\n",
      "Train Epoch: 1 [560/3177 (88%)]\tlr:0.00527\tAccuracy: 0.5000\tLoss: 0.867480\n",
      "Train Epoch: 1 [560/3177 (88%)]\tlr:0.00527\tAccuracy: 0.5000\tLoss: 0.867480\n",
      "Train Epoch: 1 [560/3177 (88%)]\tlr:0.00527\tAccuracy: 0.5000\tLoss: 0.867480\n",
      "Train Epoch: 1 [560/3177 (88%)]\tlr:0.00527\tAccuracy: 0.5000\tLoss: 0.867480\n",
      "Train Epoch: 1 [560/3177 (88%)]\tlr:0.00527\tAccuracy: 0.5000\tLoss: 0.867480\n",
      "Train Epoch: 1 [560/3177 (88%)]\tlr:0.00527\tAccuracy: 0.5000\tLoss: 0.867480\n",
      "Train Epoch: 1 [560/3177 (88%)]\tlr:0.00527\tAccuracy: 0.5000\tLoss: 0.867480\n",
      "Train Epoch: 1 [560/3177 (88%)]\tlr:0.00527\tAccuracy: 0.5000\tLoss: 0.867480\n",
      "Train Epoch: 1 [580/3177 (91%)]\tlr:0.00518\tAccuracy: 0.7000\tLoss: 1.438024\n",
      "Train Epoch: 1 [580/3177 (91%)]\tlr:0.00518\tAccuracy: 0.7000\tLoss: 1.438024\n",
      "Train Epoch: 1 [580/3177 (91%)]\tlr:0.00518\tAccuracy: 0.7000\tLoss: 1.438024\n",
      "Train Epoch: 1 [580/3177 (91%)]\tlr:0.00518\tAccuracy: 0.7000\tLoss: 1.438024\n",
      "Train Epoch: 1 [580/3177 (91%)]\tlr:0.00518\tAccuracy: 0.7000\tLoss: 1.438024\n",
      "Train Epoch: 1 [580/3177 (91%)]\tlr:0.00518\tAccuracy: 0.7000\tLoss: 1.438024\n",
      "Train Epoch: 1 [580/3177 (91%)]\tlr:0.00518\tAccuracy: 0.7000\tLoss: 1.438024\n",
      "Train Epoch: 1 [580/3177 (91%)]\tlr:0.00518\tAccuracy: 0.7000\tLoss: 1.438024\n",
      "Train Epoch: 1 [580/3177 (91%)]\tlr:0.00518\tAccuracy: 0.7000\tLoss: 1.438024\n",
      "Train Epoch: 1 [580/3177 (91%)]\tlr:0.00518\tAccuracy: 0.7000\tLoss: 1.438024\n",
      "Train Epoch: 1 [580/3177 (91%)]\tlr:0.00518\tAccuracy: 0.7000\tLoss: 1.438024\n",
      "Train Epoch: 1 [580/3177 (91%)]\tlr:0.00518\tAccuracy: 0.7000\tLoss: 1.438024\n",
      "Train Epoch: 1 [580/3177 (91%)]\tlr:0.00518\tAccuracy: 0.7000\tLoss: 1.438024\n",
      "Train Epoch: 1 [580/3177 (91%)]\tlr:0.00518\tAccuracy: 0.7000\tLoss: 1.438024\n",
      "Train Epoch: 1 [580/3177 (91%)]\tlr:0.00518\tAccuracy: 0.7000\tLoss: 1.438024\n",
      "Train Epoch: 1 [580/3177 (91%)]\tlr:0.00518\tAccuracy: 0.7000\tLoss: 1.438024\n",
      "Train Epoch: 1 [580/3177 (91%)]\tlr:0.00518\tAccuracy: 0.7000\tLoss: 1.438024\n",
      "Train Epoch: 1 [580/3177 (91%)]\tlr:0.00518\tAccuracy: 0.7000\tLoss: 1.438024\n",
      "Train Epoch: 1 [580/3177 (91%)]\tlr:0.00518\tAccuracy: 0.7000\tLoss: 1.438024\n",
      "Train Epoch: 1 [580/3177 (91%)]\tlr:0.00518\tAccuracy: 0.7000\tLoss: 1.438024\n",
      "Train Epoch: 1 [580/3177 (91%)]\tlr:0.00518\tAccuracy: 0.7000\tLoss: 1.438024\n",
      "Train Epoch: 1 [580/3177 (91%)]\tlr:0.00518\tAccuracy: 0.7000\tLoss: 1.438024\n",
      "Train Epoch: 1 [580/3177 (91%)]\tlr:0.00518\tAccuracy: 0.7000\tLoss: 1.438024\n",
      "Train Epoch: 1 [600/3177 (94%)]\tlr:0.00509\tAccuracy: 0.2000\tLoss: 1.016995\n",
      "Train Epoch: 1 [600/3177 (94%)]\tlr:0.00509\tAccuracy: 0.2000\tLoss: 1.016995\n",
      "Train Epoch: 1 [600/3177 (94%)]\tlr:0.00509\tAccuracy: 0.2000\tLoss: 1.016995\n",
      "Train Epoch: 1 [600/3177 (94%)]\tlr:0.00509\tAccuracy: 0.2000\tLoss: 1.016995\n",
      "Train Epoch: 1 [600/3177 (94%)]\tlr:0.00509\tAccuracy: 0.2000\tLoss: 1.016995\n",
      "Train Epoch: 1 [600/3177 (94%)]\tlr:0.00509\tAccuracy: 0.2000\tLoss: 1.016995\n",
      "Train Epoch: 1 [600/3177 (94%)]\tlr:0.00509\tAccuracy: 0.2000\tLoss: 1.016995\n",
      "Train Epoch: 1 [600/3177 (94%)]\tlr:0.00509\tAccuracy: 0.2000\tLoss: 1.016995\n",
      "Train Epoch: 1 [600/3177 (94%)]\tlr:0.00509\tAccuracy: 0.2000\tLoss: 1.016995\n",
      "Train Epoch: 1 [600/3177 (94%)]\tlr:0.00509\tAccuracy: 0.2000\tLoss: 1.016995\n",
      "Train Epoch: 1 [600/3177 (94%)]\tlr:0.00509\tAccuracy: 0.2000\tLoss: 1.016995\n",
      "Train Epoch: 1 [600/3177 (94%)]\tlr:0.00509\tAccuracy: 0.2000\tLoss: 1.016995\n",
      "Train Epoch: 1 [600/3177 (94%)]\tlr:0.00509\tAccuracy: 0.2000\tLoss: 1.016995\n",
      "Train Epoch: 1 [600/3177 (94%)]\tlr:0.00509\tAccuracy: 0.2000\tLoss: 1.016995\n",
      "Train Epoch: 1 [600/3177 (94%)]\tlr:0.00509\tAccuracy: 0.2000\tLoss: 1.016995\n",
      "Train Epoch: 1 [600/3177 (94%)]\tlr:0.00509\tAccuracy: 0.2000\tLoss: 1.016995\n",
      "Train Epoch: 1 [600/3177 (94%)]\tlr:0.00509\tAccuracy: 0.2000\tLoss: 1.016995\n",
      "Train Epoch: 1 [600/3177 (94%)]\tlr:0.00509\tAccuracy: 0.2000\tLoss: 1.016995\n",
      "Train Epoch: 1 [600/3177 (94%)]\tlr:0.00509\tAccuracy: 0.2000\tLoss: 1.016995\n",
      "Train Epoch: 1 [600/3177 (94%)]\tlr:0.00509\tAccuracy: 0.2000\tLoss: 1.016995\n",
      "Train Epoch: 1 [600/3177 (94%)]\tlr:0.00509\tAccuracy: 0.2000\tLoss: 1.016995\n",
      "Train Epoch: 1 [600/3177 (94%)]\tlr:0.00509\tAccuracy: 0.2000\tLoss: 1.016995\n",
      "Train Epoch: 1 [600/3177 (94%)]\tlr:0.00509\tAccuracy: 0.2000\tLoss: 1.016995\n",
      "Train Epoch: 1 [620/3177 (97%)]\tlr:0.00501\tAccuracy: 0.5000\tLoss: 1.138574\n",
      "Train Epoch: 1 [620/3177 (97%)]\tlr:0.00501\tAccuracy: 0.5000\tLoss: 1.138574\n",
      "Train Epoch: 1 [620/3177 (97%)]\tlr:0.00501\tAccuracy: 0.5000\tLoss: 1.138574\n",
      "Train Epoch: 1 [620/3177 (97%)]\tlr:0.00501\tAccuracy: 0.5000\tLoss: 1.138574\n",
      "Train Epoch: 1 [620/3177 (97%)]\tlr:0.00501\tAccuracy: 0.5000\tLoss: 1.138574\n",
      "Train Epoch: 1 [620/3177 (97%)]\tlr:0.00501\tAccuracy: 0.5000\tLoss: 1.138574\n",
      "Train Epoch: 1 [620/3177 (97%)]\tlr:0.00501\tAccuracy: 0.5000\tLoss: 1.138574\n",
      "Train Epoch: 1 [620/3177 (97%)]\tlr:0.00501\tAccuracy: 0.5000\tLoss: 1.138574\n",
      "Train Epoch: 1 [620/3177 (97%)]\tlr:0.00501\tAccuracy: 0.5000\tLoss: 1.138574\n",
      "Train Epoch: 1 [620/3177 (97%)]\tlr:0.00501\tAccuracy: 0.5000\tLoss: 1.138574\n",
      "Train Epoch: 1 [620/3177 (97%)]\tlr:0.00501\tAccuracy: 0.5000\tLoss: 1.138574\n",
      "Train Epoch: 1 [620/3177 (97%)]\tlr:0.00501\tAccuracy: 0.5000\tLoss: 1.138574\n",
      "Train Epoch: 1 [620/3177 (97%)]\tlr:0.00501\tAccuracy: 0.5000\tLoss: 1.138574\n",
      "Train Epoch: 1 [620/3177 (97%)]\tlr:0.00501\tAccuracy: 0.5000\tLoss: 1.138574\n",
      "Train Epoch: 1 [620/3177 (97%)]\tlr:0.00501\tAccuracy: 0.5000\tLoss: 1.138574\n",
      "Train Epoch: 1 [620/3177 (97%)]\tlr:0.00501\tAccuracy: 0.5000\tLoss: 1.138574\n",
      "Train Epoch: 1 [620/3177 (97%)]\tlr:0.00501\tAccuracy: 0.5000\tLoss: 1.138574\n",
      "Train Epoch: 1 [620/3177 (97%)]\tlr:0.00501\tAccuracy: 0.5000\tLoss: 1.138574\n",
      "Train Epoch: 1 [620/3177 (97%)]\tlr:0.00501\tAccuracy: 0.5000\tLoss: 1.138574\n",
      "Train Epoch: 1 [620/3177 (97%)]\tlr:0.00501\tAccuracy: 0.5000\tLoss: 1.138574\n",
      "Train Epoch: 1 [620/3177 (97%)]\tlr:0.00501\tAccuracy: 0.5000\tLoss: 1.138574\n",
      "Train Epoch: 1 [620/3177 (97%)]\tlr:0.00501\tAccuracy: 0.5000\tLoss: 1.138574\n",
      "Train Epoch: 1 [620/3177 (97%)]\tlr:0.00501\tAccuracy: 0.5000\tLoss: 1.138574\n",
      "Starting Validation\n",
      "Starting Validation\n",
      "Starting Validation\n",
      "Starting Validation\n",
      "Starting Validation\n",
      "Starting Validation\n",
      "Starting Validation\n",
      "Starting Validation\n",
      "Starting Validation\n",
      "Starting Validation\n",
      "Starting Validation\n",
      "Starting Validation\n",
      "Starting Validation\n",
      "Starting Validation\n",
      "Starting Validation\n",
      "Starting Validation\n",
      "Starting Validation\n",
      "Starting Validation\n",
      "Starting Validation\n",
      "Starting Validation\n",
      "Starting Validation\n",
      "Starting Validation\n",
      "Starting Validation\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Expected hidden size (1, 10, 16), got [1, 2, 16]",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-162-ccb374a62185>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;31m# if __name__ == '__main__':\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-162-ccb374a62185>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;31m#val_acc, val_loss = validationXXreverse(args, model, device, validation_loader, args.batch_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         \u001b[0mval_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;31m# Save\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-161-6bbc331d4464>\u001b[0m in \u001b[0;36mvalidation\u001b[1;34m(args, model, device, data_loader, batch_size)\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[1;31m# data = data.float().unsqueeze(1).to(device) # add channel dimension\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mtotal_acc\u001b[0m  \u001b[1;33m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\software\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-158-cb37e5a1efc0>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, hidden)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;31m#     encode_samples[i-1] = z[:,t_samples+i,:].view(batch,512) # z_tk e.g. size 8*512\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mforward_seq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# e.g. size 8*100*512\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mforward_seq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# output size e.g. 8*100*256\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[0mc_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# c_t e.g. size 8*256\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimestep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# e.g. size 12*8*512\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\software\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\software\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    735\u001b[0m             \u001b[0mhx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 737\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    738\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001b[1;32mD:\\software\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[0mexpected_hidden_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_expected_hidden_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_hidden_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpermute_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpermutation\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\software\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_hidden_size\u001b[1;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[0;32m    194\u001b[0m                           msg: str = 'Expected hidden size {}, got {}') -> None:\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpected_hidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected hidden size (1, 10, 16), got [1, 2, 16]"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    ## Settings\n",
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    parser.add_argument('--train-raw', default='LibriSpeech/train-Librispeech.h5')\n",
    "    parser.add_argument('--validation-raw', default='LibriSpeech/validation-Librispeech.h5')\n",
    "    parser.add_argument('--eval-raw', default='LibriSpeech/eval-Librispeech.h5')\n",
    "    parser.add_argument('--train-list', default='LibriSpeech/list/train.txt')\n",
    "    parser.add_argument('--validation-list', default='LibriSpeech/list/validation.txt')\n",
    "    parser.add_argument('--eval-list')\n",
    "    parser.add_argument('--logging-dir', default='snapshot/cdc/',\n",
    "                        help='model save directory')\n",
    "    parser.add_argument('--epochs', type=int, default=60, metavar='N',\n",
    "                        help='number of epochs to train')\n",
    "    parser.add_argument('--n-warmup-steps', type=int, default=50)\n",
    "    parser.add_argument('--batch-size', type=int, default=10, \n",
    "                        help='batch size')\n",
    "    parser.add_argument('--window', type=int, default=20, \n",
    "                        help='window length to sample from each utterance')\n",
    "    parser.add_argument('--timestep', type=int, default=10) \n",
    "    parser.add_argument('--masked-frames', type=int, default=20)\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "#     args = parser.parse_args()\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "    print('use_cuda is', use_cuda)\n",
    "    global_timer = timer() # global timer\n",
    "    logger = setup_logs(args.logging_dir, run_name) # setup logs\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    model = CPC(52, args.timestep, args.batch_size, args.window).to(device)\n",
    "    #model = CDCK5(args.timestep, args.batch_size, args.audio_window).to(device)\n",
    "    #model = CDCK6(args.timestep, args.batch_sz\n",
    "    params = {'num_workers': 0,\n",
    "              'pin_memory': False} if use_cuda else {}\n",
    "\n",
    "    logger.info('===> loading train, validation and eval dataset')\n",
    "    training_set   = TETrainDataset(window=args.window+args.timestep)\n",
    "    #training_set   = ReverseRawDataset(args.train_raw, args.train_list, args.audio_window)\n",
    "    #training_set   = RawXXreverseDataset(args.train_raw, args.train_list, args.audio_window)\n",
    "    test_set = TETestDataset(window=args.window+args.timestep)\n",
    "    #validation_set = ReverseRawDataset(args.validation_raw, args.validation_list, args.audio_window)\n",
    "    #validation_set = RawXXreverseDataset(args.validation_raw, args.validation_list, args.audio_window)\n",
    "    train_loader = data.DataLoader(training_set, batch_size=args.batch_size, shuffle=True, **params) # set shuffle to True\n",
    "    test_loader = data.DataLoader(test_set, batch_size=args.batch_size, shuffle=False, **params) # set shuffle to False\n",
    "    # nanxin optimizer  \n",
    "    optimizer = ScheduledOptim(\n",
    "        optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, model.parameters()), \n",
    "            betas=(0.9, 0.98), eps=1e-09, weight_decay=1e-4, amsgrad=True),\n",
    "        args.n_warmup_steps)\n",
    "\n",
    "    model_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    logger.info('### Model summary below###\\n {}\\n'.format(str(model)))\n",
    "    logger.info('===> Model total parameter: {}\\n'.format(model_params))\n",
    "    ## Start training\n",
    "    best_acc = 0\n",
    "    best_loss = np.inf\n",
    "    best_epoch = -1 \n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        epoch_timer = timer()\n",
    "\n",
    "        # Train and validate\n",
    "        #trainXXreverse(args, model, device, train_loader, optimizer, epoch, args.batch_size)\n",
    "        #val_acc, val_loss = validationXXreverse(args, model, device, validation_loader, args.batch_size)\n",
    "        train(args, model, device, train_loader, optimizer, epoch, args.batch_size)\n",
    "        val_acc, val_loss = validation(args, model, device, test_loader, args.batch_size)\n",
    "        \n",
    "        # Save\n",
    "        if val_acc > best_acc: \n",
    "            best_acc = max(val_acc, best_acc)\n",
    "            snapshot(args.logging_dir, run_name, {\n",
    "                'epoch': epoch + 1,\n",
    "                'validation_acc': val_acc, \n",
    "                'state_dict': model.state_dict(),\n",
    "                'validation_loss': val_loss,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "            })\n",
    "            best_epoch = epoch + 1\n",
    "        elif epoch - best_epoch > 2:\n",
    "            optimizer.increase_delta()\n",
    "            best_epoch = epoch + 1\n",
    "        \n",
    "        end_epoch_timer = timer()\n",
    "        logger.info(\"#### End epoch {}/{}, elapsed time: {}\".format(epoch, args.epochs, end_epoch_timer - epoch_timer))\n",
    "    \n",
    "    ## end \n",
    "    end_global_timer = timer()\n",
    "    logger.info(\"################## Success #########################\")\n",
    "    logger.info(\"Total elapsed time: %s\" % (end_global_timer - global_timer))\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}